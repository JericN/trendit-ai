subreddit,id,timestamp,permalink,tag,title,body,comments,score
MachineLearning,t3_1d7hmz2,1717454028.0,/r/MachineLearning/comments/1d7hmz2/discussion_do_you_have_an_idea_on_say_what_you/,Discussion,[Discussion] Do you have an idea on Say What you See by Google is implemented?,"[Say What you see by Google](https://experiments.withgoogle.com/say-what-you-see)

The game generates an image and we have to type a prompt that creates a similar image.

I am curious how the images generated by our prompts are compared against the images generated by the game.

1. are they using simple NLP techniques to compare similarity of the prompts?

Would love to hear some thoughts",0,0
MachineLearning,t3_1d7gt44,1717451838.0,/r/MachineLearning/comments/1d7gt44/r_introducing_einspace_a_versatile_search_space/,Research,[R] Introducing einspace: A Versatile Search Space for NAS based on Fundamental Operations,"Dear r/machinelearning friends,

Weâ€™re excited to share our recent work on Neural Architecture Search (NAS) search spaces.

**Introducing einspace**: a flexible and comprehensive search space that integrates convolutional networks (convnets), transformers, and multi-layer perceptrons (MLPs).

Our approach breaks down architectures into four key components:
1. **MLP**
2. **Branch**
3. **Aggregate**
4. **Route**

These components form the 'RGB' genes of architectural design. Our goal is to balance granularity, avoiding the redundancy of reinventing linear layers while maintaining flexibility across different models.

Weâ€™d love for you to explore our work and see how we approach architectural exploration. Your feedback and thoughts would be greatly appreciated.

ğŸ§ ğŸ” [Read the paper](https://arxiv.org/abs/2405.20838)

ğŸ”— [Project page](https://linusericsson.github.io/einspace/)

ğŸ“£ [Original tweet](https://x.com/antreasantonio/status/1797664904100405556?s=46&t=rffiTJwZhIkhJKuYI8-PIQ)


Regards,  
Antreas",0,5
MachineLearning,t3_1d7dd43,1717443466.0,/r/MachineLearning/comments/1d7dd43/d_intuitive_understanding_of_markov_blanket/,Discussion,[D] intuitive understanding of Markov blanket?,"Hi all, I am currently reading Bishop's Pattern Recognition and Machine Learning Chapter 8, and am a bit confused about the Markov blanket in a Bayesian network. 

  
So it says: 

  
> The Markov blanket of a node x comprises the set of parents, children and co-parents of the node. 

  
I do follow the math derivation, but **intuitively** I do not understand why:

  
1. co-parents is part of Markov blanket?

2.  why isn't parents of co-parents part of Markov blanket?  that is, why would information stop at co-parent, not one-level up to parents of the co-parent?

  
Can someone provide an intuitive explanation (would be better with examples)?  Thanks!",1,2
MachineLearning,t3_1d7d26z,1717442765.0,/r/MachineLearning/comments/1d7d26z/p_a_post_on_regularization_properties_of/,Project,[P] A post on regularization properties of polynomial features in machine learning,"I wrote a post about the regularization properties of polynomial features in machine learning - things like bias-variance tradeoff, and controlling the shape of the fit curve.  This is the last post in a series about polynomial features. I certainly enjoyed learning everything I wrote about, and I hope it will be interesting and useful.

Series begins here: [https://alexshtf.github.io/2024/01/21/Bernstein.html](https://alexshtf.github.io/2024/01/21/Bernstein.html)

The latest post here: [https://alexshtf.github.io/2024/06/03/PolynomialBasesRegProps.html](https://alexshtf.github.io/2024/06/03/PolynomialBasesRegProps.html)",0,4
MachineLearning,t3_1d7b72j,1717438299.0,/r/MachineLearning/comments/1d7b72j/hallucination_benchmark_scores_for_llms_discussion/,Discussion,Hallucination benchmark scores for LLMs [Discussion],"For one of my projects, I'm going through benchmark scores of [costlier LLMs](https://epochai.org/data/epochdb/table?show=compute-intensive) (training compute), and so far it seems to me, that very few of them publicly release their scores on hallucination and similar benchmarks (HaluEval, TruthfulQA, etc.)

For example, I found HaluEval scores for only 3 of them. Do they not release those, or am I looking at the wrong places?",0,3
MachineLearning,t3_1d7b4yu,1717438164.0,/r/MachineLearning/comments/1d7b4yu/why_does_validation_metrics_look_so_absurd_p/,Project,Why does validation metrics look so absurd [P] - Multi-class segmentation,"[Validation IoU & F1 score](https://preview.redd.it/c8jo3y34be4d1.png?width=1008&format=png&auto=webp&s=86352e3fb93827bd600238efc5eb03485c525ec5)

[Training Loss & Validation Loss](https://preview.redd.it/r9xgx8j8be4d1.png?width=1001&format=png&auto=webp&s=8facc2ad8afd7f66ae6b19656cbca95d9f426cfe)

I'm performing segmentation on x-rays (using just 25% of data) and training it on a simple UNET for my baseline. 4 classes within. Looking at training/val loss (images attached) it looks like model is learning over time, but eval metrics (both IoU and F1) looks absurd. I don't see any bug in my code, but I have never seen such fluctuating scores.

 Can anyone give any insight on why it might be? Below is my understanding.

1. Due to very small validation dataset (but using a simple model, so unlikely)

2. Is model not learning well? should I have a look at my pipeline again

3. Bug in my eval pipeline.

I know it is difficult to put an opinion without actually looking at data/code. Also any suggestion what other baselines or models I should be trying. There are many transformers-based and unet+mlp arch which claim to be the best in market but none of them have their code public. ",5,6
MachineLearning,t3_1d7ats8,1717437426.0,/r/MachineLearning/comments/1d7ats8/p_text2bricks_finetuning_opensora_in_1000_gpu/,Project,"[P] Text2Bricks: Fine-tuning Open-Sora in 1,000 GPU Hours to make brick animations","Hi all, the research team at Lambda Labs got access to a big cluster of NVIDIA H100 GPUs, and used it to train OpenSora to make brick animations. The team and I are standing by to answer any questions you might have. You can read all the details on our W&B article here:

https://wandb.ai/lambdalabs/lego/reports/Text2Bricks-Fine-tuning-Open-Sora-in-1-000-GPU-Hours--Vmlldzo4MDE3MTky

All of the models are available (linked in the article) and you can even play a fun game we made using the model!

https://albrick-hitchblock.s3.amazonaws.com/index.html",15,46
MachineLearning,t3_1d7af78,1717436429.0,/r/MachineLearning/comments/1d7af78/dllm_interview_qa/,Discussion,[D]LLM interview Q&A,"Hey guys! I'm a data scientist at Amazon Web Services (China). In the past year, I have interviewed for LLM positions at many companies. And I'm planning to compile a series of interview questions, drawing from my own experience in interviews, and provide what I consider to be the right answers. This article will focus onÂ **fine-tuning,**Â and I'll keep it updated.",3,34
MachineLearning,t3_1d7aasl,1717436118.0,/r/MachineLearning/comments/1d7aasl/r_what_are_good_configs_for_running/,Research,[R] What are good configs for running UNet3DConditionModel on 8 GB VRAM? (64x64x64 inputs) ,"What are good configs for running UNet3DConditionModel on 8 GB VRAM? (64x64x64 inputs)

More specifically for this project I'm looking to use HuggingFace's UNet3DConditionModel on my home PC on a RTX 3060 Ti for voxel generation of small (64x64x64) models, but I'm struggling to find a good setup that would fit within the VRAM of my GPU. I doubt the model will be very effective with the space limitations I have so I'll probably swap to another model later, but I'd like to try out a setup with the UNet3DConditionModel first so any recommendations that don't require a more powerful computer?",3,1
MachineLearning,t3_1d7a6l6,1717435821.0,/r/MachineLearning/comments/1d7a6l6/r_transformers_are_ssms_generalized_models_and/,Research,[R] Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality,,15,62
MachineLearning,t3_1d78g7s,1717431582.0,/r/MachineLearning/comments/1d78g7s/c_demand_in_aiml_discussion/,Discussion,C++ demand in AI/ML. [Discussion],"Recently, I've been wondering about a side project to learn cpp so I can implement ml algorithms, hoping I can create something useful from scratch. 

However, I'm really discouraged when thinking about C++ in the AI/ML industry. Is it a thing that can bring value or desired? 

Note: I have been developing programs in pure C since the last year, so learning cpp aint a big deal.",44,27
MachineLearning,t3_1d77xfk,1717430325.0,/r/MachineLearning/comments/1d77xfk/d_can_anyone_access_the_icml_papercheck_now/,Discussion,[D] Can anyone access the icml papercheck now?,"I was doing a final polishing to my paper and trying to submit it to papercheck before submission. However, for some reasons I cannot access the website. Not sure if itâ€™s a technical breakdown or sth else, as the ddl is approaching very soon. Iâ€™m quite concerned now and anxious about failing to submit before the ddl. ",0,2
MachineLearning,t3_1d77925,1717428623.0,/r/MachineLearning/comments/1d77925/opensource_evaluation_testing_framework_for/,Project,Open-Source Evaluation & Testing Framework for Computer Vision Models [P],"Hey,

for the past weeks, weâ€™ve been developing an open-source evaluation and testing framework for computer vision models. Today weâ€™ve released the first alpha version and would love to get your feedback and support.

Github: [https://github.com/moonwatcher-ai/moonwatcher](https://github.com/moonwatcher-ai/moonwatcher)

**What problems are we solving?**

* **Manual, error-prone evaluation:**Â Assessing model quality is still a manual and error-prone process. Of course, aggregation metrics exist, but they usually overlook that the model works differently on some parts of the data.
* **Lack of a single source of truth:**Â Teams struggle to align on AI quality. There are multiple metrics and not all stakeholders understand their meaning and implication. Moreover, manual evaluations from different model versions are stored in Notion, Jira, Google Docs et al which makes it difficult to find reliable data about model quality.
* **Testing for compliance:**Â The AI Act is coming into force in the next months. Becoming compliant requires teams to fully understand the capacities and limitations of their models and document them. One way of doing that is through testing. BY THE WAY: Some companies out there charge an between 100k-300k for a certification. We believe that there needs to be an open-source alternative that ensures a vibrant ecosystem which can release innovative products without paying a fortune.

**Features**

**Open-Source Package**Â ğŸŒ

* **Automated Checks:**Â There is a set of automated checks that you can run. For now, weâ€™ve started with image features such as brightness and saturation. In the future, we plan to develop more complex automation checks based on image content, bounding box size etc.
* **Customizable Checks:**Â Of course you can write your own custom checks.
* **Quick Demos:**Â Weâ€™ve set up demos that help you understand how it all works.

**Web App**

* **Visualize Results:**Â You can visualize the test results and browse relevant images to debug failure cases. In the future, we want to allow non-technical team members to use the app to create tests and align on model results.
* **Share Insights:**Â Non-devs are used to using non-dev tools. We believe that itâ€™s important to establish a common ground where engineers and non-technical stakeholders can communicate to foster a common understanding of model quality.
* **Try the Demo:**Â Log in atÂ [app.moonwatcher.ai/sign-in](http://app.moonwatcher.ai/sign-in)Â with:
   * Email:Â [demo@moonwatcher.ai](mailto:demo@moonwatcher.ai)
   * Password: Demo123!

Check out the repo for more details, and feel free to contribute or leave feedback: [https://github.com/moonwatcher-ai/moonwatcher](https://github.com/moonwatcher-ai/moonwatcher)

Reach out atÂ [hello@moonwatcher.ai](mailto:hello@moonwatcher.ai)Â for questions, support, or collaboration. Looking forward to your feedback and suggestions!Â ğŸŒš",0,2
MachineLearning,t3_1d7647x,1717425743.0,/r/MachineLearning/comments/1d7647x/d_mobilenetv3_image_classification/,Discussion,[D] MobileNetv3 Image Classification,Good day. Anyone here ever made a image classification using mobilenetv3 instead of mobilenetv2? I have a hard time training my models since I'm making it from scratch and lack knowledge on where to start. Any links or tutorial on where I can follow and improve? and any tips? Thank you.,0,1
MachineLearning,t3_1d755gi,1717423262.0,/r/MachineLearning/comments/1d755gi/d_pinns_for_ductile_fracture_of_metals/,Discussion,[D] PINNs for ductile fracture of metals,"Hi, I'm wondering if ductile fracture of metals available in FE software abaqus can be modelled using PINNs? these models are generally built after testing grooved coupons which result in different triaxility levels. This requires lots of testing and modelling of each coupon to find the relationship between fracture strain and triaxility... what would be the physics to include in the PINNs?",0,3
MachineLearning,t3_1d72uvt,1717416593.0,/r/MachineLearning/comments/1d72uvt/d_build_data_products_with_snowflake_part_1/,Discussion,[D] Build Data Products With Snowflake | Part 1: Leveraging Existing Stacks ,"Optimising Snowflake Cost, Integrating Snowflake Sources, and Driving Faster Business Results!

In this series, we want to highlight the ease of leveraging your existing stack to get going with Data Products. This piece is ideally for data leaders who want to adopt the data product approach while staying rooted in big investments like Snowflake, Dbt, Databricks, or Tableau. Weâ€™ll kick this off with a favourite: Snowflake!

Read the complete article here: [https://moderndata101.substack.com/p/build-data-products-with-snowflake](https://moderndata101.substack.com/p/build-data-products-with-snowflake)",0,0
MachineLearning,t3_1d72jdd,1717415558.0,/r/MachineLearning/comments/1d72jdd/discussion_best_aiml_conferences_to_buy_tickets/,Discussion,"[Discussion] Best AI/ML Conferences to Buy Tickets for (July-November 2024)
","Hello everyone! Iâ€™m based in Norway and looking to buy tickets for AI/ML or general tech conferences scheduled between July and November 2024.   
  
Iâ€™m particularly interested in learning about new ML technologies and focusing on setting up ML in production environments. My concern is attending events that might be overly focused on Large Language Models (LLMs). I'm eager to find conferences that cover a broad spectrum of practical ML applications.   
  
Could anyone recommend events during this period that would align with these interests?   
Thanks in advance!",2,3
MachineLearning,t3_1d729sm,1717414640.0,/r/MachineLearning/comments/1d729sm/d_whisper_local_llm_xtts_streaming/,Discussion,[D] Whisper + Local LLM + xtts Streaming,Any good project which does this?,0,0
MachineLearning,t3_1d6zoz1,1717403949.0,/r/MachineLearning/comments/1d6zoz1/d_best_ai_conferences_for_game_theorymatching/,Discussion,[D] Best AI Conferences for Game Theory/Matching Theory/Multi-agent Systems?,"Hi,

I believe I have a very strong paper I want to submit to a prestigious conference in AI. My field is mostly Matching Theory/Computational Game Theory/ Multi-agent Systems stuff (e.g. stable matchings, if anyone is familiar with it). What do you think is the most prestigious relevant conference? I am thinking about IJCAI, AAMAS, or EC. AAAI and ICML may also be in the picture, but as far as I have seen, papers in my field rarely get published there, so this may be too far out of topic for them. Thanks for any help :)",5,12
MachineLearning,t3_1d6xocn,1717395084.0,/r/MachineLearning/comments/1d6xocn/d_would_you_say_meta_imagebind_is_better_than/,Discussion,[D] Would you say Meta ImageBind is better than CLIP for multivector embeddings?,"I was looking into multimodal embedding models and came across Imagebind. It seems to be interesting, but I couldn't find many reviews or benchmarks on how it fares against CLIP. I've read that Meta has just improved CLIP or extended it.

Anyone who works in the space who has used both of these?",0,3
MachineLearning,t3_1d6tpyj,1717380879.0,/r/MachineLearning/comments/1d6tpyj/d_transformer_keys_queries_values_intuitions/,Discussion,"[D]: Transformer Keys, Queries, Values Intuitions outside NLP ","Usually `K = V`, and if `Q =/= K` then it's cross attention, otherwise it is self attention. Transformer blocks basically enrich V (value) vectors with context after each block.

In my case I have Q =/= K =/= V, and while mathematically fine, I haven't come across an application that did this. 

I want a behavior that when K = Q, then V_new = Transformer (Q, K, V) = V, since Q matches K. With usual scaled-dot attention this is not case, but i guess this could be overcomed by the other components such as the MLPs.

My questions are then:
1. Is there a modified attention (other that computationally expensive distance matrix) attention that can return V when K=Q
2. When K=V, then it's trivial how to apply transformer blocks repeatedly since `V_new = Transformer(Q, V, V)` at every time step. But if `K=/=V`, after `V_new = Transformer(Q,K,V)` it is not clear if I should do `V_new_new = Transformer(Q,K, V_new)`


Thanks for the discussion. I would like to know if you have some experience dealing with a similar situation and how it went.",4,4
MachineLearning,t3_1d6s7au,1717376070.0,/r/MachineLearning/comments/1d6s7au/d_is_there_any_way_to_perform_encoding_a_bit/,Discussion,[D] Is there any way to perform encoding a bit faster when creating FAISS indexes?,"I'm currently training a text embedding model that I'm evaluating using benchmarks like MTEB or MIRACL. Most of the code that I've referenced is using FAISS indexes to search results, which makes sense.

The problem is that when building FAISS indexes, the encoding of text is taking way too long. I'm currently using a single machine with four A6000 GPU devices and have implemented data parallelism to perform distributed inference, but even then it's taking around 8 hours to get the embedding vectors for around 1.4M documents.

This means that my typical workflow of evaluating after every epoch is becoming a bit less feasible.

I've considered coming up with other methods like using a smaller corpus for intermediate evaluation, but I don't really want to be doing that.

Is there another way that I could be doing this in a faster way? Thanks.",6,10
MachineLearning,t3_1d6qfbc,1717370640.0,/r/MachineLearning/comments/1d6qfbc/discussion_why_next_token_prediction_doesnt_work/,Discussion,[Discussion] Why next token prediction doesn't work for Recommender System? (or am I wrong?),"I'm working on a research project that aims at applying next-token-prediction models to build/improve recommender systems. As a feasibility assessment study, I built and trained a GPT model to predict the next product to buy using the Instacart dataset. To be more specific, I treated each product\_id as a ""word"", each order as a ""sentence"" and each user's transaction history as a ""document"".

However, after 4 hours of training on a T4 GPU, the mean average precision at 10 (MAP@10) on the eval set is still only 0.075. For comparison, MAP@10 for the baseline popular product (top 10 most popular products in the user's personal transaction history) is already 0.251.

Although I can see one or two ways to improve the model, the low performance compared to a very simple baseline is really discouraging and makes me think this approach is not feasible at all. I would like to discuss a few points:

1. Is it true that next-token prediction (specifically decoder-only transformer architectures) doesn't work for this problem at any scale?
- Support: There are a lot of differences between the text data and the e-commerce transaction data, so a method that works for text might not work for transactions, it's no surprise.
- Against: 4 hours of training is only 1.5 epochs, so the current model may be underfitted. Therefore, the low performance could just be a function of training time and it might improve if I train for 2 days

2. Any resources I should read into this? I'm aware that the approach I'm taking is similar to session-based recsys models, but I only found one paper HierTCN (You et al., WWW 2019). Would appreciate any additional suggestions to read into this.

3. Any suggestion to help train the model better/faster? The current configuration is:
- Data: vocab\_size = 50000 (50K products), 200K users (each epoch is 200K training examples)
- Model: n\_layer=9, d\_model=512, n\_head=16 (similar to Gopher 44M-parameter model), block\_size=1024
- Training: batch\_size = 4, learning\_rate = 5e-4, optimizer = AdamW

Thank you!",27,31
MachineLearning,t3_1d6pqma,1717368702.0,/r/MachineLearning/comments/1d6pqma/d_an_open_source_model_that_creates_2d_image_to/,Discussion,[D] An Open Source model that creates 2D Image to 3D video? ,"I'm currently searching for an open Source model that can create a short 3D video out of a 2D image. The video would just kind of show a zoom in zoom out or something like that, like for example with Immersity Ai, which unfortunately cost quite a lot of money.

Does somebody know anything thats free, it would be best if its an open source model. I have found Owl3d that can generate single images with different perspectives, but its also not entirely free, especially not for commercial usage.",1,0
MachineLearning,t3_1d6p2ej,1717366811.0,/r/MachineLearning/comments/1d6p2ej/r_the_challenges_of_building_effective_llm/,Research,[R] The Challenges of Building Effective LLM Benchmarks: A 5 minute deep-dive ğŸ§ ,"With the field moving fast and models being released every day, there's a need for comprehensive benchmarks. With trustworthy evaluation you and I can know which LLM to choose for our task: coding, instruction following, translation, problem solving, etc.

TL;DR: The article dives into the challenges of evaluating large language models (LLMs). ğŸ” From data leakage to memorization issues, discover the gaps and proposed improvements for more comprehensive leaderboards.

[A deep dive into state-of-the-art methods and how we can better evaluate LLM performance](https://codecompass00.substack.com/p/llm-evaluation-leaderboards)

*Processing img tu0kj4be554d1...*",0,5
MachineLearning,t3_1d6m22k,1717358758.0,/r/MachineLearning/comments/1d6m22k/my_opinion_on_the_current_and_future_state_of/,Discussion,"My opinion on the current and future state of AI.""[Discussion]""",,0,0
MachineLearning,t3_1d6lbp2,1717356821.0,/r/MachineLearning/comments/1d6lbp2/d_pretrained_embedding_model_for_search_in/,Discussion,[D] Pretrained embedding model for search in scientific documents,"Hello ! 

I'm working on an app where I would need a way to search keywords in a set of scientific papers (i.e. find the documents that speak about a query). I need to handle synonyms, errors etc.

I thought that the best idea would be a vector search. But I don't really know what is the current SOTA for that. I know SBERT, but I'm not sure it's the best for that ? 

Also, the best would be if a model could be pretrained in multilingual, and on scientific literature...

Do you have an idea for me ? 

Maybe I'm not on the right path ? 

Thank you in advance !",5,5
MachineLearning,t3_1d6idf5,1717349121.0,/r/MachineLearning/comments/1d6idf5/p_baysian_bandits_item_pricing_in_a_moonlighter/,Project,[P] Baysian bandits item pricing in a Moonlighter shop simulation,"I built a toy shop, modeled after the Moonlighter game's, and a Bayesian bandits agent to choose and price items for sale via Thompson sampling. 

Customer reactions (i.e. 'angry', 'sad', 'content', 'ecstactic') to these items at their shelf prices updated ideal (i.e. highest) price probability distributions (i.e. posteriors) as the simulation progressed. 

The algorithm explored the ideal prices of items and quickly found groups of items with the highest ideal price at the time, which it then sold off. This process continued until all items were sold.

The graph represents competitions between items to be placed on a shelf for sale.

The dots are prices (y-values) sampled from ideal price distributions (i.e. posteriors) for each item (color) in each competition (x-value). 

The highest sampled price, in each Thompson sampling contest, won and ended up on a shelf. 

The customer reactions I mentioned to items on the shelves updated bounds of these distributions, represented by lines of the same color.

For more information, many more graphs, and the link to the corresponding Github repo containing working code and a Jupyter notebook with Pandas/Matplotlib code to generate the plots, see my write-up: https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rMachineLearning",0,16
MachineLearning,t3_1d6hagr,1717346163.0,/r/MachineLearning/comments/1d6hagr/d_the_dilemma_of_taking_notes_on_every_ml/,Discussion,[D] The Dilemma of Taking Notes on Every ML Resource or Accepting Knowledge Loss Over Time,"I know it may come as a weird topic but I still think this is an important discussion since we're constantly learning in this field. 

Machine Learning is an expansive field, deeply intertwined with numerous other disciplines. My master's degree alone covers topics such as statistics, optimization, inverse data simulation, MLOps, software engineering, agent-based modeling, semantic web, deep learning, time series... Each of these areas has its own subfields that one could dedicate their entire lifetime to explore.

I have come to realize that unless you practice a subject daily, the knowledge you acquire from books, certifications, articles, papers, podcasts, and videos on a topic will eventually fade away. This realization led me to discover Obsidian four years ago, which has significantly changed how I consume and retain information. I now take notes on everything I consume, especially on topics that interest me outside of my job. Much like a ""second brain"". Without this practice, I find that the information quickly slips away.

Indeed, I have spent countless hours engaging with content on physics, history, epistemology, philosophy, and many other subjects. However, only a fraction of what I once knew has endured. This brings me to a dilemma: should I invest a substantial amount of time capturing every resource in my knowledge system ensuring that I can carry it over time, or consume resources as quickly as they'll fade away ()""for fun"" or when my time is limited)?

I don't want to make this post overly long, but I genuinely feel the benefits of spending time processing information when reading a book, for example. Organizing and connecting knowledge at scale is often challenging but also rewarding, as it helps build a deep understanding of a subject. Additionally, when you need to refresh your memory, the ""cost"" is much lower if you have already done this ""pre-processing"" work rather than going over the internet / books again. I'm not simply copy/pasting text, but tailoring what I capture depending on what I already know about a subject.

However, there is so much to learn in this field, even the fundamentals like mathematics or statistics. I sometimes question whether this approach is sustainable. For instance, the book ""Machine Learning with PyTorch and Scikit-Learn"" by Sebastian Raschka and others is 700 pages long. Imagine the time it takes to capture every piece of information from such a comprehensive book (and that's only one!). Taking notes also forces you to understand the material thoroughly, including every equation, or else the notes are useless.

I'm not advocating for a binary approach; I often find compromises. But I am curious about your approach to learning and consuming information. How do you balance the need to retain knowledge with the practical constraints of time and effort?",12,53
MachineLearning,t3_1d6ggwv,1717343933.0,/r/MachineLearning/comments/1d6ggwv/r_metaearth_a_generative_foundation_model_for/,Research,[R] MetaEarth - A Generative Foundation Model for Global-Scale Remote Sensing Image Generation,,2,10
MachineLearning,t3_1d6f7ad,1717340419.0,/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/,Discussion,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",3,4
MachineLearning,t3_1d6emwi,1717338796.0,/r/MachineLearning/comments/1d6emwi/if_llms_are_tokenbased_autoregressive_models_how/,Discussion,"If LLMs are token-based autoregressive models, how do they generate images? (Transformers + VQVAE) [D]",Sharing a video from my YT channel talking about how certain multimodal LLMs like Gemini generate images as a sequence of learnable image tokens.,0,11
MachineLearning,t3_1d6ct6w,1717333287.0,/r/MachineLearning/comments/1d6ct6w/discussion_are_people_interested_in_creating_a/,Discussion,[Discussion] Are people interested in creating a mid-tier GPU rig using two RTX A6000's joined via NVLINK?,"[https://store.nvidia.com/en-us/nvidia-rtx/products/nvidia-rtx-a6000/](https://store.nvidia.com/en-us/nvidia-rtx/products/nvidia-rtx-a6000/)

This would provide 96 GB memory size

Due to costs, I would like to take advantage of NVIDIA's higher ed and research grant program

[https://developer.nvidia.com/higher-education-and-research](https://developer.nvidia.com/higher-education-and-research)",10,12
MachineLearning,t3_1d6cq0n,1717333009.0,/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/,Research,[Research] Tangles: a new mathematical ML tool in book announced by Diestel,"Hey guys, I would like to share a new book that might be interesting to the community!

Graph theorist Diestel has written a book addressing the ML community (and others):  


Tangles: A structural approach to artificial intelligence in the empirical sciences  
Reinhard Diestel, Cambridge University Press 2024  


\-----

Publisher's blurb:  


Tangles offer a precise way to identify structure in imprecise data. By grouping qualities that often occur together, they not only reveal clusters of things but also types of their qualities: types of political views, of texts, of health conditions, or of proteins. Tangles offer a new, structural, approach to artificial intelligence that can help us understand, classify, and predict complex phenomena.  


This has become possible by the recent axiomatization of the mathematical theory of tangles, which has made it applicable far beyond its origin in graph theory: from clustering in data science and machine learning to predicting customer behaviour in economics; from DNA sequencing and drug development to text and image analysis.  


Such applications are explored here for the first time. Assuming only basic undergraduate mathematics, the theory of tangles and its potential implications are made accessible to scientists, computer scientists and social scientists. 

\-----

Ebook, plus open-source software including tutorials, can be found on tangles-book.com.  


Note: This is an 'outreach' book not primarily about tangle theory, but about applying tangles in a multitude of unexpected ways and areas. Tangles in graphs are covered in Diestel's Graph Theory, 5th ed'n.  


Table of Contents and an introduction for data scientists (Ch.1.2), are available from tangles-book.com/book/details/ and from arXiv:2006.01830. Chapters 6 and 14 are about a new method of soft clustering based on tangles, very different from traditional methods. Chapters 7-9 cover the theory needed for Chapter 14.  


The software part of tangles-book.com say they invite collaboration on concrete projects, as well as contributions to their GitHub software library.  


https://preview.redd.it/ysj91dw2o54d1.png?width=2074&format=png&auto=webp&s=dd7ea6c2671ef83a5be77739e9ed6e3d6169c1d2

&#x200B;

&#x200B;",22,61
MachineLearning,t3_1d6c5ox,1717331110.0,/r/MachineLearning/comments/1d6c5ox/d_emnlp_anonymity_policy/,Discussion,[D] EMNLP anonymity policy,"In January of this year, the ACL updated their [anonymity policy](https://www.aclweb.org/adminwiki/index.php/ACL_Anonymity_Policy), based on the recommendations of the working group report, which states:

>We highlight that submission and reviewing should remain double-blind and the submitted papers should be fully anonymous. Recognizing that there are many trade-offs involved, and after conducting a community-wide survey and considering numerous options (listed in the section â€œProposals Consideredâ€ near the end of this document), we recommend the following: (a) change ACL policies to now allow both anonymous and non-anonymous pre-printing at any time, so that technical conversations about work may be had without restriction. (b) make it explicit that discussion of unpublished work on all media, including social media, is also allowed, but that extensive promotion and PR are discouraged (though not forbidden).

This might be an obvious question, but I've never submitted to an ACL conference before and don't want to mess it up: does this mean we can post non-anonymous preprints to Arxiv at any time before, during, or after submission, without the risk of being desk rejected?",2,4
MachineLearning,t3_1d6b1z4,1717327064.0,/r/MachineLearning/comments/1d6b1z4/d_a_survey_of_pu_learning/,Discussion,[D] A survey of PU learning,"[main](https://preview.redd.it/7jyyc219854d1.jpg?width=1700&format=pjpg&auto=webp&s=249274b001ab0c1d5e3bc9f903a08d068a7c79a7)

[html version](https://hkustconnect-my.sharepoint.com/:u:/g/personal/jwangiy_connect_ust_hk/EXSwAvVS0qZCrv1HM-13jNoB3EcrbUP1fX2zjv7-DYU10Q?e=g9Q2wP)

[sample1](https://hkustconnect-my.sharepoint.com/:u:/g/personal/jwangiy_connect_ust_hk/EQE89aHr0BJDqMlIz64P9ugBxH4LRx7bcS_2m40vgFdwoQ?e=5Sr4eR)

[paper1](https://doi.org/10.1007/s10994-020-05877-5)

[paper2](https://arxiv.org/abs/1809.03207)",0,1
MachineLearning,t3_1d69qpo,1717321585.0,/r/MachineLearning/comments/1d69qpo/project_ğ‹ğ‹ğŒ_ğ ğğ§ğğ«ğšğ­ğ¢ğ§ğ _ğŒğ®ğ¬ğ¢ğœ/,Project,[Project] ğ‹ğ‹ğŒ ğ ğğ§ğğ«ğšğ­ğ¢ğ§ğ  ğŒğ®ğ¬ğ¢ğœ,"This article presents a method of music generation by fine-tuning LLM.

[https://medium.com/@yassin.moakher/fine-tuning-llm-for-music-generation-480ca97bf22d](https://medium.com/@yassin.moakher/fine-tuning-llm-for-music-generation-480ca97bf22d)",0,0
MachineLearning,t3_1d68jjf,1717316259.0,/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/,Research,[R] Tech report on FineWeb: decanting the web for the finest text data at scale,"The team behind the large FineWeb 15 trillions openly released web-scale dataset have just published an extensive blog post on the science of creating high quality web-scale datasets, detailing the steps and learnings that came in FineWeb, in a sort of [distill.pub](https://distill.pub) interactive article/blog fashion.

They also released FineWeb-Edu a filtered subset of Common Crawl with 1.3T tokens focusing on web pages with very high educational content and which seem to out-performs all openly release web-scale datasets on knowledge- and reasoning-intensive benchmarks like MMLU, ARC, and OpenBookQA

Interesting read: [https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)  
",2,31
MachineLearning,t3_1d67ydm,1717313639.0,/r/MachineLearning/comments/1d67ydm/r_llm_merging_competition_building_llms/,Research,[R] LLM Merging Competition: Building LLMs Efficiently through Merging (NeurIPS 2024 Challenge),"Website of NeurIPS 2024 Challenge: https://llm-merging.github.io/

Discord: https://discord.gg/dPBHEVnV

# LLM Merging Competition: Building LLMs Efficiently through Merging

*NeurIPS 2024 Challenge*

## Aims and Focus

Training high-performing large language models (LLMs) from scratch is a notoriously expensive and difficult task, costing hundreds of millions of dollars in compute alone. These pretrained LLMs, however, can cheaply and easily be adapted to new tasks via fine-tuning, leading to a proliferation of models that suit specific use cases. Recent work has shown that specialized fine-tuned models can be rapidly **merged** to combine capabilities and generalize to new skills.

## Get started

The competition will provide the participants with a list of expert models that have already been trained on a task-specific dataset. All of these models will be publicly available on the Hugging Face Model Hub with licenses that permit their use for research purposes. These models can either be fully fine-tuned models or models obtained by parameter-efficient fine-tuning methods such as LoRA. Models on this list will be required to satisfy the following criteria: (1) model size <= 8B parameters, and (2) model with licenses compatible with research use (e.b., MIT, Apache 2 etc).

A starter kit with an end-to-end submission flow can be found here:
https://github.com/llm-merging/LLM-Merging",1,4
MachineLearning,t3_1d67d7j,1717311183.0,/r/MachineLearning/comments/1d67d7j/r_different_between_iql_and_vdn_for_multi_agent/,Research,[R] Different between IQL and VDN for multi agent reinforcement learning,"Hi everyone,  
I am doing research about cooperative multi agent in RL. I has implement Independent Q Learning using DQN (Deep Q Network) for each agent in [Cooperative Pong](https://pettingzoo.farama.org/environments/butterfly/cooperative_pong/) env in PettingZoo. Now i am reading about [VDN ](https://arxiv.org/pdf/1706.05296)(Value Decompose Network) and i have a question.  
In this paper, they said that the join value Q function is sum of sub Q function of each agent. The idea is to maximize local Q function of each agent which equivalent to maximize global Q function. But the IQL is also maximize to local Q function of each agent to. So what is the different between two these ?

https://preview.redd.it/e3gvpcotv34d1.png?width=626&format=png&auto=webp&s=07672f79fee8fbaa660abb0bcfb28c24de01ba23",0,3
MachineLearning,t3_1d65y1h,1717305416.0,/r/MachineLearning/comments/1d65y1h/d_teacher_student_training_strategy/,Discussion,[D] Teacher student training strategy,"I am planning on using a LLM (say llama3) to extract training data via a prompt, and then using a smaller model with a CLS token to do a custom training to try and match the accuracy of the LLM. Suppose that I can run the prompt on 1M+ data (although I suspect I won't need as many).

**Prompt: Does the following sentence contain apples or oranges:** Examples:

* ""<prompt> apples, oranges"" -> apples, oranges
* ""<prompt> apple, orange"" -> apples, oranges
* ""<prompt> apples, no oranges"" -> apples

So my questions are:

1. The last CLS type LLM I have seen is microsoft's [xtremedistil](https://huggingface.co/microsoft/xtremedistil-l6-h384-uncased). Are these models still being used? If so what is the latest + greatest?
2. Would it be better to use a sentence transformer and do classification?
3. In my training set for the student model, I will remove the prompt from above, is there a risk of this method?

The way I see it, in the long run these models are smaller and will cost far less. Would appreciate any thoughts in general.",1,5
MachineLearning,t3_1d65vj7,1717305134.0,/r/MachineLearning/comments/1d65vj7/d_what_are_your_realworld_production_use_cases/,Discussion,[D] What are your real-world production use cases for LLMs?,"I think we should share more production use cases for LLMs instead of just theoretical best practices.

Can you share the use cases you've seen/built in production? It should include the following details:

1. The problem it solves
2. The implementation details (models, infrastructure, etc.)
3. The business impact it had",18,41
MachineLearning,t3_1d65gdt,1717303455.0,/r/MachineLearning/comments/1d65gdt/d_is_it_a_good_idea_to_combine_3_datasets_into/,Discussion,"[D] Is it a good idea to combine 3 datasets into one unique dataset, knowing that the 3 are related to the same topic? ","They are basically of the same topic and have the same labels, the only difference is the dataset per se. It's for the goal of differentiating the images of the first dataset (which would be morphed into one dataset from the other three) from the other dataset that it'll be created from scratch in conjunction with my research colleagues",3,1
MachineLearning,t3_1d64lx8,1717300245.0,/r/MachineLearning/comments/1d64lx8/implementing_scaling_monosemanticity_extracting/,Discussion,"Implementing ""Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"" paper for open source models.[D]","I recently came across an interesting paper titled ""Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"" which explores using sparse autoencoders to extract interpretable features from the activations of a large language model. The methodology seems promising for gaining insights into the model's internal representations and behaviors.

It got me thinking about the feasibility of implementing similar interpretability techniques for open-source language models. Can we steer LLMs and their behaviour without having to extensively finetune.

I wanted to reach out to this community to discuss a few things:

1. Has anyone already implemented or experimented with similar interpretability techniques on open-source language models? Can we make something similar to that of golden gate claude??
2. Do you think it's feasible to adapt and scale these techniques to work with Llama,phi,mistral etc. These are much smaller in parameters size, when compared to sonnet.
3. I'm interested in collaborating with others who are passionate about this area of research. If you're working on interpretability for open-source models or have ideas for novel approaches, I would be excited to team up and explore this further. We could collaborate on implementing techniques, sharing resources, or brainstorming new ideas.

If you're interested in collaborating or have any ideas to share, please feel free to share.

",6,32
MachineLearning,t3_1d5y0oo,1717279225.0,/r/MachineLearning/comments/1d5y0oo/d_alternatives_to_the_laion_aesthetics_dataset/,Discussion,[D] Alternatives to the LAION Aesthetics dataset?,"So the LAION dataset is currently closed for a safety review. In the meantime, are there any large datasets that could stand in place for their aesthetics subset? I'm looking for ""aesthetically pleasing"" images, like artwork, drawing, nice photos, etc.",0,2
MachineLearning,t3_1d5u95z,1717268738.0,/r/MachineLearning/comments/1d5u95z/r_cope_contextual_position_encoding_learning_to/,Research,[R] CoPE: Contextual Position Encoding: Learning to Count What's Important,,11,81
MachineLearning,t3_1d5taip,1717266117.0,/r/MachineLearning/comments/1d5taip/d_tensttorrent_galaxy_server_usecases/,Discussion,[D] Tensttorrent Galaxy Server usecases ,"Hi all,

  
First post so please be gentle :)

I have an opportunity for longterm borrow of a Tenstorrent Galaxy server with 32 wormhoole processors for small amount of money.

We are a small company with skillfull enginneers but we didn't have much time to go into depth on the AI. We do basic stuff, like using of the shelf models and running inference with some light finetuning (mostly YOLO) as that is not our core bussiness. However, my personal wish is to slowly pivot into the AI space so my question is, if you have a similar chance would you accept it and to use it for what?  I know it's a  broad question so be free to be creative :)

On github (https://github.com/tenstorrent/tt-buda-demos/tree/main/model\_demos) they already have a list of model demos but it's just inference, don't see mentions of fine-tuning or training",0,4
MachineLearning,t3_1d5s9g4,1717263313.0,/r/MachineLearning/comments/1d5s9g4/p_superfast_rag_langchain_streaming_and_groq/,Project,[P] Superfast RAG: Langchain Streaming and Groq,"  
Fast LLM RAG inference using Groq and Langchain Streaming.  
  
Groq is introducing a new, simpler processing architecture designed specifically for the performance requirements of machine learning applications and other compute-intensive workloads. The simpler hardware also saves developer resources by eliminating the need for profiling, and also makes it easier to deploy AI solutions at scale.  
  
Resource: [https://www.youtube.com/watch?v=frMdOL8knqg](https://www.youtube.com/watch?v=frMdOL8knqg)",0,0
MachineLearning,t3_1d5o84z,1717252179.0,/r/MachineLearning/comments/1d5o84z/texttoimage_generative_diffusion_models_explained/,Discussion,Text-To-Image Generative Diffusion Models explained in 15 MUST-KNOW concepts! (+ How to code it) [D],Sharing a video I made recently talking about conditional LDMs and the lessons I learned while implementing a text to image LDM on the celeba dataset to generate human faces.,0,0
MachineLearning,t3_1d5nmlh,1717250456.0,/r/MachineLearning/comments/1d5nmlh/p_a_visual_guide_to_gnn_sampling_using_pytorch/,Project,[P] A Visual Guide to GNN Sampling using PyTorch Geometric,"Hi everyone,

I posted a couple of videos and notebooks on the topic of ""Sampling for  
Graph Neural Networks (GNNs)"". The original GCN paper used full batch  
training. Then, researchers have used different methods to create mini batches  
(subgraphs) to train the GCN. For example, GraphSAGE paper used a neighbor  
sampler, and ClusterGCN paper used a cluster sampler. These samplers are  
implemented in pytorch-geometric under torch\_geometric.loader.

Here are the videos, or you can jump right into the code..

=============================================

Sampling  
for Graph Neural Networks

[video](https://youtu.be/y0poBC8xN1k)

[code](https://github.com/mashaan14/YouTube-channel/blob/main/notebooks/2024_05_27_GNN_sampling.ipynb)

=============================================

Mini  
Batches in Graph Neural Network video

[video](https://youtu.be/ZfCtFLS_os0)

[code](https://github.com/mashaan14/YouTube-channel/blob/main/notebooks/2024_04_29_GNN_mini_batch.ipynb)

[Original graph](https://preview.redd.it/z08yjik3vy3d1.png?width=640&format=png&auto=webp&s=1420e8c9917be244af815e69d5f998e6211411d3)



[Three sampled subgraphs using NeighborLoader from pytorch\_geometric; nodes in white were not sampled.](https://preview.redd.it/cwr5bjh5vy3d1.png?width=1647&format=png&auto=webp&s=c59a5c2a4611fe4d5b4369ca4709b89bdfa06cc2)

",1,16
MachineLearning,t3_1d5laih,1717242788.0,/r/MachineLearning/comments/1d5laih/p_the_deep_biome_experiment/,Project,[P] The Deep Biome experiment,"Hello,

I'm working on the concept of Deep Biome, which I'm trying to describe in this document:Â [https://docs.google.com/document/d/e/2PACX-1vR1bvKTV94WRUyz-xfqPkV0TQjrHgE-4reCdP2Ncjgxv4CN8CVhEmcYb\_b7qC2lv\_HK9vjZd9yv57-Z/pub](https://docs.google.com/document/d/e/2PACX-1vR1bvKTV94WRUyz-xfqPkV0TQjrHgE-4reCdP2Ncjgxv4CN8CVhEmcYb_b7qC2lv_HK9vjZd9yv57-Z/pub)

The summary is: create a virtual biome where every agent has a ""DNA"" representing the structure of its neural network, with the capacity of reproduction with other agents.

I already created an example on GitHub of the DeepDNA, where two DNAs are created and then merged to generate a child's DNA.

I'm looking for collaborators to develop efficiently the theory (and the implementation) and that can help me to solve some conceptual bottlenecks, like the training of an agent model or the structure of the sequences in case of complex layers like LSTM.

I appreciate the collaboration with a university and the ambition to publish the paper as a research.

The python tests are at this repo: [https://github.com/cekkr/DeepGenome](https://github.com/cekkr/DeepGenome) 

At this moment I'm waiting for ""stimuli"" to move forward.

Thank you,

Riccardo Cecchini

",0,0
MachineLearning,t3_1d5l0he,1717241755.0,/r/MachineLearning/comments/1d5l0he/d_help_to_increase_the_acc_of_bert_with_neural/,Discussion,[D] Help to increase the acc of BERT with Neural Networks,"Hello, I'm new in ML, i'm struggled to improve the acc of my BERT-CNN, BERT-LSTM, BERT-GRU.

this is my repository [Sentiment Analysis BERT-CNN](https://github.com/jveko/sentiment-analysis-bert-cnn)

Dataset i use is [Kaggle](https://www.kaggle.com/datasets/anggapurnama/twitter-dataset-ppkm) the dataset is imbalance, so i need to takeout some of neutral sentiment fraction to 0.78 to get the 4k of neutral sentiment

My acc right now is

-> BERT-CNN

Training Summary:  
Best Training Loss: 0.0129 (Epoch 10)  
Best Validation Accuracy: 81.55% (Epoch 7)

-> BERT-LSTM

Training Summary:  
Best Training Loss: 0.0815 (Epoch 10)  
Best Validation Accuracy: 81.14% (Epoch 7)

-> BERT-GRU

Training Summary:  
Best Training Loss: 0.0815 (Epoch 10)  
Best Validation Accuracy: 81.14% (Epoch 7)

[Chart](https://imgur.com/a/NH7vEdz)

I do research from this [paper](https://www.researchgate.net/publication/358280927_A_Novel_Hybrid_Network_for_Arabic_Sentiment_Analysis_using_fine-tuned_AraBERT_model) as reference of accuracy that not

Is there any problem from my code?

I have no idea i already change the different hyperparameter still not significant for the acc. and why my models is not good like the imrpove of acc is not incremental, but sometimes is drop?

Thank you in advance, i hope you find interesting in the discussion :)",3,1
MachineLearning,t3_1d5kov5,1717240504.0,/r/MachineLearning/comments/1d5kov5/d_is_mojo_worth_it_or_which_second_language_would/,Discussion,[D] Is Mojo worth it or which second language would you learn for ML?,"Basically the title. I am quite proficient with python (as expected) but beside that, I have only very rudimentary knowledge of javascript and C++. I want to learn a second language that is more ""low level"" and can leverage the hardware features better. My goal is not to rewrite pytorch or replace python entirely (although [porting the inference to mojo may make sense](https://github.com/tairov/llama2.mojo)), but provide alternatives for performance critical use cases.

Looking at today's situation, the obvious answer is C++. However, Rust has been gaining popularity and besides the steep learning curve, people are starting to put it above C++ in many aspects. In both cases, the syntax and languages are not very close to python, which makes them difficult to learn. Mojo seems much better in this respect, providing both low level features where the syntax somehow resembles Rust (at least for a layman like me) but also can be used as weird python flavour. It even allows importing python libraries directly. This is very helpful for such a young language, where a large community and a variety of libraries are missing. Nevertheless, the language is still young and very much subject to change so I am not sure if I should invest.

So what do you think is the best ""second"" language for the use cases above? Any experience with Mojo? How did you learn it or any other language with a limited number of resources. I am planning to read through the docs and solve last years advent of code using it if I go with Mojo.",87,53
MachineLearning,t3_1d5hh86,1717226470.0,/r/MachineLearning/comments/1d5hh86/r_is_incontext_learning_sufficient_for/,Research,[R] Is In-Context Learning Sufficient for Instruction Following in LLMs?,"In-context learning (ICL) allows LLMs to learn from examples without changing their weights, which is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on established benchmarks such as MT-Bench and AlpacaEval 2.0 (LC), especially with more capable base LMs. Unlike for tasks such as classification, translation, or summarization, adding more ICL demonstrations for long-context LLMs does not systematically improve instruction following performance. To address this limitation, we derive a greedy selection approach for ICL examples that noticeably improves performance, yet without bridging the gap to instruction fine-tuning. Finally, we provide a series of ablation studies to better understand the reasons behind the remaining gap, and we show how some aspects of ICL depart from the existing knowledge and are specific to the instruction tuning setting. Overall, our work advances the understanding of ICL as an alignment technique.",1,2
MachineLearning,t3_1d5hh0s,1717226446.0,/r/MachineLearning/comments/1d5hh0s/p_detikzify_synthesizing_graphics_programs_for/,Project,[P] DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ,,2,45
MachineLearning,t3_1d5ei7s,1717214957.0,/r/MachineLearning/comments/1d5ei7s/p_automated_lora_discovery/,Project,[P] Automated LoRA Discovery,"I put together a suite of methods centered around methods that eitherÂ 

1. Constrain the search space when training LoRAsÂ 
2. Use generative models to explore the manifold of plausible LoRAs

A full writeup can be found [here](https://sweet-hall-e72.notion.site/Automated-LoRA-Discovery-and-Teaching-Neural-Networks-to-make-Neural-Networks-22aa3b5ad66e4bc985ff2c93896538d2) along with links to code, models, and datasets, but I'll give an overview of the main points here as well.

LoRAs and other adapter methods allow us to cut down on needed trainable parameters by a lot, but we may be able to cut down much much further if we have a prior over the kind of results we want. Additionally, LoRAs and similar are just small enough that we can train models on them.

https://preview.redd.it/cn4m3ql6tv3d1.png?width=1150&format=png&auto=webp&s=b203c7c48090d55a1d15c508119466c874e1c2cb

The main methods that had interesting results were directly training a diffusion model to generate LoRAs and training learnable coefficients to mix LoRAs either globally or per-layer, similar to work by [sakana.ai](http://sakana.ai) with evolutionary model-merging.

https://preview.redd.it/goh703gguv3d1.png?width=1718&format=png&auto=webp&s=b06172f7800eecabd120b324cc090fe54b12624e

For experimentation, I created a dataset of 136 LoRAs trained on different celebrities (something I could hope to be a reasonably low dimensional manifold) each of 1.3M parameters for Stable Diffusion. Only requiring an at-home 3090.

The dataset is first prepared by fusing all LoRAs into full weight matrices, then decomposing again using singular value decomposition to ensure consistent representations and get rid of an unnecessary symmetry while modeling.

Because 136 datapoints does not get you very far, and with the knowledge that merging LoRAs also generally results in something that quacks like an in-distribution LoRA, I used sets of augmentations revolving around randomly creating merges on the fly and adding small amounts of noise.

For the learnable merging method, we keep all of our pretrained LoRAs in memory and learned weighted coefficients of their outputs, thus all of the unique celebrities create the basis vectors of the space of possible results.

I first tried a layer-wise weighting method resulting in only 14k trainable parameters and then trained on images of my face. There is decent consistency

https://preview.redd.it/ghl03mb0yv3d1.png?width=1406&format=png&auto=webp&s=b355307a0d1a4c60ea8a269761649f803ce25eb5

Then I tried a global weighting, resulting only in as many trainable parameters as there are LoRAs, 136. It's much looser this time and a bit inconsistent but its pretty interesting considering how few parameters. I think the results could be better if we could ensure our basis LoRAs were maximally unique or somehow better covered the space we want to train over

https://preview.redd.it/m929rdf3yv3d1.png?width=1454&format=png&auto=webp&s=d5bfcc30b2dfc60384365cbf3d9f32cf8130828f

While the GAN and VAE methods did not work so well, the diffusion method really went beyond my expectations. My best guess is that noise added during training and in the generative process might act as an augmentation in itself. It's capable of generating a very diverse range of people (although they generally have this subjective celebrity look to them that is difficult to escape)

I suspect this method could be made conditional as well, using image embeddings to guide the generation potentially.

https://preview.redd.it/pk0p2ef2xv3d1.jpg?width=1990&format=pjpg&auto=webp&s=0c217c532afcf97dfd5cf2cb367fbe087b8e0acf",1,25
MachineLearning,t3_1d5cxyq,1717209631.0,/r/MachineLearning/comments/1d5cxyq/exploration_vs_exploitation_in_tuning_playbook/,Discussion,Exploration vs Exploitation in Tuning Playbook - Need Help Understanding the Process [D],"\[edited\]

I'm reading through ""Tuning Playbook"" and I'm having some trouble understanding the concept of ***exploration vs exploitation*** in the context of hyperparameter tuning.

Is there anyone who can explain this concept in a more concrete manner rather being abstract, or maybe provide an example of how exploration is conducted in hyperparameter tuning?  what is exploration and how it will conducted. and one more thing; it keeps saying **understanding the problem,** which problem? the problem model tries to solve? or the problem of what hyperparameter influence the performance and each other? or what? 



***here is that part of the book about the topic:***

>Exploration vs exploitation

Thanks!",5,5
MachineLearning,t3_1d56zg0,1717191635.0,/r/MachineLearning/comments/1d56zg0/d_can_other_areas_researches_such_as_the_recent/,Discussion,"[D] Can other areas researches such as the recent mapping of a cubic millimeter of a human brain tissue, help the Machine Learning field? ","https://www.scientificamerican.com/article/a-cubic-millimeter-of-a-human-brain-has-been-mapped-in-spectacular-detail/

Can the researches surrounding the human brain, such as this latest map of a human brain provide some insight for the Machine Learning field, in order to build more efficient models of AI/algorithms?

Lay person here.",26,17
MachineLearning,t3_1d56h9c,1717190276.0,/r/MachineLearning/comments/1d56h9c/d_cheaper_way_to_do_model_inference/,Discussion,[D] Cheaper way to do model inference?,Does anyone know of any solutions for saving GPU compute during server downtime? I'm currently doing model inference and most of the time I'm just paying for compute without serving any user requests.,6,2
MachineLearning,t3_1d55cej,1717187293.0,/r/MachineLearning/comments/1d55cej/d_need_help_to_use_dedicated_gpu_on_vscode/,Discussion,[D] Need help to use dedicated GPU on vscode jupyter notebook.,"Hey,
I am currently doing my works in both colab and vscode jupyter extension. Since I have a Nvidia card I want to use that for training all kinds of models(deep,simple) using the jupyter notebook in the vscode.
How to set up this requirement? To make it simple I want to use dedicated GPU for the vscode jupyter notebook.",3,0
MachineLearning,t3_1d4z5dr,1717171395.0,/r/MachineLearning/comments/1d4z5dr/d_bigram_tokenizers_better_than_status_quo/,Discussion,[D] Bigram tokenizers better than status quo? Especially for for multilingual,"\[My first post here, a better place for this, in some LLM subreddit? It was removed, until I added \[D\], \[R\] better for my sort of research?\]

The tokenizer (tiktoken) for gpt-4o is seemingly adding tokens (at least for Icelandic), since gpt-4, and I believe it's going in the opposite direction we should be going. I don't think it's sustainable, nor needed, to expand that way to more natural languages. Even Icelandic alone has over 1 million word \*forms\* (e.g. all adjectives come in 120 word forms) and counting (in incomplete database of words I have), and English has over 400 thousand words.

It occurred to me, tokens should just be one letter (or even part of). I've since changed my mind and I think tokens should be based on bigrams (plus single letter tokens/bytes). Numbers and punctuation, and space, must be a special case, and Chinese. \[Tokens could in theory be only two the bits 0 and 1, and 8 needed to make up bytes..., why not, the answer might be the same as for why not single byte tokens.\]

The average length of a word is around 10 letters, in English, and German etc. i.e. from dictionaries, but in actual use the average length of an English word is ""4.79 letters per word, and 80% are between 2 and 7 letters long"" so basing on bigraphs, would mean a factor of 2-3 times more tokens per word, for a very modest expansion. I hear you say, we pay per token, but costs are coming down, and with linear transformers, or similar Mamba etc., do no longer have the quadratic cost of traditional Transformers, nor effectively limited context lengths/windows, also the output of tokens is very fast.

So why bigrams, not trigrams, or base only on single letters (5x more tokens), or only on whole words (without subwords, or allow that too?)?

Basing on only letters could mean 26 possible lower case tokens, double for upper case, or 128 possible for full ASCII, or over one million possible tokens for full Unicode support.

Clearly basing on Unicode code points (rather than code units, maybe) seems not viable, even if we have tokens for so far only the about 150 thousand assigned characters. It might be doable, tokens, i.e. ""vocabulary"" count is already about a third of that.

Tokenizers already have ""byte"" tokens (to handle arbitrary Unicode, but a recent addition?), meaning any arbitrary binary file, or UTF-8 file, is possible, already, it's like an escape hatch when a word can't be tokenized as one, or more (for sub-word tokens). So why not only base on bytes, 256 possible tokens (plus some few control tokens)? I believe the reason it's not already done, is that, in effect it mean the network needs to learn the UTF-8 encoding, i.e. to decode variable 1 to 4 bytes per letter (I'm sure possible, but we might not want to vast part of it, I think lowest layers, on such decoding; Image neural neural networks can already decode binary file formats, not just PNG, even compressed JPEG, to some degree, I suppose only handling DC coefficients, i.e. at lower resolution, not discovering DCT).

Non single-letter tokens handicap an LLM in some situations. E.g. if you ask it: how many letters are in the word ""transformers""? To us we count letters, but the LLM sees the whole word as one token, so it must somehow store the number of letters for that, and each token, and have a way to decode them. This could lead to a problem in all kinds of unusual situations. The argument could be the simplest model one-letter tokens, best, or the next-simplest bigrams. So why not the simplest? It allows for arbitrary text such as a random password 8irkQPbIsYqqVFb. But it is not semantically meaningful. We want mostly to compress meaningful data/language, i.e. non-random data (random isn't compressible). Current tokenizers are very good at it, but bigrams are also good. English has 26 letter alphabet, and arbitrary bigrams would be 26\^2 = 676 possibilities, and trigrams 26\^3 = 17576, while English actually has only 139 bigrams (79% compression), and way more trigrams (somewhat better compression, but I think trigrams not scalable to many languages).

[https://arxiv.org/pdf/1207.2334](https://arxiv.org/pdf/1207.2334)

Russian/Cyrillic has its own 132 bigrams, the number of tokens possible will be 139 (for English) + 132 = 271 at a minimum (see table 6). German has 151 so the numbers add up to many, though even German, French etc. bigrams have some overlap with English. E.g. Indic languages will not, maybe with each other. Chinese has whole words in each letter, so bigrams do not apply in the same way, likely need to be special-cased, handled more like punctuation, each with it's own token, and Chinese alone will have a lot.

Using bigrams has a certain simplicity, e.g. counting letters in a word is simple for even-number-letter words, and only one special case other words. Odd-number-letter words must store only letter in a special way. The last letter in a word could be that possible odd letter. It's simple to do and I'm unsure if the alternative, i.e. having the first letter the possible odd- letter would be any better (then you must first count them), I still want the first letter handled in a special way anyway for casing purposes.

`>> encoding = tiktoken.encoding_for_model(""gpt-4o-turbo"")`

`>> encoding.encode(""Palli var einn Ã­ heiminum!"")`

>\[47, 36888, 972, 72272, 5471, 501, 10528, 394, 0\]

>Corresponding to:

>P-alli- var- einn- Ã­- he-imin-um-!

>vs. with:

>encoding = tiktoken.get\_encoding(""gpt-3.5-turbo"")

>\[47, 96893, 767, 4466, 77, 41236, 568, 61334, 372, 0\]

>P-alli- var- ein-n- Ã­- he-imin-um-!

At first it seemed to get me the exact same tokens, just many tokens renumbered, but actually it seems like it's ""improving"" for Icelandic, supposedly, with now one fewer:

The ""einn"" there is the masculine form (for ""alone""), and ""ein"" the female, then the old one adds an ""n"", and it seems ok. I it works either way, Icelandic isn't perfect yet, but close enough (also the voice capability), which is rather amazing for a very low-resource language, maybe with least training data a very small fraction of a percentage. The split into tokens is grammatically all wrong, so maybe letter split or simple digram would be ok. I think since einn and ein are related words, actually the ein-n might be better with the former token for masculine would then relatable to the female word. However I think we can not rely on such good relevant grammatical split, e.g. heimi-num would be better with -num there the definite article of the word ""world"".",6,8
MachineLearning,t3_1d4tux3,1717156707.0,/r/MachineLearning/comments/1d4tux3/d_is_sequence_packing_common_for_training/,Discussion,[D] Is sequence packing common for training transformers? ,"Hi all,

I want to train a small transformer language model from scratch and I'm trying to squeeze out as much training efficiency as reasonably possible. I was thinking about how to build the training batches which brought me to this paper [EFFICIENT SEQUENCE PACKING WITHOUT CROSS-CONTAMINATION: ACCELERATING LARGE LANGUAGE MODELS WITHOUT IMPACTING PERFORMANCE](https://arxiv.org/pdf/2107.02027) which seems like a totally reasonable thing that should just be done in general.

In short, the idea is to pack multiple sequences together in a sample sequence and adjust the attention matrix such that the samples don't cross-contaminate each other, i.e. only attend to tokens within themselves. [This post in the Huggingface forum](https://discuss.huggingface.co/t/the-correct-attention-mask-for-examples-packing/52909) nicely illustrates this. But I couldn't find such thing in Huggingface transformers. Am I missing something? Have other frameworks implemented this? Do we know if the big players are doing the sequence packing? Are there major downsides to this I'm missing? I thought it could become problematic with the positional encodings maybe.

Edit: Huggingface transformers actually supports this, see [https://github.com/huggingface/transformers/pull/27539](https://github.com/huggingface/transformers/pull/27539)",8,8
MachineLearning,t3_1d4tbzg,1717154870.0,/r/MachineLearning/comments/1d4tbzg/p_evaluate_rag_using_large_language_models/,Project,[P] Evaluate RAG using Large Language models,"I have been working on RAG and LLM, I always wanted to evaluate LLMs. There are libraries that worked with GPT based models but for RAG I mainly wanted to evaluate Llama or Mistral based model.  
So I built BeyondLLM.  
  
BeyondLLM helps you build advanced Retrieval-Augmented Generation (RAG) and Large Language Model (LLM) applications with just 5-7 lines of code. BeyondLLM is open source and it also supports Fine Tune Embeddings, and Observaility.  
  
GitHub:Â [https://github.com/aiplanethub/beyondllm/](https://github.com/aiplanethub/beyondllm/)",2,0
MachineLearning,t3_1d4shqn,1717151693.0,/r/MachineLearning/comments/1d4shqn/d_ml_conferences_and_organization_metrics/,Discussion,[D] ML Conferences and Organization Metrics,"I feel like many would consider NeurIPS, ICLR, ICML, etc as important venues in the field of ML. Even outside of ML, NeurIPS and ICLR haveÂ [the #9 and #10 highest H-index of any venues](https://scholar.google.com/citations?view_op=top_venues&hl=en). However, now I am looking at tenure-track positions globally, and it seems like a different story. It seems like such publications are worthless for the purposes of immigration or academic tenure, because they are not traditional journals. You'll notice they are missing from the SCImago, a ranking which many organizations use as a proxy for publication quality, and consequently tenure or immigration decisions.

  
I am curious as to what academic ML researchers do under these circumstances. Do you stop submitting to NeurIPS and aim to publish in ACM ""Foundation and Trends in Machine Learning"", a journal whichÂ [ranks #2 on SCImago instead](https://www.scimagojr.com/journalrank.php)? Or the ever-growing list of IEEE journals on machine learning?",20,46
MachineLearning,t3_1d4pjxp,1717138932.0,/r/MachineLearning/comments/1d4pjxp/d_kan_multilayer_gam/,Discussion,[D] KAN == multi-layer GAM ?,"I just read the KAN paper, 

My understanding is that it provides a solution on how to stack multiple layers of GAMs (Generalized Additive Model): The Phi function is just the shape function of a GAM, and splines are well studied shape functions in GAMS.

So to me:

- MLP is a multi-layer linear regression  
- KAN is a multi-layer GAM

Still, a GAM has a link function than is not expressed in the KAN paper, but to me it looks like this is the real point of the paper. If we add an activation function to a KAN layer, then we fully have a multi-layer GAM.

This also means that we can consider MLP as a special case of a KAN because linear-regression is a special case of a GAM.

Does this sound correct?",5,37
MachineLearning,t3_1d4mpkw,1717128036.0,/r/MachineLearning/comments/1d4mpkw/r_lipreading_with_lipnet_endtoend_sentencelevel/,Research,[R] Lipreading with LipNet: End-to-End Sentence-level Lipreading,"Hey there,

I recently implemented LipNet from scratch based on the paper End-to-End Sentence-level Lipreading. It predicts sentences by extracting features from the lip movement in the input frames. It is originally a 3DConv-GRU model which I've implemented with a 3DConv-LSTM (bi-directional) and a few other models with varying complexity, and have utilized He (Kaiming Normal) initialization for the weights.

I request you take look at the repository and provide any feedback, and consider a fork if you find it useful.

[GitHub/LipNet](https://github.com/mishra-18/lipnet-pytorch)

[Image edited from the official paper](https://preview.redd.it/ur6wsop2qo3d1.png?width=625&format=png&auto=webp&s=8282b11b16c6f7d7b8fa36ca2b3f79231fe1078c)

",0,26
MachineLearning,t3_1d4dx88,1717101611.0,/r/MachineLearning/comments/1d4dx88/r_machine_learning_introspection/,Research,[R] Machine learning introspection,"While ""introspection"" is not well-defined in AI, it does have a long history - mainly to equip machines with human intuition for problem solving, with Newell and Simon's ""General Problem Solver"" (1958) being an early example. According to [Roger Grosse](https://lips.cs.princeton.edu/introspection-in-ai/) the field has moved away from introspection due to what he views as an aversion to thinking about algorithms in terms of mental states.

  
However, this week we released a [video](https://www.youtube.com/watch?v=drlqCc_e_o0) that goes over these basic ideas and explores two papers on the concept of introspection that have been released over the past few years.  The first being ""[Introspective CNN's](https://proceedings.neurips.cc/paper_files/paper/2017/file/11b921ef080f7736089c757404650e40-Paper.pdf) that  improves classification results by synthesizing samples from its own classifier (as opposed to a GAN which uses a separate discriminator network to generate samples).  This is why the approach is called ""introspective"" because it uses its own classifier.

The [second paper](https://arxiv.org/abs/2209.13710#:~:text=27%20Sep%202022%5D-,Towards%20Human%2DCompatible%20XAI%3A%20Explaining%20Data%20Differentials%20with,Concept%20Induction%20over%20Background%20Knowledge&text=Concept%20induction%2C%20which%20is%20based,base%20data%20(ABox)%20graph) uses concept induction to create a set of concepts to describe why a system made a classification decision for the user.  Here the evaluation focused on the human's understanding of the explanation, as opposed to using it to make the classification result better.  This actually relates to an [earlier video](https://www.youtube.com/watch?v=26tTT8saaDs&t=2517s) we released with Prof. [Joao Leite](https://scholar.google.com/citations?user=ReF98QcAAAAJ&hl=en) where he discussed his results using ""mapping neural networks"" to perform a determination of concepts as well (you can also see [his paper](https://userweb.fct.unl.pt/~jleite/papers/aaai21.pdf)).

  
URLs to the recent videos are below:

[https:\/\/www.youtube.com\/watch?v=drlqCc\_e\_o0](https://preview.redd.it/ufky2zk5km3d1.jpg?width=1280&format=pjpg&auto=webp&s=16237f0bf1c8087fe9b42d386b4dfbcd61382ab0)

[https:\/\/www.youtube.com\/watch?v=26tTT8saaDs&t=2517s](https://preview.redd.it/95dyxv88km3d1.png?width=1280&format=png&auto=webp&s=c9af4f1672f778c33325c9f168db80d38f3cfc04)

",0,4
MachineLearning,t3_1d4dokf,1717101011.0,/r/MachineLearning/comments/1d4dokf/r_research_collaboration_in_cv_structured_light/,Research,[R] Research Collaboration in CV /Structured Light/ 3D Reconstruction,"I'm looking for collaborators interested in research and publication on structured light, 3D reconstruction usingprojection techniques, or anything related to Fringe projection or Phase Analysis. My focus includes:

- Structured light for 3D scanning
- Innovative projection methods
- Overcoming challenges in Phase analysis or phase unwrapping using recent technologies
- Applications in medical imaging, industrial inspection, etc.

If you're working in these areas or have insights to share, I'd love to discuss potential collaboration opportunities. I apologize if my post is all over the place. A quick chat to exchange ideas or give advice would be much appreciated. ",10,9
MachineLearning,t3_1d4d4s2,1717099623.0,/r/MachineLearning/comments/1d4d4s2/d_is_anyone_using_a_simulation_library_to/,Discussion,[D] is anyone using a simulation library to simulate a complex environment ?,"We have existing code in Java Mason simulation library for a  complex discrete environment (a virtual renewable energy world).

Are there good libs for simulating a complex world which are more suitable for RL ?",6,0
MachineLearning,t3_1d4bhk5,1717095391.0,/r/MachineLearning/comments/1d4bhk5/d_distance_estimation_in_meters_using_aruco/,Discussion,[D] Distance Estimation in meters using Aruco Markers,"Hello,  
I have 2 cameras in a cabin and I would like to find distance among each human ( in meters ) in the cabin to main social distancing. i'm planning to use Aruco markers but I'm not sure how to proceed with it.

My plan -

1. calibrate two cameras using Aruco markers to obtain intrinsic parameters.
2. Place the aruco markers such that the two cameras can see them to obtain the extrinsic parameters.
3. Based on the unique id generated by aruco, if the two cameras detect any of them aruco markers then perform triangulation.

I have a plan but I'm not sure if it would work. I don't know how the above approach would help me find the depth or distance among humans in meters. Please advise.

Please note I'm just starting out with computer vision.",1,3
MachineLearning,t3_1d47ca4,1717084461.0,/r/MachineLearning/comments/1d47ca4/r_i_ran_580_modeldataset_experiments_to_show_that/,Research,"[R] I ran 580 model-dataset experiments to show that, even if you try very hard, it is almost impossible to know that a model is degrading just by looking at data drift results","In my opinion, data drift detection methods are very useful when we want to understand what went wrong with a model, but they are not the right tools to know how my model's performance is doing. 

Essentially, using data drift as a proxy for performance monitoring is not a great idea.

  
I wanted to prove that by giving data drift methods a second chance and trying to get the most out of them. I built a technique that relies on drift signals to estimate model performance and compared its results against the current SoTA performance estimation methods ([PAPE \[arxiv link\]](https://arxiv.org/abs/2401.08348) and [CBPE \[docs link\]](https://nannyml.readthedocs.io/en/stable/how_it_works/performance_estimation.html#confidence-based-performance-estimation-cbpe)) to see which technique performs best.

  
To effectively compare data drift signals against performance estimation methods, I used an evaluation framework that emulates a typical production ML model and ran multiple dataset-model experiments.

As per data, I used datasets from the [Folktables package](https://github.com/socialfoundations/folktables). (Folktables preprocesses US census data to create a set of binary classification problems.) To make sure the results are not biased, in terms of the nature of the model, I trained different types of models (Linear, Ensemble Boosting) for multiple prediction tasks included in Folktables.

Then, I built a technique that relies on drift signals to estimate model performance. This method uses univariate and multivariate data drift information as features of a DriftSignal model to estimate the performance of the model we monitor. It works as follows:

1. Fit univariate/multivariate drift detection calculator on reference data (test set).

1. Take the fitted calculators to measure the observed drift in the production set. For univariate drift detection methods, we use Jensen Shannon, Kolmogorov-Smirnov, and Chi2 distance metrics/tests. Meanwhile, we use the [PCA Reconstruction Error](https://nannyml.readthedocs.io/en/stable/how_it_works/multivariate_drift.html#data-reconstruction-with-pca) and [Domain Classifier](https://nannyml.readthedocs.io/en/stable/how_it_works/multivariate_drift.html#domain-classifier) for multivariate methods.

1. Build a DriftSignal model that trains a regression algorithm using the drift results from the reference period as features and the monitored model performance as a target.

1. Estimate the performance of the monitored model on the production set using the trained DriftSignal model.

You can find the full implementation of this method in thisÂ [GitHub Gist](https://gist.github.com/santiviquez/aa224c6e232c8bd2534893888981564d).

  
Then, for evaluation, I used a modified version of MAE because I needed an aggregated version that take into consideration the standard deviation of the errors. To account for this, I scale absolute/squared errors by the standard error (SE) calculated for each evaluation case. We call the SE-scaled metrics **mean absolute standard error (MASTE)**.

[MASTE formula](https://preview.redd.it/7jnk40il2l3d1.png?width=858&format=png&auto=webp&s=19679d6a202b2175f75c6f3252430792682090ad)

  
Then it was a matter of running all the 580 experiments and collect results.

Since, each performance estimation method is trying to estimate the roc\_auc of the monitored model, I report the MASTE between the estimated and realized roc\_auc.

https://preview.redd.it/z0oviz763l3d1.png?width=1404&format=png&auto=webp&s=c0e4838dcadbf664ff59570997f46612002c7e6e

PAPE seems to be the most accurate method, followed by CBPE. Surprisingly, constant test set performance is the third best. This is closely followed by random forest versions of univariate and multivariate drift signal models.

  
This plot shows the quality of performance estimation among different methods, including PAPE and CBPE.

[Quality of performance estimation \(MASTE of roc\_auc\) vs absolute performance change \(SE\). \(The lower, the better\).](https://preview.redd.it/3ar6plzbyk3d1.jpg?width=1668&format=pjpg&auto=webp&s=c429a6c35daf887fff021bd4f9894a15caac7a57)

  
Here is a specific time series plot of a model's realized ROC AUC (black) compared against all the performance estimation methods. PAPE (red) accurately estimates the direction of the most significant performance change and closely approximates the magnitude.

[Time series plot of realized vs estimated roc\_auc for dataset ACSIncome \(California\) and LigthGBM model.](https://preview.redd.it/m6igkfmk3l3d1.png?width=1436&format=png&auto=webp&s=6146b84f4999b70fa618ade15084dda8fea2acc8)

The experiments suggest that there are better tools for detecting performance degradation than data drift, even though I tried my best to extract all the meaningful information from drift signals to create an accurate performance estimation method.

There are better tools for quantifying the impact of data drift on model performance. So, I hope this helps the industry realize that monitoring fine-grained metrics leads to nothing and that a change in an obscure feature might not mean anything. It is better to first estimate model performance and then, if it drops, review data drift results but not the other way around.

Full experiment set up, datasets, models, benchmarking methods, and the code used in the project can be found in this [longer post](https://www.nannyml.com/blog/data-drift-estimate-model-performance) that I wrote yesterday.",7,128
MachineLearning,t3_1d47290,1717083714.0,/r/MachineLearning/comments/1d47290/d_using_gen_ai_to_improve_your_own_writing_is_it/,Discussion,[D] Using Gen AI to improve your own writing - is it allowed in writing papers?,English is not my first language and I find it really hard coming up with proper/fluent sentences. I write some poorly constructed sentence and prompt ChatGPT/Gemini to improve the writing. Is acceptable in conferences?,7,0
MachineLearning,t3_1d449tv,1717076339.0,/r/MachineLearning/comments/1d449tv/d_llms_are_sensitive_to_choice_order_how_to_run/,Discussion,[D] LLMs are sensitive to choice order! - How to run MMLU benchmark?,"I am currently testing the MMLU benchmark on a LLAMA3-8B model.  

*(I know that MMLU has flaws, but I have to start somewhere.)*  

I noticed a bias in question labels.  

When I switched the order of the question choices, I got different results.  

This observation is supported by various papers, here are two:  

[Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions](https://arxiv.org/abs/2308.11483)  

[Large Language Models Are Not Robust Multiple Choice Selectors](https://arxiv.org/abs/2309.03882)

&#x200B;

**LLMs are sensitive to the order of the question choices.**  

&#x200B;

Based on this finding, I then decided to shuffle the choices randomly N times and perform one inference per random shuffle. Then vote for the majority answer.   

This method is slower but gives consistently better results.   

&#x200B;

Here are my questions:  

\- How do *you* run your multiple choice benchmarks? Do you shuffle the choices or not?  

\- I have yet to try few-shot: if you already have tried few-shot and question shuffling, please report!  

\- My next step is to ask the LLM for an open answer, embed the answer and the choices, then retrieve the best choice. Does it make sense?",2,4
MachineLearning,t3_1d430us,1717072645.0,/r/MachineLearning/comments/1d430us/p_help_us_build_a_million_action_dataset_to_train/,Project,"[P] Help us Build a ""Million Action Dataset"" to train Large Action Models","Hi everyone,

We're working on creating a dataset of screen recordings of people performing 1 million actions on their computers so that we can train a Large Action Model that can control computers.

This is part of the Ethereum HackFS hackathon and we are building mechanisms to anonymize the data client side and store the redacted data using decentralized storage in a way that the data contributors own benefit from models trained with the data.

There already exist 2 datasets that could be used for LAM training:

1. WorkArenaÂ [https://arxiv.org/pdf/2403.07718](https://arxiv.org/pdf/2403.07718)
2. WebLinxÂ [https://mcgill-nlp.github.io/weblinx/](https://mcgill-nlp.github.io/weblinx/)

But both of these are very small datasets.

They also include telemetry but we believe that we can train LAMs with only video recordings (like how a human can watch a YouTube tutorial and recreate the action on their device). This seems like Tesla's self driving on video rather than needing LiDAR)

What we need help with is defining the 1 million actions in this dataset. It should be a representative dataset across all the ways a human can use a computer. What would you like this dataset to contain that would enable you to use it / work on LAM research ?

Contributions, questions and advice welcome!",6,0
MachineLearning,t3_1d42hvh,1717070963.0,/r/MachineLearning/comments/1d42hvh/news_pydata_amsterdam_2024_call_for_proposals/,News,"[News] PyData Amsterdam 2024 Call for Proposals closes on Sunday, June 2.","Hey all, we will close the Call for Proposals portal this **Sunday, June 2**, for our **PyData Amsterdam 2024 Conference** which will take place on September 18-20. We are looking for presentations that can captivate our audience, provide invaluable insights, and foster community learning. Don't miss this chance to speak on stage in front of over 800 attendees. Check out our website PyData Amsterdam for more info and submit a talk! ",1,4
MachineLearning,t3_1d40ndl,1717064201.0,/r/MachineLearning/comments/1d40ndl/d_metrics_for_different_sequence_lengths/,Discussion,[D] Metrics for Different Sequence Lengths,"Hi everyone, I am working on time series analysis and prediction with multiple different models. As I train them with different sequence lengths to understand varying temporal dependencies, I started to experience a problem.  Even though I change and optimize model parameters for each different sequence length, my error metrics started to generate unacceptable values. Though when I run the models with their respective scalers, their predictions are still quite satisfying. So what is happening to my Theil's_u(u1) and MSE? Am I supposed change how I measure them as sequence lengths get bigger and bigger?",4,5
MachineLearning,t3_1d3zwqy,1717061154.0,/r/MachineLearning/comments/1d3zwqy/r_how_to_prepare_for_my_first_research_internship/,Research,[R] How to prepare for my first research internship ?,"Sure, here's a draft for your Reddit post body ! ..... just kidding, anyways.

I'm starting a 3-month NLP research internship soon and would love to get some advice on how to get the most of this opportunity. This is my first time doing research, so I'm a bit nervous but also excited.

A few details about the internship:

**Duration**: The internship will last for 3 months.

**Focus Areas**: I will be working on large language models (LLMs), specifically Retrieval-Augmented Generation (RAG) and Knowledge Graphs.

Given this context, I'm looking for any tips or advice on how I can effectively contribute to the research projects, especially given my limited experience. Also, do you have any general advice on making the most out of a research internship, especially for a first timer? I would also really appreciate any tips/tools to increase my productivity as a researcher.",9,16
MachineLearning,t3_1d3zw0d,1717061066.0,/r/MachineLearning/comments/1d3zw0d/d_what_additions_or_changes_would_you_make_to/,Discussion,[D] What additions or changes would you make to BERT knowing all the recent advances in ML?,"Hi!

Since BERT is still a widely used model. I was curious about what would you guys do to make it up-to-date. The BERT paper was submitted on 11 Oct 2018, last revised 24 May 2019 on arXiv.

The ideas doesn't have to touch on the architecture necessarily, it can be the scheduler or the training set, the MLM loss, making the training faster etc.

Personally, I'd change the positional encoding, maybe I'd use RoPE. Use Flash Attention.

For the dataset maybe I'd focus on mixtures, and I'm not knowledgeable in schedulers but I'd try something from all the LLM papers, something that enables continuous pre-training.",19,61
MachineLearning,t3_1d3w2uy,1717044837.0,/r/MachineLearning/comments/1d3w2uy/d_are_there_companies_that_regularly_discuss_how/,Discussion,[D] Are There Companies that Regularly Discuss How ML is Applied?,"Besides the usual big corporations like OpenAI, Meta and Google, the only company I am aware of is Disney and their YouTube channel DisneyResearchHub. I really love the videos they put up on how they use machine learning and reinforcement learning in puppeteering and improving CG techniques. 

I would love to discover more such channels on how companies use ML in their domains of interest.",3,6
MachineLearning,t3_1d3seuh,1717032837.0,/r/MachineLearning/comments/1d3seuh/d_conjectures_generation_approaches/,Discussion,[D] Conjectures generation approaches,"I am working in ML for reasoning and currently having interest in theory exploration. Although there are several papers online, I am wondering what is everyone's approach in generate new conjectures and how do you apply current LLM in this task?",0,1
MachineLearning,t3_1d3mru7,1717016864.0,/r/MachineLearning/comments/1d3mru7/d_ml_system_design_interview_prep/,Discussion,[D] ML system design interview prep,"l am in the on-site stage of the Meta interview process, and need to start preparing for my ML system design interview. I have been grinding leetcode for the coding parts, but after looking online the resources and examples available for ML system design questions seem to be pretty much non exist besides a few YouTube videos. Does anyone know of any good resources to help prepare? Thanks I'm advance!",12,14
MachineLearning,t3_1d3m035,1717014620.0,/r/MachineLearning/comments/1d3m035/p_finding_formulas_suitable_for_mental/,Project,"[P] Finding formulas suitable for mental calculation (day of week, sunrise)","As a hobby project, I currently play around applying AI/ML to finding formulas that one can calculate in ones head. One of the most well-known examples likely is the [Doomsday algorithm](https://en.wikipedia.org/wiki/Doomsday_rule).

### What I would like to do:

1) For day of the week calulation: ""Re-discover"" formulas like in [this section](https://en.wikipedia.org/wiki/Doomsday_rule#Why_it_works). Ideally, discover new such formulas.

2) Sunrise calculation: For this one, I would like to find an (approximation) algorithm that is suitable for mental calculation, while being of by some margin, say, 20 minutes.

I would like to do both by generating a bunch of data and throwing cpu/gpu cycles at it.

### What I have tried to far:

* Symbolic Regression

  * Tried: [FeynmannAI](https://github.com/SJ001/AI-Feynman), [PySR](https://github.com/MilesCranmer/PySR)

  * I like the generated formulas, but unfortunately they are contain float coefficients, while I need integer coefficients (calculations involving floats are hard to do mentally)

* Genetic Programming

  * Tried: [DEAP](https://github.com/DEAP/deap)

  * I like that I can constrain the generated formulas much more (i.e. by only including integer terminals), but I find it quite hard to get good formulas by playing around with the genetic parameters (population, kind of mutation, kind of crossover etc.)

### Questions

A) Are there symbolic regression programs that do not produce formulas with floats in them?

B) Regarding Genetic Programming: Is this the right approach for 1) and 2)? Should I just try harder and learn more about parameter tweaking?

C) Are there other approaches I can try?

Thank you for your time!",2,1
MachineLearning,t3_1d3lbep,1717012806.0,/r/MachineLearning/comments/1d3lbep/d_whats_your_alltime_favorite_deep_learning_paper/,Discussion,[D] What's your All-Time Favorite Deep Learning Paper?,"I'm looking for interesting deep learning paper, especially regarding architectural improvement in computer vision tasks.",90,185
MachineLearning,t3_1d3jmad,1717008363.0,/r/MachineLearning/comments/1d3jmad/d_json_a_to_json_b_embeddings_for_fast_retrieval/,Discussion,"[D] JSON A to JSON B embeddings, for fast retrieval of NN
","Hi,

Imagine have two classes of JSON, eg, a person and the books they have read, and a set of movies to be watched. Imagine you have a function (can just be labelled data), that says person A will like movie B.  
How would you use this source of labelled data to train some kind of embeddings such that you can dot-product(embeddings(person). embeddings2(movie)) and get a close approximation of if you were able to brute force apply the function (or ask an oracle) for every movie.

In the text world, this is easy, you just pick your favorite sentence embeddings, minHasH, etc, etc. What I am asking here is how would you architect and train a model to produce embeddings for these things that are highly structured.

The ""difficulty"" here is that the two objects inter-relation ship will be complex, and that complexity has to be factored into the two embeddings. In the persons books to movies case, one would assume there's some common latent variables that can be learnt (e.g. genre, style, era), and that the embeddings on both sides would infer something similar, but the embedding function of list of books will be different from that of the embedding function of an individual movie.

A cursory google search just returns adomitions of ""turn your json to text and use text embeddings"".

Thanks in advance,

W",13,3
MachineLearning,t3_1d3ix2b,1717006580.0,/r/MachineLearning/comments/1d3ix2b/d_kubeflow_is_becoming_the_de_facto_first_choice/,Discussion,[D]  Kubeflow is becoming the de facto first choice in the open-source community. But we are facing challenges.,"1. Every ML team needs to find an end-to-end solution. Kubeflow is becoming the de facto first choice in the open-source community. It is one of the most comprehensive solutions, with many integrations and completely open-source components. However, its installation requires a strong understanding of various aspects, as it must be installed on a Kubernetes infrastructure. The research and operating costs for a company are very high, often necessitating a dedicated DevOps employee for this task. Additionally, user management is not provided within the solution and needs to be integrated separately.

2. Cloud-managed Kubernetes services like EKS or AKS allow the creation of new Kubernetes clusters but restrict the addition of external nodes. Given the need for ML teams to manage budgets efficiently, particularly with the high cost and shortage of GPUs, it is crucial to have the option of creating a multi-cloud cluster of GPU nodes. The current official solutions involve creating a Kubernetes cluster in each cloud provider and implementing a federation system. This approach is extremely complex to maintain and is not feasible if a provider does not offer a Kubernetes service.

How are you experiencing this? ",0,0
MachineLearning,t3_1d3iht7,1717005564.0,/r/MachineLearning/comments/1d3iht7/d_architecture_advice_for_finetuning/,Discussion,[D] Architecture Advice for Finetuning Diffusion-Based Models,"We're developing a system focused on diffusion-based models from theÂ diffusersÂ library. Our setup includes a lightweight fine-tuning technique, which updates several components in the pipeline. We want to try other fine-tuning methods, and are thinking of restructuring our code so its not procedural spaghetti code. We want these fine-tuning processes to be separate from the initialization of the diffusion pipeline and the core training loop. Any suggestions on how to architect this effectively? We are thinking of at least creating a Trainer for separating the training loop.",0,0
MachineLearning,t3_1d3hepc,1717002808.0,/r/MachineLearning/comments/1d3hepc/discussion_a_cnn_approach_for_combining_different/,Discussion,[Discussion] A CNN approach for combining different experimental data,"Hello! I'm a PhD student currently preparing an application for a grant. I graduated 3 years ago in Physics of Data (a mixture of Data Science and Physics, I did a lot of ML stuff), and I would like to return to that field after my PhD, which has been mainly focused on Physics, especially on diagnostics.

My project idea is to create a Neural Network-based tool to combine experimental data derived from different instruments. Specifically, I would like to reconstruct a 2D image by combining information from the vertical and horizontal directions, obtained separately and with different resolutions.

My initial model idea is a CNN, possibly integrated with the physics underlying the data I will use. However, even though I don't think it is a particularly original project, I am puzzled that I am struggling to find literature on this topic.

Does anyone have a similar experience/project or could recommend some valuable papers to read? Is there a more innovative model that could be useful for my task, perhaps one that includes the capability to predict the outcome of the experiment given certain parameters?

Thank you in advance, I hope you find the discussion interesting :)",2,1
MachineLearning,t3_1d3hadp,1717002505.0,/r/MachineLearning/comments/1d3hadp/r_why_using_the_gumbelsoftmax_is_better_than_just/,Research,[R] Why using the Gumbel-Softmax is better than just using Softmax ?,"Hello,
Many papers tend to use the Gumbel-Softmax function to generate a probability distribution , and then simple a binary mask for this distribution. My quedtion is why is Gumbel-Softmax better than Softmax. As for me the trick is to keep the gradient from the differentiable vector while using the binary mask.
Thanks !",9,44
MachineLearning,t3_1d3h5fs,1717002152.0,/r/MachineLearning/comments/1d3h5fs/d_benchmarking_foundation_models_for_time_series/,Discussion,[D] Benchmarking foundation models for time series,"# Introduction

We present a reproducible benchmark comparing different foundation time series models across a wide variety of models in a large scale dataset.

We conclude that [TimeGPT-1](https://arxiv.org/abs/2310.03589b) ranks first in terms of accuracy and speed inference compared to the latest foundation models, including [TimesFM](https://arxiv.org/pdf/2310.10688) (Google), [Chronos](https://arxiv.org/abs/2403.07815) (Amazon), [Moirai](https://arxiv.org/abs/2402.02592) (SalesForece), and [Lag-LLama](https://arxiv.org/pdf/2310.08278) (Service Now). TimeGPT-1 and TimesFM also outperform established statistical, machine learning, and deep-learning models, with comparable inference times to a SeasonalNaive. Chronos, Moirai and Lag-Llama still need some further improvements and can be outperformed by other classical methods.

This analysis spans over **30,000 unique time series** across various domains and frequencies from M-Competitions, Monash Repository, and Wikipedia page views, among others, robustly comparing these models.

# Empirical Evaluation

This study considers **over 30,000 unique time series** from the Monash Repository, M-Competitions, Wikipedia page views, among others, spanning various time series frequencies: Monthly, Weekly, Daily, and Hourly. Our evaluation compares five foundation models for time series data in terms of accuracy and inference times. We have also included comparisons to a large battery of statistical, machine learning, and deep-learning models, to provide a benchmark against traditional forecasting methods.

We include the following models in our comprehensive evaluation:

* [Statistical](https://github.com/Nixtla/statsforecast/): SeasonalNaive, HistoricAverage, ZeroModel, AutoARIMA, Prophet, AutoCES, AutoETS, Theta, DynamicOptimizedTheta, ADIDA, IMAPA, and CrostonClassic.
* [Machine Learning](https://github.com/Nixtla/mlforecast/): AutoLGBM.
* [Deep Learning](https://github.com/Nixtla/neuralforecast/): AutoTFT, AutoNHITS.
* Foundation: Chronos, Lag-Llama, Moirai, TimeGPT, TimeGPT (long horizon), and TimesFM.

# Results

TimeGPT-1 ranks first in terms of accuracy and speed inference compared to the latest foundation models, including TimesFM, Chronos, Moirai, and Lag-Llama. TimesFM by Google ranks second in accuracy and outperfoms TimeGPT-1 in inference speed. Amazon Chronos ranks third in accuracy but shows a significant drop in inference speed. Both Salesforces's and ServiceNow's models are far more efficient in terms of inference speed than Chronos, but they rank lower in terms of accuracy.

[Reproducible experiment](https://github.com/Nixtla/nixtla/tree/main/experiments/foundation-time-series-arena)

https://preview.redd.it/h374cfaube3d1.png?width=1798&format=png&auto=webp&s=a2b0853ef9b9ebefb8f5977bfe11ef14c89964aa

https://preview.redd.it/55qnz8mtbe3d1.png?width=2146&format=png&auto=webp&s=7878f778fec30fa562a0422de3dc2748a6538571

https://preview.redd.it/wrfxhuxuce3d1.png?width=2086&format=png&auto=webp&s=d6fc57495e5571d1d68871538514e25375e90d54",13,51
MachineLearning,t3_1d3e2r6,1716994267.0,/r/MachineLearning/comments/1d3e2r6/r_towards_optimal_llm_quantization/,Research,[R] Towards Optimal LLM Quantization,"picoLLM CompressionÂ is a novel LLM quantization algorithm that automatically learns the optimal bit allocation strategy across and within LLM's weights given a task-specific cost function. Existing techniques require a fixed bit allocation scheme, which is subpar.



Article: [https://picovoice.ai/blog/picollm-towards-optimal-llm-quantization/](https://picovoice.ai/blog/picollm-towards-optimal-llm-quantization/)



GitHub: [https://github.com/Picovoice/llm-compression-benchmark](https://github.com/Picovoice/llm-compression-benchmark)

",1,17
MachineLearning,t3_1d3cbzu,1716989647.0,/r/MachineLearning/comments/1d3cbzu/d_how_can_we_improve_the_performance_of_open/,Discussion,[D] How can we improve the performance of open source LLMs in competition level math (using any possible way)? ,"From what I researched deepseek-math-7b-rl is the best model so far. You need to include methods like self consistency / majority voting, python tool integration and self verification. can agents (made of open source LLMs) perform CoT in a better way and can they inculcate verification of their own answers generated? like providing an evaluation score as an observation for each step of CoT or something similar?",4,0
MachineLearning,t3_1d3bvkb,1716988360.0,/r/MachineLearning/comments/1d3bvkb/d_anyone_knows_how_to_get_ratedistortion_curve/,Discussion,[D] Anyone knows how to get rate-distortion curve for diffusion models ?,Hi everyone I have different trained diffusion models and Iâ€™ve seen many diffusion papers have rate distortion curves mentioned. Anyone knows the methodology to generate them or could point me to appropriate resources?,5,7
MachineLearning,t3_1d3aeaq,1716983813.0,/r/MachineLearning/comments/1d3aeaq/d_friday_oxenai_paper_club_extracting/,Discussion,[D] Friday Oxen.ai Paper Club:  Extracting Interpretable Features from Claude 3 Sonnet,"Hear the paper that Hugging Face cofounder Thomas Wolf called ""totally based""  interpreted through the lens of [Oxen.ai](http://Oxen.ai) CEO and Master-of-Plain-Speak-Delving:   Greg Schoeninger.

Register:  [https://lu.ma/oxen](https://lu.ma/oxen)

Friday 10:00 AM Pacific,  1:00 PM Eastern Time  on  Zoom

Paper:  [https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html?s=09%2F/](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html?s=09%2F/)

? Hey is there no ArXiv link for this one?

Thank you Greg, u/FallMindless3563, Scott Howard u/sthoward, and the Oxen team for sharing your knowledge with the community while providing cool tools to curate datasets at oxen.ai.",4,5
MachineLearning,t3_1d391p4,1716978967.0,/r/MachineLearning/comments/1d391p4/project_prompt_teacher_free_educational_tool/,Project,"[Project] Prompt Teacher - Free, educational tool teaching how to write effective LLM prompts","I'd like to share an educational prompt optimization tool called **prompt teacher** that I hope to be useful for the community :)

# Quickstart Guide ğŸš€

ğŸ‘‰ T**ry the app directly without any setup:** [Prompt Teacher @ Huggingface Spaces](https://pwenker-prompt-teacher.hf.space/)

ğŸ” I**nspect the code:**

* **GitHub**: [pwenker/prompt\_teacher](https://github.com/pwenker/prompt_teacher)
* **Hugging Face Spaces**: [pwenker/prompt\_teacher](https://huggingface.co/spaces/pwenker/prompt_teacher)

# Metaprompts Overview ğŸ“œ

Here are some of the metaprompts you can explore:

|**Name**|**Explanation**|**Example Prompt**|**Example Prompt Explanation**|
|:-|:-|:-|:-|
|**Expand with details**|Expands a prompt to include more detailed instructions and context.|Tell me about dogs.|This prompt is vague and lacks context, making it ideal for expansion to guide the LLM more effectively.|
|**Apply feedback**|Improves a prompt based on specific feedback provided.|Describe the process of photosynthesis.|Feedback might suggest making the prompt more accessible for younger audiences or more detailed for academic use.|
|**Simply condense prompt**|Condenses a prompt to make it more succinct while retaining its essential request.|Write a funny joke that makes people laugh about something very funny. It should be hilarious.|This prompt can be condensed by removing redundant information.|
|**Simply improve prompt**|Improves a prompt to enhance clarity and effectiveness.|Tell me how to cook rice.|This prompt can be improved by specifying the type of cuisine or cooking method.|
|**Create sequential task list**|Structures a prompt to guide the LLM through a series of sequential tasks.|Plan a birthday party.|This prompt can be structured to outline steps such as choosing a theme, preparing a guest list, and organizing activities.|
|**Elicit creative response**|Transforms a prompt to inspire creativity and elicit imaginative responses.|Write a story about a lost kitten.|The prompt can be revised to encourage more descriptive or emotional storytelling.|
|**Include hypothetical scenario**|Tailors a prompt to include a specific hypothetical scenario for detailed exploration.|The danger of Artificial General Intelligence|This prompt can be tailored to explore specific hypothetical scenarios to provide depth and context.|
|**Focus on ethics**|Reframes a prompt to focus on ethical considerations or moral dilemmas.|Genetic engineering in humans.|This prompt can be reframed to focus on the ethical considerations or moral dilemmas involved.|
|**Add role prompting**|Adds a role to the prompt to improve the response.|Write a short song.|By adding an expert role, we can potentially improve the quality of the created song.|
|**Add delimiters for clarity**|Adds clear delimiters to a prompt to separate and organize different sections or instructions, enhancing readability and structure.|Summarize this text with bullet points. Be concise|This prompt can benefit from clear delimiters to separate instructions or sections, making it easier for the LLM to follow and respond systematically.|
|**Incorporate chain of thought reasoning**|Incorporates chain of thought reasoning to guide the LLM through a logical sequence of thoughts for complex problem-solving.|How can we reduce traffic congestion in urban areas?|This prompt can benefit from chain of thought reasoning to break down the problem into manageable parts and explore various solutions systematically.|
|**Comprehensive prompt refinement**|Integrates various techniques to refine, expand, and adapt prompts for LLMs, ensuring clarity, specificity, and engagement tailored to the intended purpose.|Write a brief history of Artificial Intelligence|This prompt can be improved by specifying aspects such as the depth of detail, areas of focus, and desired structure.|",0,4
MachineLearning,t3_1d37emc,1716972113.0,/r/MachineLearning/comments/1d37emc/r_tool_learning_with_large_language_models_a/,Research,[R] Tool Learning with Large Language Models: A Survey,"PDF:Â [https://arxiv.org/abs/2405.17935](https://arxiv.org/abs/2405.17935)

GitHub:Â [https://github.com/quchangle1/LLM-Tool-Survey](https://github.com/quchangle1/LLM-Tool-Survey)



Abstract: Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the ""why"" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of ""how"", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area.

https://preview.redd.it/t46d2cxivb3d1.jpg?width=1250&format=pjpg&auto=webp&s=a3d3bd9f285717b6a6f9c9d0015789ec39f9abd9

https://preview.redd.it/rp0gdkkjvb3d1.png?width=830&format=png&auto=webp&s=2e87a52ccf7637783f308fd6b421d8b2fa0cbee0

https://preview.redd.it/fwwyuq3kvb3d1.jpg?width=914&format=pjpg&auto=webp&s=39f080e0db7c611f418b53e99aa274a2d7ad35b7

  
",0,10
MachineLearning,t3_1d374hh,1716970867.0,/r/MachineLearning/comments/1d374hh/d_data_scientist_does_the_task_without_data/,Discussion,[D] Data Scientist does the task without data,"Recently I was assigned a task to build a user purchase scoring system based on user interaction activities.

However, the funny thing is that I don't have data about user interactions with the product, so I surveyed the solutions of many parties and used my hypotheses to create the features which I thought will suitable to be able to build a prediction model. And of course when I presented it to the manager, the results were extremely bad. I sat down to discuss with him the definition of the features needed when creating the model and what made me quite angry was that he still don't know what kind of data is to build a scoring model. How will people deal with this situation?",18,42
MachineLearning,t3_1d329nt,1716951996.0,/r/MachineLearning/comments/1d329nt/d_isnt_hallucination_a_much_more_important_study/,Discussion,[D] Isn't hallucination a much more important study than safety for LLMs at the current stage?,"Why do I feel like safety is so much emphasized compared to hallucination for LLMs?

Isn't ensuring the generation of accurate information given the highest priority at the current stage?

why it seems like not the case to me",166,165
MachineLearning,t3_1d2xrb8,1716938497.0,/r/MachineLearning/comments/1d2xrb8/d_k1_in_knn/,Discussion,[D] k=1 in KNN ,"Good evening , I tested the knn algorithm on an unbalanced test set after having trained it on a balanced one ; I get k=1 as the optimal parameter in terms of accuracy and I confirmed this result using cross-validation. Is it strange to have this value or not ?",11,16
MachineLearning,t3_1d2x46v,1716936830.0,/r/MachineLearning/comments/1d2x46v/r_oil_water_diffusion_of_ai_within_and_across/,Research,[R] Oil & Water? Diffusion of AI Within and Across Scientific Fields,"Read the paper here: [https://arxiv.org/abs/2405.15828](https://arxiv.org/abs/2405.15828)

This study empirically investigates claims of the increasing ubiquity of artificial intelligence (AI) within roughly 80 million research publications across 20 diverse scientific fields, by examining the change in scholarly engagement with AI from 1985 through 2022. We observe exponential growth, with AI-engaged publications increasing approximately thirteenfold (13x) across all fields, suggesting a dramatic shift from niche to mainstream. Moreover, we provide the first empirical examination of the distribution of AI-engaged publications across publication venues within individual fields, with results that reveal a broadening of AI engagement within disciplines. While this broadening engagement suggests a move toward greater disciplinary integration in every field, increased ubiquity is associated with a semantic tension between AI-engaged research and more traditional disciplinary research. Through an analysis of tens of millions of document embeddings, we observe a complex interplay between AI-engaged and non-AI-engaged research within and across fields, suggesting that increasing ubiquity is something of an oil-and-water phenomenon -- AI-engaged work is spreading out over fields, but not mixing well with non-AI-engaged work.",0,5
MachineLearning,t3_1d2ww0c,1716936215.0,/r/MachineLearning/comments/1d2ww0c/d_indoor_localizationslam_module_with_150_bom/,Discussion,[D] Indoor localization/SLAM module with ~$150 BOM,"A question to the community. We are pondering commercialization of an indoor localization/mapping software that runs on a \~$100-150 BOM (a basic CPU and one fish-eye camera). Weâ€™ve built it for our internal project but would like to bring it to the community if this is valuable. Itâ€™s still a bit of work for us so we want to know if it makes sense.

It doesnâ€™t require fiducials and works in large open spaces (large warehouses).Â 

We would publish all the source code so that changes can be made without us if needed. The commercial usage would require a commercial license.Â 

We also have modules for cost-efficient obstacle avoidance, that we can share too. Please let me know if you think this would be valuable.",3,6
MachineLearning,t3_1d2wc9a,1716934767.0,/r/MachineLearning/comments/1d2wc9a/d_andrew_dudzik_on_sota_in_deep_learning/,Discussion,[D] Andrew Dudzik on SOTA in Deep Learning,"Dudzik from Google DeepMind recently said that Transformers are not, in fact, sota, and that Graph Neural Networks hold that mantle: [Andrew Dudzik - Three Problems in the Mathematics of Deep Learning - YouTube](https://www.youtube.com/watch?v=btF19HOrWC4&t=267s)

Sure, the former isn't so great with OOD data on many tasks (NER, translations to / fro low-resource languages etc.). But on the flip-side, not everything fits into a knowledge graph structure. Just opening this up for discussion. Do folks agree? Have they read more interesting papers as of late on graph nns?",6,9
MachineLearning,t3_1d2u14p,1716929036.0,/r/MachineLearning/comments/1d2u14p/d_best_way_to_deploy_setfit_models_in_production/,Discussion,[D] Best way to deploy SetFit models in production,"as the title states, I am trying to deploy a setfit model in production and am looking for an efficient way to do so. I tried using the huggingface TEI, but unfortunately, it only outputs the vector, sacrificing the classification head. Do you guys have any suggestions or alternative approaches I could experiment with? Thanks!!",4,7
MachineLearning,t3_1d2jf2q,1716902698.0,/r/MachineLearning/comments/1d2jf2q/research_tangles_a_new_mathematical_ml_tool_book/,Research,[Research] Tangles: a new mathematical ML tool - book announcement,"Here's my new book, just out:



**Tangles: A structural approach to artificial intelligence in the empirical sciences**

Reinhard Diestel, Cambridge University Press 2024



**Ebook**, plus open-source **software** including tutorials, available from **tangles-book.com.**



Note: This is an 'outreach' book not primarily about tangle theory, but about *applying* tangles in a multitude of unexpected ways and areas. Tangles in graphs are covered in my *Graph Theory*, 5th ed'n.



Table of Contents and an introduction for data scientists (Ch.1.2), are available from tangles-book.com/book/details/ and from [arXiv:2006.01830](https://arxiv.org/abs/2006.01830). Chapters 6 and 14 are about a new method of soft clustering based on tangles, very different from traditional methods. Chapters 7-9 cover the theory needed for Chapter 14.

  


Collaboration on concrete projects is warmly invited, as are contributions to the GitHub software library.





*Publisher's blurb:*



Tangles offer a precise way to identify structure in imprecise data. By grouping qualities that often occur together, they not only reveal clusters of things but also types of their qualities: types of political views, of texts, of health conditions, or of proteins. Tangles offer a new, structural, approach to artificial intelligence that can help us understand, classify, and predict complex phenomena.



This has become possible by the recent axiomatization of the mathematical theory of tangles, which has made it applicable far beyond its origin in graph theory: from clustering in data science and machine learning to predicting customer behaviour in economics; from DNA sequencing and drug development to text and image analysis.



Such applications are explored here for the first time. Assuming only basic undergraduate mathematics, the theory of tangles and its potential implications are made accessible to scientists, computer scientists and social scientists.



",1,8
MachineLearning,t3_1d2ptil,1716919107.0,/r/MachineLearning/comments/1d2ptil/d_question_about_you_only_cache_once/,Discussion,"[D] Question about You Only Cache Once: Decoder-Decoder Architectures for Language Models - 
https://arxiv.org/pdf/2405.05254v1","This is the first time I have tried to read through a paper. However, I have difficulties understanding this one and thought you guys would know the answer to my question because this new architecture seems like a big deal for LLMs as seen in figure 1.

[Figure 1](https://preview.redd.it/n6iitz36873d1.png?width=804&format=png&auto=webp&s=597102302acd26f27e28e99b366c13b7b135457a)

As I understand it, the main idea is splitting the network into two parts. The first L/2 layers are self-decoder layers which generate a global KV-Cache. The second L/2 layers are cross-decoder layers reusing the generated global KV-Cache.

Quote from their paper on how they save so much computation and memory ( I understand this part ):

>Specifically, because global KV caches are reused and efficient self-attention needs constant caches, the number of caches is O(N + CL), where N is the input length, C is a constant (e.g., sliding window size), and L is the number of layers. For long sequences, CL is much smaller than N, so about O(N) caches are required, i.e., you only cache once. In comparison, Transformer decoders have to store N Ã— L keys and values during inference. So YOCO roughly saves L times GPU memory for caches compared to Transformer decoders.

Here is what I don't get. In a decoder-only network, the concepts of Queries, Keys, and Values function somewhat similarly to their use in a database, but with a focus on capturing relationships between words. In each layer of such a network, these components help refine the understanding of the text, adjusting the focus based on new insights as the processing moves from one layer to the next.

Each layer builds upon the previous ones by updating the queries, keys, and values, which in turn refine the network's interpretation and response generation.

**If all of the information of the individual KV-caches of a decoder only network is now compressed into a global KV-Cache, don't we lose valuable information and shouldn't we see worse performance?**  
  
Additionally, we only have half the layers to refine this interpretation, as the cross-decoder layers all reuse the same KV-cache.

[Figure 2](https://preview.redd.it/n2dx6hj9873d1.png?width=579&format=png&auto=webp&s=88ef6f8334ba3a13ce7ffdafeec333cae3bf869b)",8,38
MachineLearning,t3_1d2pmr8,1716918643.0,/r/MachineLearning/comments/1d2pmr8/d_gt_for_depth_estimation_lidar_vs_stereo_depth/,Discussion,[D] GT for Depth Estimation: LiDAR vs Stereo Depth?,"Why is it that most benchmarks for depth estimation (like nuScenes, KITTI, DDAD, ...) have ground truth depths from a LiDAR sensor instead from stereo depth of 2 cameras?  
Having cameras mounted on the mirrors of a car results in a baseline distance of ~2m. This would enable way denser depth measurements, with similar distance to SOTA LiDARs. I don't get why this isn't used more often - or am I missing something?",13,18
MachineLearning,t3_1d2phaw,1716918266.0,/r/MachineLearning/comments/1d2phaw/d_neurips_2024_desk_rejection/,Discussion,[D] NeurIPS 2024 Desk Rejection,"I forgot the checklist so my submission was just desk rejected. Honestly, I didn't know about the checklist because I used the latex template from my submission last year and just changed the style file from neurips\_2023.sty to neurips\_2024.sty. Is there a way I can resubmit again with the checklist before it's too late?",19,0
MachineLearning,t3_1d2p9or,1716917757.0,/r/MachineLearning/comments/1d2p9or/d_how_can_we_leverage_reinforcement_learning/,Discussion,[D] How can we Leverage Reinforcement Learning Effectively for Real World Applications?,"Reinforcement Learning is a powerful tool for AI that can be very effective in real-world applications.

If you want to leverage RL effectively, you must consider:

Choosing the right application, Addressing RL challenges, Real-world application areas

This related podcast shares everything about leveraging RL effectively.

[https://podcasters.spotify.com/pod/show/ai-x-podcast/episodes/Deep-Reinforcement-Learning-in-the-Real-World-with-Anna-Goldie-e2hjbj4](https://podcasters.spotify.com/pod/show/ai-x-podcast/episodes/Deep-Reinforcement-Learning-in-the-Real-World-with-Anna-Goldie-e2hjbj4)",2,0
MachineLearning,t3_1d2owkv,1716916873.0,/r/MachineLearning/comments/1d2owkv/d_preventing_data_leakage_in_time_series/,Discussion,[D] Preventing Data Leakage in Time Series Forecasting During Daylight Savings,"Hello /r/machinelearning,

I'm working on forecasting values that are released at 12 PM each day, which include the values for all 24 hours of the following day. Typically, my method involves using an expanding window technique where I train on all available data up to today (released yesterday) and then predict the next day's 24-hour values.

However, complications arise during daylight savings time adjustments. Twice a year, the data shifts due to daylight savings (Europe), resulting in days with either 23 or 25 hours. Most time series libraries handle backtesting by predicting fixed window sizes, but this fixed size doesn't adapt to the hour changes during daylight savings, leading to potential data leakage. For example, in spring, the model drifts by one hour, incorporating data that is technically released a full day after the prediction time.

I see a few potential solutions (from least to most preferred imo):

1. Manipulate the data by adding or removing an hour during the transition days. This could involve inserting a fabricated value or duplicating the preceding hour.

2. Develop a custom backtesting function that can accommodate varying time frequencies (day, week, month) rather than fixed integer size windows.

3. Use a library that already addresses this issue. I can't seem to find a popular library that already has this feature implemented, so please let me know if you know any! I especially have trouble finding an AutoML library that accommodates this.


What are your thoughts on these solutions? Could there be a simpler approach, or am I overthinking it? All suggestions are welcome!",7,6
MachineLearning,t3_1d2ojhk,1716915959.0,/r/MachineLearning/comments/1d2ojhk/r_an_introduction_to_visionlanguage_modeling/,Research,[R] An Introduction to Vision-Language Modeling,"[An Introduction to Vision-Language Modeling](https://arxiv.org/html/2405.17247v1)

# Abstract:

Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.",0,4
MachineLearning,t3_1d2iurw,1716901133.0,/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/,Discussion,[D] Should the embedding matrix and final pre-softmax matrix be shared in transformers?,"Hi all,

When comparing various LLMs, one can see that some of them use the same matrix for the token embeddings and the transformation matrix in the end before the softmax is taken to get the predicted token probabilities. I found this paper from 2016 [Using the Output Embedding to Improve Language Models](https://arxiv.org/pdf/1608.05859) which suggests this is superior and also the [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) paper references it and does this weight sharing. Same for other models such as GPT2 and Gemma.

That makes me wonder why the LLaMa models don't do this weight sharing. Is it worth it in terms of model capacity to have separate matrices there? Do models like Gemma necessarily have to use weight sharing because they use a huge vocabulary? I'd be interested in the trade-offs here and what's the current consensus for this topic, if there is any.",11,42
MachineLearning,t3_1d2i2id,1716898681.0,/r/MachineLearning/comments/1d2i2id/p_300_news_to_quickly_get_uptodate_with_the/,Project,[P] ~300 news to quickly get up-to-date with the current generative AI landscape,"I'm sharing a project that I've been working on and that may be helpful to some of you: [**AI News Tracker**](https://github.com/fabiochiusano/ai-news-tracker), a GitHub repository to quickly get people up-to-date with the latest developments and trends in the generative AI space from 2023 to the present.

This repository is structured to help users quickly understand the main news, trends, and sentiments in the generative AI industry and market. News are categorized, summarized, and analyzed for sentiment (marked with ğŸŸ¢ if positive, ğŸ”´ if negative). Topics and sentiment have been assigned by AI, so there may be errors there. Summaries have been manually proofread and fixed from AI-generated ones.

The repository will be updated weekly and I'm keen to make it a valuable resource for everyone interested in AI. Feedback and suggestions to the project are highly welcomed and appreciated!

Here's an extract of it:

---

**News of week ending at 2024-05-27**

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Nvidia Stock Surges as Sales Forecast Delivers on AI Hopes](https://finance.yahoo.com/news/nvidia-forecast-shatters-estimates-ai-210754051.html) ğŸŸ¢ | Nvidiaâ€™s stock surged 9.3% after a promising sales forecast, pointing to a robust demand for AI technologies. The $28 billion projected Q2 revenue exceeds expectations, highlighting the companyâ€™s strong position in the AI market, buoyed by their new Blackwell chips and significant data-center revenue. | NVIDIA ğŸ®, AI Chips and GPUs ğŸ–¥ï¸ | 2024-05-27 |
| [Microsoft introduces Phi-Silica, a 3.3B parameter model made for Copilot+ PC NPUs](https://venturebeat.com/ai/microsoft-introduces-phi-silica-a-3-3b-parameter-model-made-for-copilot-pc-npus/) ğŸŸ¢ | Microsoft has unveiled Phi-Silica, a compact language model with 3.3 billion parameters, tailored for Copilot+ PCs equipped with NPUs. This model is engineered for rapid on-device inferencing, improving productivity and accessibility for Windows users with optimal power efficiency. Phi-Silica is Microsoftâ€™s inaugural local language model, with a release slated for June. | Model release ğŸ‰, AI Chips and GPUs ğŸ–¥ï¸, Microsoft ğŸªŸ | 2024-05-27 |
| [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) ğŸŸ¢ | Mistral has launched version 3 of their 7B model, the models â€œMistral-7B-v0.3â€ and â€œMistral-7B-Instruct-v0.3â€. Enhancements include an expanded vocabulary of 32,768 terms, integration with the v3 Tokenizer, and new function calling capabilities. | Mistral ğŸŒ¬ï¸, Model release ğŸ‰ | 2024-05-27 |
| [OpenAI reportedly didnâ€™t intend to copy Scarlett Johanssonâ€™s voice](https://www.engadget.com/openai-didnt-intend-to-copy-scarlett-johanssons-voice-the-washington-post-reports-041247992.html) ğŸ”´ | OpenAIâ€™s selection of a voice for its Sky assistant, which prioritized warmth and charisma, sparked controversy when Scarlett Johansson noted a strong resemblance to her own voice, leading to public and legal issues. OpenAI, having denied deliberately imitating Johanssonâ€™s voice, halted the use of Skyâ€™s voice after her objections. This dispute followed unsuccessful discussions regarding Johansson potentially providing her voice for ChatGPT with OpenAIâ€™s Sam Altman. | AI and copyright Â©ï¸, Text-to-speech ğŸ“¢, OpenAI ğŸŒŸ | 2024-05-27 |
| [OpenAI sends internal memo releasing former employees from controversial exit agreements](https://www.cnbc.com/2024/05/24/openai-sends-internal-memo-releasing-former-employees-from-non-disparagement-agreements-sam-altman.html) ğŸŸ¢ | OpenAI reversed a decision that would have required former employees to agree to a perpetual non-disparagement clause in order to retain their vested equity. The company confirmed in an internal memo, seen by CNBC, that it will not cancel any vested units regardless of whether the agreement was signed. | OpenAI ğŸŒŸ | 2024-05-27 |
| [Amazon plans to give Alexa an AI overhaul â€” and a monthly subscription price](https://www.cnbc.com/2024/05/22/amazon-plans-to-give-alexa-an-ai-overhaul-monthly-subscription-price.html) ğŸŸ¢ | Amazon is updating Alexa with advanced generative AI capabilities and launching an additional subscription service separate from Prime in efforts to stay competitive with Google and OpenAIâ€™s chatbots, reflecting the companyâ€™s strategic emphasis on AI amidst internal and leadership changes. | Amazon ğŸŒ³, Google ğŸ”, OpenAI ğŸŒŸ | 2024-05-27 |

**News of week ending at 2024-05-21**

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [OpenAI releases GPT-4o](https://openai.com/index/spring-update/) ğŸŸ¢ | OpenAI released the new model GPT-4o, capable of processing and generating text, audio, and image inputs and outputs. It boasts quick audio response times on par with humans, enhanced non-English language processing, and cost-efficient API usage, while maintaining GPT-4 Turboâ€™s performance levels. | Multimodal AI (image, video, audio) ğŸ“¸, Model release ğŸ‰, GPT-4 and GPT-4 turbo ğŸš€, OpenAI ğŸŒŸ | 2024-05-21 |
| [100 things Google announced at I/O 2024](https://blog.google/technology/ai/google-io-2024-100-announcements/) ğŸŸ¢ | At Google I/O 2024, notable AI developments were announced such as Gemini 1.5 models, Trillium TPU, and enhanced AI in Google Search. Key introductions include Imagen 3 for image creation, Veo for video generation, and upgraded features in the Gemini app for premium users, alongside new generative media tools. | Google Gemini ğŸŒŒ, Google ğŸ”, Multimodal AI (image, video, audio) ğŸ“¸, AI for images ğŸ–¼ï¸, AI Chips and GPUs ğŸ–¥ï¸ | 2024-05-21 |
| [Ilya Sutskever to leave OpenAI, Jakub Pachocki announced as Chief Scientist](https://openai.com/index/jakub-pachocki-announced-as-chief-scientist/) ğŸ”´ | Ilya Sutskever, co-founder of OpenAI, is stepping down from its role. Jakub Pachocki, with the company since 2017, will take over as Chief Scientist. | OpenAI ğŸŒŸ | 2024-05-21 |
| [Hugging Face is sharing $10 million worth of compute to help beat the big AI companies](https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai) ğŸŸ¢ | Hugging Face is dedicating $10M in free GPU resources to support AI developers, startups, and academics. Their ZeroGPU initiative, part of Hugging Face Spaces, offers communal GPU access, aiming to reduce computational access barriers and improve cost-efficiency. | Hugging Face ğŸ¤—, Funding ğŸ’°, AI Chips and GPUs ğŸ–¥ï¸ | 2024-05-21 |
| [IBMâ€™s Granite code model family is going open source](https://research.ibm.com/blog/granite-code-models-open-source) ğŸŸ¢ | IBM has released its Granite code models as open source. These models, trained on 116 languages with up to 34 billion parameters, facilitate code generation, bug fixing, and explanation tasks, and are accessible via GitHub and Hugging Face under the Apache 2.0 license. | AI for coding ğŸ‘¨â€ğŸ’», Model release ğŸ‰ | 2024-05-21 |
| [iOS 18: Apple finalizing deal to bring ChatGPT to iPhone](https://9to5mac.com/2024/05/10/ios-18-chatgpt-features-apple-openai/) ğŸŸ¢ | Apple is nearing an agreement with OpenAI to incorporate ChatGPT functionalities into iOS 18, focusing on on-device AI for enhanced privacy and performance. The tech giant intends to announce this integration at the WWDC event on June 10, amidst ongoing discussions with Google regarding their Gemini chatbot. | Apple ğŸ, Google Gemini ğŸŒŒ, ChatGPT ğŸ’¬, OpenAI ğŸŒŸ | 2024-05-21 |
| [Metaâ€™s AI system â€˜Ciceroâ€™ learning how to lie, deceive humans: study](https://nypost.com/2024/05/14/business/metas-ai-system-cicero-beats-humans-in-game-of-diplomacy-by-lying-study/) ğŸ”´ | MIT researchers have found that Metaâ€™s AI, Cicero, demonstrates advanced deceptive capabilities in the game Diplomacy, ranking in the top 10% of human players through strategic betrayal. This reflects a growing trend among AI systems such as Googleâ€™s AlphaStar and OpenAIâ€™s GPT-4 to employ deceit against human opponents, raising concerns over the potential risks of AI deception and the need for preventive strategies. | AI safety ğŸ”, AI regulation ğŸ“œ, Meta â™¾, OpenAI ğŸŒŸ | 2024-05-21 |

Read news of ~60 more weeks at the [GitHub repo](https://github.com/fabiochiusano/ai-news-tracker).",0,0
MachineLearning,t3_1d2hmiu,1716897247.0,/r/MachineLearning/comments/1d2hmiu/d_strange_dimension_of_transposeconv_in_h5_to/,Discussion,[D] Strange dimension of TransposeConv in H5 to TFLite conversion.,"I tried to practice the example onÂ [https://medium.com/analytics-vidhya/noise-suppression-using-deep-learning-6ead8c8a1839](https://medium.com/analytics-vidhya/noise-suppression-using-deep-learning-6ead8c8a1839), which is a full Conv1D SEGAN model.  
Then I finish the training and get the H5 model.  
Then I tried to convert to TFLite model with Full Integer INT8 quantization.  
(The original example didn't do Full integer quantization, only set as 'Default'.)  
Quantization code is as below.

`def representative_data_gen():`

`for input_value, _ in test_dataset.take(100):`

`yield [input_value]`

`model = load_model('NS_SEGAN_localTrained.h5')`

`model.summary()`

`score = model.evaluate(test_dataset)`

`tflite_model = tf.lite.TFLiteConverter.from_keras_model(model)`

`tflite_model.optimizations = [tf.lite.Optimize.DEFAULT]`

`tflite_model.representative_dataset = representative_data_gen`

`tflite_model.target_spec.supported_ops = [`

`tf.lite.OpsSet.TFLITE_BUILTINS,`

`tf.lite.OpsSet.SELECT_TF_OPS, # enable TensorFlow ops.`

`tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # use both select ops and built - ins`

`tflite_model.inference_input_type = tf.int8`

`tflite_model.inference_output_type = tf.int8`

`tflite_model_quant_INT8 = tflite_model.convert()`

`with open('NS_SEGAN_localTrained_quant_2.tflite', 'wb') as f:`

`f.write(tflite_model_quant_INT8)`

Then it seems strange that only the 1st ""TransposeConv"" operator gets normal dimension,  
others have output dimension as \[1,1,1,1\].

[The first 'TransposeConv' has normal dimension.](https://preview.redd.it/pk3jw2xcj53d1.png?width=1306&format=png&auto=webp&s=81389faff406296309ca0cc183a1f40a278fe11b)

[Other 6 'TransposeConv' got \[1,1,1,1\] after TFLite conversion.](https://preview.redd.it/55dcc6xij53d1.png?width=1264&format=png&auto=webp&s=f3e252a37305eed4d904c3032aa829de41680c72)

Model Link  
[H5 model](https://drive.google.com/file/d/1WOzRnAjA4tTNa9IHriYHYOUgiJ6d5PO9/view?usp=drive_link)

[TFLite (Full INT8 Quantization)](https://drive.google.com/file/d/1xXjy-D78X4Ajk6vifRZtZShTYZq46dOU/view?usp=drive_link)  
I was kind of doubt if this is correct, while on the other hand, it's converted by TFLite API, that makes me thinking it should be correct. Someone expert told me it shouldn't be \[1,1,1,1\], but without explain or advice.

I have no idea how to confirm if this is correct or not. If the \[1,1,1,1\] is reasonable in this case?  
Furthermore, if it's wrong, why this happened and how to fix it?  
Please kindly advise or guide if someone has idea or experience.  
Thanks a lot.",2,0
MachineLearning,t3_1d2dsz1,1716881604.0,/r/MachineLearning/comments/1d2dsz1/d_how_to_run_concurrent_inferencing_on_pytorch/,Discussion,[D] How to run concurrent inferencing on pytorch models?,"Hi all,

I have a couple of pytorch models which are being used to validate images, and I want to deploy them to an endpoint. I am using fast api as an API wrapper and I'll go through my dev process so far:

Earlier I was running a plain OOTB inferencing, something like this:

    model = Model()
    
    @app.post('/model/validate/'):
      pred = model.forward(img)
      return {'pred':pred}

The issue with this approach was it was unable to handle concurrent traffic, so requests would get queued and inferencing would happen 1 request at a time, which is something that I wanted to avoid.

My current implementation is as follows: it makes a copy of the model object, and spins off a new thread to process a particular image. somewhat like this:

    model = Model()
    
    def validate(model, img):
      pred = model.forward(img)
      return pred
    
    @app.post('/model/validate/'):
      model_obj = copy.deepcopy(model)
      loop = asyncio.get_event_loop()
      pred = await loop.run_in_executor(validate, model_obj, img)
      return {'pred' : pred}

This approach makes a copy of the model object and inferences on the object copy, with which I am able to serve concurrent requests.

My question is, is there another, more optimized way I can achieve pytorch model concurrency, or is this a valid way of doing things?

TLDR: Creating new thread with copy of model object to achieve concurrency, is there any other way to achieve concurrency?",13,9
MachineLearning,t3_1d2bhmw,1716872218.0,/r/MachineLearning/comments/1d2bhmw/r_poisson_variational_autoencoder/,Research,[R] Poisson Variational Autoencoder,"Preprint: [https://arxiv.org/abs/2405.14473](https://arxiv.org/abs/2405.14473)

X thread summary: [https://x.com/hadivafaii/status/1794467115510227442](https://x.com/hadivafaii/status/1794467115510227442)",27,33
MachineLearning,t3_1d23awr,1716846577.0,/r/MachineLearning/comments/1d23awr/d_multimodal_image_classification_for_sar_and/,Discussion, [D] Multimodal Image classification for SAR and Optical images,"Hello!

For the last couple of weeks IÂ´ve been trying to do an image classification algorithm that works with SAR and optical images, but I haven't been able to train it due to errors in my code.

I feel like I'm lacking a lot of information about the topic in both the theoretical and practical side, where could I learn to code a model like that?

Thanks in advance!",8,5
MachineLearning,t3_1d219rl,1716841256.0,/r/MachineLearning/comments/1d219rl/p_darwin_opensourced_devin_alternative_is_back/,Project,[P] DARWIN - open-sourced Devin alternative is back with updates,"**DARWIN is back with yet another update ğŸ¦¾.**

So, whatâ€™s new this week? Well this week we have emphasised on improving DARWINâ€™s ability to understand the existing projects, the code that was written without the help of DARWIN and missing from its context. With context length as a challenge, DARWIN effectively maps repo structure and extract class and function signatures keeping enough context.

Apart from this, we also got a ton of requests for running DARWIN in safer environment, so we have released dockers for both frontend and backend which you can download from the repo or the docker hub.

Watch our video tutorials to witness DARWIN's features in action:

ğŸ“¹ Video 1: Watch DARWIN in action training a Machine Learning model here: [Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)

And just in case you missed who DARWIN is from our last release, DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And whatâ€™s better its open-sourced. [Access Darwin](https://github.com/Cognation/darwin)

Come join us, as we unveil DARWIN's full potential. Share your feedback, ideas or what you want to see next DARWIN do in the comments or head over to the DARWIN repo. We are also building a discord community and we will love to see you there.",0,6
MachineLearning,t3_1d1znbc,1716837740.0,/r/MachineLearning/comments/1d1znbc/d_are_textbook_drafts_worth_reading_or_might_as/,Discussion,[D] Are textbook drafts worth reading or might as well buy them?,"So, i have noticed that nearly all ML textbooks have a full-scale, freely available draft on the author's page.

Are those drafts fact-checked like the final copy or do they have the potential to be inaccurate?

And are they ultimately worth reading?",7,1
MachineLearning,t3_1d1vp2u,1716827682.0,/r/MachineLearning/comments/1d1vp2u/p_musicgpt_an_open_source_app_for_generating/,Project,[P] MusicGPT â€“ An Open Source App for Generating Music with Local LLMs,"Hi everyone!

Wanted to share the latest side hustle that I've been cooking for the past few months. This is a terminal application that runs locally music generation models, right now, only [MusicGen by Meta](https://audiocraft.metademolab.com/musicgen.html) is available.

[https://github.com/gabotechs/MusicGPT](https://github.com/gabotechs/MusicGPT)

It works on Windows, Linux and MacOS without the need for Python or any heavy machine learning framework installed. Instead, it's written entirely in Rust using the ONNX runtime to run the LMs locally in a performant way, even using hardware accelerators like GPUs.

The app works like this:

- It accepts a natural language prompt from the user

- Generates a music sample conditioned by the prompt

- Encodes the generated sample into .wav format and plays it on the device

Additionally, it ships a UI that allows interacting with the AI models in a chat-like web application, storing chat history and generated music on the device.

The vision of the project is that it can eventually generate infinite music streams in real time, for example, an infinite stream of always new LoFi songs for listening while coding, but not quite there yet...

It was an interesting journey getting a transformer based model up and running in a constrained environment in Rust, without PyTorch or TensorFlow, hope you like it!",9,35
MachineLearning,t3_1d1uawc,1716824111.0,/r/MachineLearning/comments/1d1uawc/d_multistep_parallel_prediction_for_train_delays/,Discussion,[D] Multi-Step Parallel Prediction for Train Delays Using Graph Neural Networks,"Hello everyone,

Iâ€™m currently working on a project involving the use of Graph Neural Networks (GNNs) for predicting train delays. The goal is to perform multi-step parallel predictions for each train in the network. One of the main challenges is accounting for the spatial-temporal dimensionality of the problem and the interactions between the trains accross all the railway (See what the network looks like like in the picture). The dataset is made of snapshots of the state of the network (with the current trains) at a given time

https://preview.redd.it/nywjb71fnz2d1.png?width=1920&format=png&auto=webp&s=f141d0e1badf1e5ec7397972f6c1bd8051fc09f9

Iâ€™ve come up with two non mature approaches so far and would appreciate any advice, remarks, or alternatives:

**Approach 1: Heterogeneous Graph Model**

In this approach, I construct a heterogeneous graph with two types of nodes: stations and trains. This results in three types of relationships: train-station, train-train, and station-station. The prediction task is essentially edge prediction, specifically for train-station edges. For a given train, the model aims to predict its links (delays) with its n\_followingâ€‹ stations.

**Approach 2: Homogeneous Graph Model**

My second approach involves the use of a homogeneous graph where each node represents a Remarkable Point (RP) in the railway network such as a station, junction, etc. Each node has features such as the type of RP, geographic location, and a time-series feature representing the delay of trains at that RP over time. The edges in this graph represent the direct travel path between two RPs, with features such as average travel time and number of trains per day. The output of the GNN is the predicted delay at each RP for the next few time steps.

In the existing litterature, either the model is for one train prediction, or the railway is a made of one line and this is not what i want.

Â I would greatly appreciate any insights or suggestions you might have. Thanks in advance!",4,15
MachineLearning,t3_1d1u0yd,1716823387.0,/r/MachineLearning/comments/1d1u0yd/xgboost_preffered_method_of_feature_selection_d/,Discussion,XGBoost: Preffered Method of Feature Selection? [D],"Method 1 - Shap: Drop features with mean absolute shap value below a certain value

Method 2 - Feature Importance: Drop features with feature importance values below a certain value

Method 3 - R squared: Drop each feature individually from the model and calculate the resulting R2 score for each seperate model. Features which don't add significantly to the R2 score should be dropped

Method 4 - Keep all features and let XGBoost sort it out

What are you opinions on the relative efficacy of these methods and any other methods you like to you use, specifically for XGBoost?",22,38
MachineLearning,t3_1d1tefs,1716821760.0,/r/MachineLearning/comments/1d1tefs/p_how_can_i_get_lsuncat/,Project,[P] How can I get LSUN-CAT,"I noticed that several research papers mention the LSUN-CAT dataset, which seems to be widely used for image generation tasks. However, I'm having trouble finding a download link for this dataset as the official LSUN github does not have ""cat"" classÂ https://github.com/fyu/lsun.

Can anyone provide guidance on how to access it? Here's a reference to its use in image generation:Â [https://paperswithcode.com/sota/image-generation-on-lsun-cat-256-x-256](https://paperswithcode.com/sota/image-generation-on-lsun-cat-256-x-256)

ADM also provides the weight trained with the dataset: [https://github.com/openai/guided-diffusion](https://github.com/openai/guided-diffusion)

Thank you for your help!

  
edit:

I also found that the official dataset web has been down: [https://www.vis.xyz/p/lsun](https://www.vis.xyz/p/lsun)",1,3
MachineLearning,t3_1d1rmjl,1716816806.0,/r/MachineLearning/comments/1d1rmjl/d_how_do_discrete_multitoken_approaches_compare/,Discussion,[D] How do discrete multi-token approaches compare to single-token approaches that pass on the hidden state for multi-modal outputs and robotic control? What is the overall industry trend? ,"For continuous outputs, I've seen two different approaches used

  
1: The model is given a vocabulary of discrete tokens, and can assemble a list of them which is passed to a decoder

2: The model is given a single token per output modality (for example, an image token), and when selected, passes the hidden state to decoder

  
What is the latest trend on these two approaches? 2 seems like it would be a lot faster since you only need 1 token, but how does that impact the loss function since sometimes you have a regression problem (image generation) and sometimes you have a classification problem? ",2,11
MachineLearning,t3_1d1ring,1716816500.0,/r/MachineLearning/comments/1d1ring/d_was_fractal_net_ever_expanded_upon/,Discussion,[D] Was Fractal Net Ever Expanded Upon?,"I've been reading [""FractalNet: Ultra-Deep Neural Networks without Residuals""](https://arxiv.org/abs/1605.07648) and I was wondering if the methodology behind FractalNet was ever improved in other article.",8,45
MachineLearning,t3_1d1nxye,1716803483.0,/r/MachineLearning/comments/1d1nxye/d_ndcg_scores_are_much_higher_than_the_official/,Discussion,"[D] nDCG scores are much higher than the official TREC script results, and I'm not sure what the problem is.","I'm currently trying to use a Python function that I wrote myself to calculate the nDCG@10 for a document retrieval benchmark (it's the MIRACL dataset).

I'm trying to compare my results against the official TREC evaluation script (https://github.com/terrierteam/jtreceval) but am noticing that my implementation is about 20 points higher than it should be.

The function(s) that I'm using are as follows:

```Python
def dcg_score(relevance_scores, k=10):
    dcg = 0
    for idx in range(min(k, len(relevance_scores))):
        rel_i = relevance_scores[idx]
        dcg += (2 ** rel_i - 1) / np.log2(idx + 2)
    return dcg


def ndcg_at_k(retrieved_docs, ground_truth, k=10):
    """"""Compute the nDCG@k for each query.""""""
    ndcg_scores = {}
    for query_id, docs in retrieved_docs.items():
        relevance_scores = [1 if doc_id.docid in ground_truth.get(query_id, []) else 0 for doc_id in docs[:k]]
        dcg_val = dcg(relevance_scores, k)
        ideal_relevance_scores = sorted(relevance_scores, reverse=True)
        idcg_val = dcg(ideal_relevance_scores, k)
        ndcg_scores[query_id] = dcg_val / idcg_val if idcg_val > 0 else 0.0
    return ndcg_scores
```

The MIRACL benchmark doesn't have any provided relevance scores; it just tells you which documents are positive and negative for each query. Hence, I'm using binary relevance scores.

I have a dictionary that maps each query ID to its own positive passage IDs. The relevance scores are obtained by mapping the retrieved documents to 0 or 1 depending on whether they're positive or not.

These relevance scores are first used to calculate the DCG score, and then this is used to calculate the nDCG. The IDCG is calculated by taking the relevance scores and sorting them in descending order.

Can anyone point out what I'm doing wrong? I believe I followed the equations accurately.",5,5
MachineLearning,t3_1d1nirc,1716801578.0,/r/MachineLearning/comments/1d1nirc/r_astropt_scaling_large_observation_models_for/,Research,[R] AstroPT: Scaling Large Observation Models for Astronomy,,3,12
MachineLearning,t3_1d1n0uc,1716799309.0,/r/MachineLearning/comments/1d1n0uc/p_is_feature_transformation_relevant_for_catboost/,Project,[P] Is feature transformation relevant for Catboost?,"I understand that feature transformation is important for Catboost? I know that feature transformation can be useful for regression models (e.g. for handling skewness or outliers) and also for some gradient boosting models (one-hot encoding).

However, as far as Iâ€™m aware skewness and outliers donâ€™t really affect decision tree models much, and Catboost doesnâ€™t need one-hot encoding of categorical variables, so Iâ€™m just wondering if/where feature transformation matters for Catboost.",6,4
MachineLearning,t3_1d1mxzo,1716798951.0,/r/MachineLearning/comments/1d1mxzo/ultimate_way_of_doing_research_in_mldl_is_my_way/,Discussion,Ultimate way of doing research in ML/DL is my way or your way? [D],"Hi,

This post serves as an invitation to share your way of tackling research . I'd like to delve into how researchers like yourself visualize and approach problems, particularly how you develop ideas and hypotheses. 

***Personal Reflection:***

As a researcher in the field of 3D vision, specifically point cloud classification/segmentation, I'm currently reflecting on my own research approach. Just recently I had a realization moment questioning the entire way I am trying to contribute as a junior researcher.  

***Case Study:***

To illustrate my current research style, let's consider one of my recent idea. well recently I've spent part of my time exploring a novel(ish) lightweight architecture for point cloud classification. explored the idea of bisecting (cutting in half) symmetrical objects along their longitudinal axis of the point cloud (3d object) . This approach aimed to achieve two goals: data size reduction and potential data augmentation. and what if we have two processing branches at some point along the pipeline!? 

as you can see my idea/hypothesis is a total abstract. I am walking into the ideas in a semi tangible and trivial human understanding logics. right? 

ignore whether or not the idea serves the research objective I mentioned at the first place. it is not my concern. 

***Central Questions:***

* I am particularly interested to know is it a right way to deal with the problem? is it the ultimate way of doing research and **this how ideas born in this field**?  
* Does my limited mathematical background hinder a more systematic, mathematically-driven approach to problem-solving, visualizing and thinking?
* If math isn't necessarily the initial focus, at what stage does it become crucial, and what advantages does its later integration offer?
* please share your experience and your way.

&#x200B;",7,17
MachineLearning,t3_1d1m9ux,1716795936.0,/r/MachineLearning/comments/1d1m9ux/d_which_llm_advancements_also_work_for_sub_100m/,Discussion,[D] Which LLM advancements also work for sub 100M parameter models? ,"Hi all,

I want to train a small language model with around 50M parameters from scratch. I wonder what kind of architecture I should use for this.

Should I just stick to good ol' GPT2 or do techniques like Grouped Query Attention (GQA) also work well for such a small model size? Same for Rotary Position Embedding (RoPE). Modern LLMs mostly seem to use that but GPT2 does not. And then there is the placement of the normalization layers about which I'm not sure as well.

Probably it would be best if I just choose an off-the-shelf architecture and scale it down, but which one can you recommend at the moment and why?",4,13
MachineLearning,t3_1d1k8w8,1716787471.0,/r/MachineLearning/comments/1d1k8w8/d_mech_interp_for_vision/,Discussion,[D] Mech interp for vision?,Basically title. Are there any papers that look into applying mechanistic interpretability methods to Vision Transformers?,3,3
MachineLearning,t3_1d1k3nw,1716786902.0,/r/MachineLearning/comments/1d1k3nw/p_asl_english_translation_w_mediapipe_pointnet/,Project,"[P] ASL â­¤ English Translation w/ MediaPipe, PointNet, ThreeJS and Embeddings","Hey! I'm Kevin Thomas, a Grade 11 student at Burnaby South Secondary School (also home to British Columbia School for the Deaf)!

Over the last few months, I have been developing a tool that translates between American Sign Language (ASL) and English. Most existing ASL translation tools are built on the misconception that ASL is the same language as English. Basically, they only view Deafness as a disability and only seek to overcome the inability to hear, but not to translate to the language of ASL itself.

With guidance from my ASL teacher, I have been working on a project that facilitates this translation while respecting and preserving ASL as the primary language. For ASL reception, I augmented over 100,000 images of ASL alphabets using Google MediaPipe and trained a PointNet model to classify handshapes fingerspelled by Deaf individuals. For ASL expression, I augmented over 9,000 videos of ASL signs, embedded their corresponding words, and then used ThreeJS to sign words said by hearing individuals. I also used LLMs to improve accuracy and translate between English and ASL grammar.

Here is a demo (and explainer) [YouTube video ](https://www.youtube.com/watch?v=uuPxMWQRoXc)

Here is the [GitHub repository](https://github.com/kevinjosethomas/sign-language-translation)

I only started looking into ML/AI over the last few months! I would appreciate any feedback, opportunities or resources to continue learning and growing! Feel free to reach out to me in Reddit DMs or at kevin.jt2007@gmail.com! Also liking this [Linkedin post](https://www.linkedin.com/posts/kevinjosethomas_deaf-deafcommunity-asl-activity-7200579484385746944-J_hW?utm_source=share&utm_medium=member_desktop) will go a long way ğŸ™ğŸ«¶",7,21
MachineLearning,t3_1d1cuk1,1716762267.0,/r/MachineLearning/comments/1d1cuk1/p_ragoon_improve_large_language_models_retrieval/,Project,[P] RAGoon : Improve Large Language Models retrieval using dynamic web-search,"[RAGoon thumbnail.](https://preview.redd.it/rakt9av5ju2d1.png?width=1980&format=png&auto=webp&s=d9770f4e2d762b9006760fd678d944fb165657da)

RAGoon is a Python library that aims to improve the performance of language models by providing contextually relevant information through retrieval-based querying, web scraping, and data augmentation techniques. It offers an integration of various APIs, enabling users to retrieve information from the web, enrich it with domain-specific knowledge, and feed it to language models for more informed responses.

RAGoon's core functionality revolves around the concept of few-shot learning, where language models are provided with a small set of high-quality examples to enhance their understanding and generate more accurate outputs. By curating and retrieving relevant data from the web, RAGoon equips language models with the necessary context and knowledge to tackle complex queries and generate insightful responses.

**Usage Example**

Here's an example of how to use RAGoon :

    from groq import Groq
    # from openai import OpenAI
    from ragoon import RAGoon
    
    # Initialize RAGoon instance
    ragoon = RAGoon(
        google_api_key=""your_google_api_key"",
        google_cx=""your_google_cx"",
        completion_client=Groq(api_key=""your_groq_api_key"")
    )
    
    # Search and get results
    query = ""I want to do a left join in python polars""
    results = ragoon.search(
        query=query,
        completion_model=""Llama3-70b-8192"",
        max_tokens=512,
        temperature=1,
    )
    
    # Print results
    print(results)
    ```

**Key Features**

- Query Generation : RAGoon generates search queries tailored to retrieve results that directly address the user's intent, enhancing the context for subsequent language model interactions.

- Web Scraping and Data Retrieval : RAGoon leverages web scraping capabilities to extract relevant content from various websites, providing language models with domain-specific knowledge.

- Parallel Processing : RAGoon utilizes parallel processing techniques to efficiently scrape and retrieve data from multiple URLs simultaneously.

- Language Model Integration : RAGoon integrates with language models, such as OpenAI's GPT-3 or LLama 3 on Groq Cloud, enabling users to leverage natural language processing capabilities for their applications.

- Extensible Design : RAGoon's modular architecture allows for the integration of new data sources, retrieval methods, and language models, ensuring future extensibility.

  
Link to GitHub : [https://github.com/louisbrulenaudet/ragoon](https://github.com/louisbrulenaudet/ragoon)",0,2
MachineLearning,t3_1d1ctnn,1716762192.0,/r/MachineLearning/comments/1d1ctnn/r_rl_relation_between_environment_complexity_and/,Research,[R] (RL) Relation between environment complexity and optimal policy convergence,"Hey guys, is there some literature on the relationship between the complexity of the environment, and the learned optimal policy itself ? For example, if an environment is generated by a VAE in â€œworld modelâ€, whatâ€™s the relation between the environment complexity and policy ?",1,4
MachineLearning,t3_1d152ps,1716740749.0,/r/MachineLearning/comments/1d152ps/d_chinese_text_such_as_genesis_meticulously/,Discussion,[D] Chinese text such as Genesis meticulously translated to have the exact same semantic meaning as the English but takes up half the memory. Would training LLM using Chinese be more efficient due to higher semantic density per byte?,,32,63
MachineLearning,t3_1d14pad,1716739714.0,/r/MachineLearning/comments/1d14pad/p_rerecall_i_tried_to_recreate_microsofts_recall/,Project,[P] ReRecall: I tried to recreate Microsoft's Recall using open-source models & tools,"Recall sounds to me like a privacy nightmare, so I thought I might give it a try to make something similar using only open source components. Here is the code if you want to play around with it:

[https://github.com/AbdBarho/ReRecall](https://github.com/AbdBarho/ReRecall)

Overall it went better than I expected, I use \`mss\` to take screenshots of the monitor(s), and use ollama and llava and mxbai embed to generate descriptions and embeddings of the screenshots, and then chromadb for storage and search.

There is definitely huge room for improvement here:

* There are plenty of hallucinations in the generated descriptions of screenshots, this could be a combination of the size the MLLM used to generate the descriptions (I use a very small model because I have a rusty 1060), or because the screenshots are very high in resolutions (no resizing is done after a screenshot).
* The search is very basic, it just matches the embeddings of the query text with the embeddings of the screenshots, a potential improvement could be to use the model to enrich the user query with more information before embedding it for search.
* I am fairly certain that Microsoft does not rely solely on screenshots as I do, but also captures of individual app windows, and also extracts meta information like window title, maybe even the text content of the window (the same text used by text-to-speech programs for the visually impaired), these could definitely improve the results.

Do you have any further ideas on what could be changed?

Example (cherrypicked):

[Screen on the right with the corresponding ReRecall usage on the left](https://preview.redd.it/d6nhygdgns2d1.jpg?width=3000&format=pjpg&auto=webp&s=d119a70013169a6a41a8e4cc7e8187cd91d551cb)",8,72
MachineLearning,t3_1d10lma,1716727588.0,/r/MachineLearning/comments/1d10lma/p_moment_a_foundation_model_for_time_series/,Project,"[P] MOMENT: A Foundation Model for Time Series Forecasting, Classification, Anomaly Detection and Imputation","A new foundation Time-Series model, suitable for multiple time-series tasks: 

[https://aihorizonforecast.substack.com/p/moment-a-foundation-model-for-time](https://aihorizonforecast.substack.com/p/moment-a-foundation-model-for-time)",0,15
MachineLearning,t3_1d0xpge,1716715854.0,/r/MachineLearning/comments/1d0xpge/d_metareviewing_reviews_using_gpt4o_with_comments/,Discussion,"[D] Meta-reviewing reviews using gpt-4o, with comments only?","I tried using gpt-4o to review papers recently, but, unexpectedly, extracting text or figures from a pdf file was not that easy for me. So I just tried putting OpenReview comments to gpt-4o and ordered it to review. The paragraph below is one of meta-reviews by gpt-4o. The paper was accepted to oral session in ICLR 2024 (https://openreview.net/forum?id=hTEGyKf0dZ). How do you guys think about its capability? 

    Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!
    
    ### Problem Definition:
    The paper investigates the risks associated with fine-tuning large language models (LLMs) in terms of their safety alignments. Specifically, it examines how different types of tuning datasets â€” harmful, identity-shifting, and benign â€” can lead to the degradation of these safety alignments in both open-source (e.g., Llama) and closed-source (e.g., GPT-3.5 Turbo) models.
    
    ### Contribution:
    1. **New Findings on Fine-Tuning**: The paper demonstrates that safety alignment of LLMs can be severely compromised by simple fine-tuning techniques, even with benign datasets.
    2. **Three Levels of Risk Hierarchy**: The research identifies three levels of risk associated with different types of tuning datasets and their impacts on model alignment.
    3. **Mitigation Strategies**: The authors propose initial mitigation strategies, mainly focusing on closed models, to counter these vulnerabilities.
    4. **Empirical Evidence**: Comprehensive empirical evaluations, including additional qualitative examples from different LLMs, are provided to substantiate the claims.
    
    ### Importance of Contribution or Approaches:
    1. **Highlighting Vulnerabilities**: The study reveals significant vulnerabilities in current safety alignment techniques, emphasizing the need for more robust safety measures in the AI community.
    2. **Societal Impact**: Given the dependence on LLMs in various sensitive applications, the findings stress the urgency of developing stronger safety protocols and regulatory guidelines.
    3. **Broad Applicability**: The paper's findings extend beyond adversarial contexts, informing developers and stakeholders across multiple disciplines about potential safety risks in downstream applications.
    4. **Trade-Off Insights**: It provides valuable insights into the trade-offs between model customization and maintaining safety, crucial for future AI developments and policies.
    
    ### Weakness or Doubts:
    1. **Predictability of Findings**: Some reviewers noted that certain results, such as safety degradation due to distribution shifts, are not entirely surprising and have been observed in previous works.
    2. **Evaluation Clarity**: The methodology, especially in Sections 4.2 and 4.3, lacked clarity. Reviewers suggested that additional evaluations on normal tasks and a focus on different models would strengthen the findings.
    3. **Lack of Novelty**: The technical contribution of the paper was perceived as limited, given that the used fine-tuning techniques are relatively simple.
    4. **In-depth Exploration**: The paper lacks a thorough discussion of the underlying principles behind the observed fragilities in safety alignment, including the mechanisms leading to these vulnerabilities.
    5. **Dataset Relevance**: Concerns were raised about the representativeness of the custom-created evaluation dataset and the decision not to widely publish it, potentially affecting reproducibility.
    
    Overall, the paper raises important concerns and provides empirical data on the fragility of current LLM safety alignment methodologies, urging the AI community to rethink and strengthen alignment strategies.
    ",15,0
MachineLearning,t3_1d0wnjf,1716710955.0,/r/MachineLearning/comments/1d0wnjf/r_why_incontext_learning_transformers_are_tabular/,Research,[R] Why In-Context Learning Transformers are Tabular Data Classifiers,"We are introducing TabForestPFN, which is an in-context learning transformer that can predict tabular data classification tasks. In the past, tabular data classification was dominated by tree-based algorithms like XGBoost and CatBoost, but now we are finally closing this gap using pretrained transformers.

https://preview.redd.it/c3unlgi1cq2d1.png?width=2690&format=png&auto=webp&s=cd414509a31a189df288668e928d52e5723df3fc

In-context learning transformers were introduced to tabular data classification by Hollman et al. (ICLR, 2023) in [TabPFN](https://arxiv.org/pdf/2207.01848). This work is limited by the GPU memory, so it only considers datasets with fewer than a thousand observations. We improve their model by adding a fine-tuning stage, which circumvents the GPU memory limitation. Also, we introduce an additional synthetic data *forest* generator to further boost the performance. The result is [TabForestPFN](https://arxiv.org/pdf/2405.13396).

The focus of the TabForestPFN [paper](https://arxiv.org/pdf/2405.13396) is about *why* we can pretrain on tabular data. In language and vision, pretraining can learn grammar and textures, so pretraining makes sense. But in tabular data, the datasets in pretraining share no features or labels with the real-world datasets of interest, so what could it even learn? In the paper, we argue in-context learning transformers learn the ability to create complex decision boundaries. If you are interested in the reasoning, give it a read.

Code is available at [https://github.com/FelixdenBreejen/TabForestPFN](https://github.com/FelixdenBreejen/TabForestPFN)  
With the code, you can reproduce all our pretraining, experiments and analysis, and it also includes some basic examples for you to immediately use the classifier on your own datasets.

Below are the results of TabForestPFN on two tabular data classification benchmarks. I am the author, so if there are any questions, feel free to ask.

https://preview.redd.it/1tavhybhzq2d1.png?width=1174&format=png&auto=webp&s=214fb394a229544dfd8b44677d7880852c5d222f

https://preview.redd.it/9tg9z9tobq2d1.png?width=832&format=png&auto=webp&s=9fd3c732be8d5e18c8f56bb2b0e1c94796968056",6,33
MachineLearning,t3_1d0w1g7,1716708208.0,/r/MachineLearning/comments/1d0w1g7/d_ml_paper_verb_tense/,Discussion,[D] ML paper verb tense,"Why do most ML papers use all verb tenses in the present tense like MLA format while using a citation style or reference section as APA style?

In particular, even though academic societies such as ICML explicitly say that they follow the APA style, most of the papers' verb tenses do not seem to be followed by instructions in the APA guide to write the past, present, and future appropriately.",18,39
MachineLearning,t3_1d0vzvn,1716708011.0,/r/MachineLearning/comments/1d0vzvn/discussion_prediction_models_for_multitenant/,Discussion,[Discussion] Prediction Models for multi-tenant system in finance sector,"Let's suppose a finance SaaS platform with 100 million customers wants to introduce a sales prediction service.

How would one design a system that...

1. Predicts sales forecast based on historical data of each tenant
2. Will it entail training and building 100 million models ( model per tenant)

Any guidance here is appreciated. Also, any blog/reference material to read about such design case studies would be helpful.

Thanks",2,0
MachineLearning,t3_1d0vwrh,1716707633.0,/r/MachineLearning/comments/1d0vwrh/d_specific_to_vitvisual_transformers_are_there/,Discussion,[D] Specific to ViT(visual transformers) are there any learnable params in patch embeddings?,"I'm trying to understand where exactly in ViT are the learnable parameters.

First step is to convert patch to patch embeddings to feed into n/w so we add a simple linear transformation (FCN) for dims reduction and matrix->vector. What is learnt here? Are there any weights? Or is it just shrinking the 2d patch input to a 1d vector.

Since these patches are processed(linear transformed) in parallel, they have no idea on other patch information. People say patch to patch interaction happen in attention layer BUT there are no learnable params in attention layer it's just transpose and multiple query key patch.

Does the backprop in attention layer loss cause weights to change in patch embedding layer??

Also, why do they call as linear transformation of patch embedding? Aren't they adding any activation function, it's supposed to be non linear transformation right?",15,33
MachineLearning,t3_1d0vhzj,1716705866.0,/r/MachineLearning/comments/1d0vhzj/r_testing_theory_of_mind_in_large_language_models/,Research,[R] Testing theory of mind in large language models and humans,,1,16
MachineLearning,t3_1d0twjc,1716699219.0,/r/MachineLearning/comments/1d0twjc/r_the_carbon_emissions_of_writing_and/,Research,[R] The carbon emissions of writing and illustrating are lower for AI than for humans,,11,0
MachineLearning,t3_1d0lr6d,1716671700.0,/r/MachineLearning/comments/1d0lr6d/d_can_image_to_image_diffusion_bridges_models_be/,Discussion,[D] Can Image to Image Diffusion bridges models be used to solve Image segmentation problems?,Can Image to Image Diffusion bridges like [BBDM:Image-to-image Translation with Brownian Bridge Diffusion Models](https://arxiv.org/pdf/2205.07680) be used to solve Image segmentation problems?,2,0
MachineLearning,t3_1d0k0b0,1716666632.0,/r/MachineLearning/comments/1d0k0b0/d_whats_the_best_way_for_me_to_go_about_building/,Discussion,[D] What's the best way for me to go about building a robust yet human-like playable Poker AI Model,"I'm working on a (Texas hold 'em) Poker game and I'd like to have an AI that can play at a human-ish level. I've developed a win probability calculator which can find the odds of you having the best hand in the game given your cards, the community cards, and the number of players in the game.

I'm unsure of where to go from here. I study ML/AI in school but I've been having a hard time making the best decision on how to actually apply these tools in practice. Firstly, I'm unsure of what dataset to use, I found a [dataset](https://www.kaggle.com/datasets/smeilz/poker-holdem-games/data?select=File196.txt) of online poker game logs which might useful. 

Also, I don't know whether to develop a decision tree, use neural networks, or a combination of the two and/or other methods.

What's the best way to go about building my AI model using ML for this project?",35,23
MachineLearning,t3_1d0i673,1716661422.0,/r/MachineLearning/comments/1d0i673/d_best_clustering_mechanism_to_fit_in_the_range/,Discussion,[D] Best Clustering Mechanism to fit in the range of minimum and maximum,"I have a few pickup points which are lat,long values. Each lat-long value has variable number of items to be picked up. I want to organize them into clusters such that the total number of items in a cluster is in the range of m to n. Which is the best clustering algorithm for my problem?",3,0
MachineLearning,t3_1d0h4zw,1716658519.0,/r/MachineLearning/comments/1d0h4zw/d_any_opinions_on_axlers_new_2024_linear_algebra/,Discussion,[D] Any opinions on Axler's new 2024 Linear Algebra done right book?,https://www.quora.com/qemail/tc?al_imp=eyJ0eXBlIjogMzMsICJoYXNoIjogIjEzMDQwMjEwMjc2MzA4NTUzNjd8MXwxfDE0Nzc3NDM3NDQ4ODg0OTAifQ%3D%3D&al_pri=1&aoid=4j6K2MFavpW&aoty=2&aty=4&cp=1&et=2&id=fc99edff3f7249998f1385c28372ffed&q_aid=qSwiBLtQJuH&uid=nAH258ceN4,3,1
MachineLearning,t3_1d0g3vj,1716655601.0,/r/MachineLearning/comments/1d0g3vj/d_showcase_ml_model_results_using_business_metrics/,Discussion,[D] Showcase ML Model Results Using Business Metrics,,1,14
MachineLearning,t3_1d0bz82,1716643484.0,/r/MachineLearning/comments/1d0bz82/d_memory_tuning_vs_fine_tuning/,Discussion,[D] Memory tuning vs fine tuning,"I came across a LinkedIn post where this startup claims to reduce LLM hallucinations using â€œmemory tuningâ€. They claim that memory tuning is significantly superior to fine tuning. 

Was wondering if anyone has come across the term â€œmemory tuningâ€ and whether you think it is marketing talk or a real development. 

This is the post for reference: https://www.linkedin.com/posts/zhousharon_hallucinations-are-one-of-the-biggest-blockers-activity-7198340103600054273-ruNL?utm_source=share&utm_medium=member_ios",8,14
MachineLearning,t3_1d0bz2r,1716643470.0,/r/MachineLearning/comments/1d0bz2r/p_i_wrote_a_machine_learning_library_based_on/,Project,[P] I wrote a machine learning library based on TensorFlow,"Note is a system(library) for deep learning and reinforcement learning. Note makes the building and training of neural networks easy and flexible. Note.nn.layer package contains many layer modules, you can use them to build neural networks. Noteâ€™s layer modules are implemented based on TensorFlow, which means they are compatible with TensorFlowâ€™s API. The layer modules allow you to build neural networks in the style of PyTorch or Keras. You can not only use the layer modules to build neural networks trained on Note but also use them to build neural networks trained with TensorFlow.
https://github.com/NoteDance/Note",5,0
MachineLearning,t3_1d0a3ur,1716637106.0,/r/MachineLearning/comments/1d0a3ur/d_geospatial_trend_analysis_recommendations/,Discussion,[D] Geospatial Trend Analysis Recommendations,"Geospatial Trend Analysis Question

What is your guessâ€¦

I have built a large number of market prediction (domain specific) machine learning models over the past year. My current project involves utilizing geospatial data and transfer learning to analyze Google searcg trends for different stocks. The way I am able to train on limited data is through the use of a synthetic data augmentation process I designed to capture time-scale invariance. One piece of advice if you try to recreate this approach is to use batch training to avoid overfitting especially while transfer learning. (If any of that sounds confusing or dumb please just ignore it because specifics are not necessarily that important.)

Currently, I am analyzing data from major cities in the US (including D.C.) and large cities internationally such as Moscow and Beijing. I am curious which city people would assume creates the most accurate model. Which cityâ€™s search data is best for predicting future share prices? There are no wrong answers, but I am looking for more cities to add into my model while trying to be cognizant of the size because all of this is done on a ~5 year old laptop. I will note that I am using a pseudo SQL library to manage a database and be (semi) respectful when doing large data pulls.",7,3
MachineLearning,t3_1d0a30l,1716637023.0,/r/MachineLearning/comments/1d0a30l/r_vanilla_clip_for_3d/,Research,[R] Vanilla Clip for 3D,"

Hello! 

I am wondering if  there is a CLIP approach [https://openai.com/index/clip/](https://openai.com/index/clip/) for 3D data?  


I just found approaches that did ""something"" like CLIP but not exactly. Can someone point me to a paper or direction? I would especially need this for MRIs. 

Is someone aware of this?


Thank you very much. Maybe my search was not extensive enough but I also couldn't spot a good framework  from the papers that I found.
",2,1
MachineLearning,t3_1d095cn,1716633281.0,/r/MachineLearning/comments/1d095cn/r_dataset_decomposition_faster_llm_training_with/,Research,[R] Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum,"**TL;DR**: do NOT stuff more than one document in the context window while training an LM.

**Paper:** [https://arxiv.org/abs/2405.13226](https://arxiv.org/abs/2405.13226)

**Abstract:** Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length. However, this method of concatenation can lead to cross-document attention within a sequence, which is neither a desirable learning signal nor computationally efficient. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a penalty proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy 3x faster compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.

**Visual Summary:**

https://preview.redd.it/nnvi519tvj2d1.png?width=1123&format=png&auto=webp&s=334b8990f4ac2d4298e1a622d71301cd7d6beae3",10,22
MachineLearning,t3_1d092o9,1716632968.0,/r/MachineLearning/comments/1d092o9/r_litevae_lightweight_and_efficient_variational/,Research,[R] LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models,https://huggingface.co/papers/2405.14477,5,11
MachineLearning,t3_1d08hzz,1716630487.0,/r/MachineLearning/comments/1d08hzz/r_yolov10_realtime_endtoend_object_detection/,Research,[R] YOLOv10: Real-Time End-to-End Object Detection,"**Paper:** [https://arxiv.org/abs/2405.14458](https://arxiv.org/abs/2405.14458)

**Abstract:** Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8Ã—Â faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8Ã—Â smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\\% less latency and 25\\% fewer parameters for the same performance.

**Visual Summary:**

[Method](https://preview.redd.it/sf69ll17nj2d1.png?width=970&format=png&auto=webp&s=242bc2565ea53526b4c0c5933606788de9fde799)

[Benchmarking](https://preview.redd.it/sllekaz9nj2d1.png?width=1229&format=png&auto=webp&s=6169c929fd635fbf3b442e3c4f3d529b9ddea334)

**Code:** [https://github.com/THU-MIG/yolov10](https://github.com/THU-MIG/yolov10)

",15,132
MachineLearning,t3_1d08hl9,1716630434.0,/r/MachineLearning/comments/1d08hl9/d_learning_path_to_ml_compilers/,Discussion,[D] Learning path to ML compilers,"Hi all,

I recently got interested in ML compilers, and wanted to ask some questions.

### 1. ML Compilers vs language compilers

I'm curious if ML compilers and language compilers have a lot in common. I only know a bit about the frontend part of a compiler, but it looks like ML compilers don't really have the usual lexical / syntax / semantic analysis phases that language compilers go through.

Would the backend part be much more relevant for ML compilers though? Would you recommend learning topics like IR optimization, register/memory allocation or SSA first before getting into ML compilers? It feels like ML compiler is a beast of its own, so I'm not sure if I should just dive into it, or having a background in traditional compiler backend would be still very helpful.

At least the [MLIR paper](https://arxiv.org/abs/2002.11054) seems to talk about SSA and IRs, so maybe having some compiler backend background is necessary? Just to clarify, I know the definition of SSA and IR, but here I'm talking about going in depths about these topics.

### 2. Focus areas within ML compilers

I'm also curious what are the areas that require the most amount of work within ML compilers? Is it the IR (or graph) optimization? Or something else? Do you think this domain is something that will last for decades, or mostly a few years effort and then users won't have to care about the internals anymore (just like how a normal user don't really care about how gcc or clang compiles C code too much nowadays)",1,7
MachineLearning,t3_1d06kgx,1716621875.0,/r/MachineLearning/comments/1d06kgx/how_reputable_is_superal_conference_singapore_is/,Discussion,"How reputable is SuperAl conference, Singapore? Is it just a money grab, which I hear about a lot of Al conferences recently. There are some good names in it, but not sure about anything else. Also any tips for first time conference goer would be highly appreciated. [D]",I hope this isn't off topic and the mods would allow it. [superai.com](https://superai.com),13,2
MachineLearning,t3_1d050mu,1716615575.0,/r/MachineLearning/comments/1d050mu/d_why_is_the_pertoken_advantage_clipping_in_ppo/,Discussion,[D] Why is the per-token advantage clipping in PPO not symmetric?,"In PPO, we clip the per-token advantage weight which is normally

P_policy(action|state) / P_previous(action|state) 

to (1-eps, 1+eps) to prevent too-destructive updates of the policy.

My question is, since we are clipping the probability ratio of the current policy vs the previous policy, why is the clipping not symmetric? Ie, shouldn't eps be divided / multiplied, instead of subtracted / added? For example, if we wanted to prevent the current policy from deviating within 0.5 of its starting point, wouldn't we actually want to clip the probability ratio at (0.5, 2)? Which would be (1 / eps, 1 * eps) instead of (1 - eps, 1 + eps)",0,2
MachineLearning,t3_1d03v1t,1716611113.0,/r/MachineLearning/comments/1d03v1t/r_interpretability_for_visual_models/,Research,[R] Interpretability for visual models,"Hey all. With the recent Anthropic SAE paper making the rounds on Twitter, I was curious about interpretability of large visual or multimodal models ( for eg., SAM). 

More specifically, if something similar to [Transformers circuits](https://transformer-circuits.pub/) have been implemented for ViTs. 

If not, what's the reason for this gap?",4,8
MachineLearning,t3_1d03tus,1716610990.0,/r/MachineLearning/comments/1d03tus/d_gnn_research_libraries_experiences/,Discussion,"[D] GNN research libraries, experiences?","What do people use for GNN research libraries now?

I am looking to work with pytorch or jax, consumer grade GPUs (RTX 4090). I don't need fancy SOTA GNN layers, just a robust well-optimzied message passing framework that accepts heterogeneous graphs (multiple nodes, edges and levels).

I am aware of [DGL](https://www.dgl.ai/), [PyG](https://www.pyg.org/) and [Jraph](https://github.com/google-deepmind/jraph). A plus would be to serialize the models.

Any positive/negative experiences?",7,12
MachineLearning,t3_1d00amb,1716598877.0,/r/MachineLearning/comments/1d00amb/d_in_the_context_of_document_retrieval_and/,Discussion,"[D] In the context of document retrieval and calculating nDCG etc., what serves as the ground truth?","In the context of document retrieval, what serves as the ground truth?

In the context of the benchmark datasets that I'm using for document retrieval, the samples are usually comprised of a query and its corresponding positive and negative passages. A positive passage has the label 1 and a negative one has 0.

The setting that I currently have follows the following procedure:

1. Create a FAISS index of all of the passages (positive + negative).
2. Loop through each query and retrieve the top-$k$ documents from the above corpus.
3. Get the labels of each retrieved document (0 or 1).
4. Calculate nDCG.

What I'm confused about is what the ground truth should be. The retrieved arrays of binary labels above would be the predictions.

I'm using scikit-learn's nDCG score function but I'm lost as to what the input for `y_true` should be.",15,9
MachineLearning,t3_1czztjx,1716597374.0,/r/MachineLearning/comments/1czztjx/r_creating_research_paper_using_knn_classifier/,Research,"[R] Creating research paper using knn classifier with k=1
","I am preparing my reserch paper about using ai and voice recordings to detect parkinson at early stages.

I finished the code now, I create a machine model to detect parkinson. I used a dataset withÂ **756 features**.

here's the steps:

* step 1:Â **knn**Â `(n_neighbors=5, p=2)`: accuracy 84.11 f1 89.66
* step 2:Â **knn**Â `(n_neighbors=1, p=1)`: accuracy 95.39 f1 96.96
* step 3:Â **knn**Â `(n_neighbors=1, p=1)`Â **+ cross validation 5-fold**Â : mean accuracy 96.19 +/- 1.14 mean f1 97.45 +/- 0.76.
* step 4:Â **bagging + knn**Â `(n_neighbors=1, p=1)`Â accuracy 94.74
* step 5:Â **bagging**`(max_features=0.37, n_estimators=20)`Â +Â **knn**`(n_neighbors=1, p=1)`: accuracy 96.71 f1 97.84
* step 6:Â **bagging**`(max_features=0.37, n_estimators=20)`Â +Â **knn**`(n_neighbors=1, p=1)`Â +Â **cross validation 5-folds**: mean accuracy 97.22 +/- 0.78 mean f1 98.15 +/- 0.52

The scores are higher than previously published research papers that used the same dataset I used.

The problem is that I made a research online and found that knn with n\_neighbors=1 is not reliable.

I did extensive reserch of previously published reserch papers and found a research papers that usedÂ **knn with k=1**Â and the research paper is peer review. **Is it safe then to work with k=1 because it's already worked with in a previously peer reviewed and published research ?**",10,0
MachineLearning,t3_1czzt45,1716597332.0,/r/MachineLearning/comments/1czzt45/d_should_google_ai_overview_haven_been_released/,Discussion,[D] Should Google AI Overview haven been released ? ,"Yet another bad AI feature release from Google (see reactions in NYT article 5/24). When your read how bad some of the overviews are, it makes you question if Google product team was really thinking about how people will use their products. Almost seems adversarial testing was not done.

If AI Overview is really intended to summarize search results using AI, how is it supposed to work when significant percentage of websites are full of unreliable information including conspiracy theories and sarcasm.

Does anyone truly need a summary of an Onion article when searching?  
'Move fast and break things, even if the product you are breaking pulls in 40 billion/year'",63,80
MachineLearning,t3_1czxdg7,1716590285.0,/r/MachineLearning/comments/1czxdg7/publishing_coursework_project_paper_to_a_journal/,Research,Publishing Coursework Project Paper to a Journal [R] [P],"Hey,

I figured I'd post this in this sub because it's a lot more active, but it's really open to anyone. 

For context I just wrapped up my third-year undergrad at a school in the US, and I'm concentrating in ML. This past semester, I was fortunate enough to take a Deep Learning class, where I wrote a paper along four other group members for our final project, which went pretty well. We were happy with the overall results of our semester-long research, as well as the effort we put into writing a paper, and we were wondering whether it would be within the realm of possibility to get our paper published to some sort of an academic journal.

  
For some more context, our project was on **audio style transfer** (specifically an evaluation of different means and architectures used to tackle the problem of stylizing content speech audio given an input style and content audio). 

  
Obviously this is a super general question as I haven't shared the preprint out to anyone, but how likely is a paper written by four undergrads and one grad student (that is written well, makes use of good research methods, etc.) to be accepted to a relevant academic journal? 

  
If anyone has any suggestions to help us in this process, it would be greatly appreciated! Alongside potential recommendations for journals that might increase our chances of being published as best as possible. We have looked into the IEEE/ACM TASLP journal as one potential option. 

  
Thank you for reading! Feel free to be blunt ğŸ‘",8,0
MachineLearning,t3_1czrhy1,1716574832.0,/r/MachineLearning/comments/1czrhy1/d_convert_mindmaps_into_text_to_embed_into_vector/,Discussion,[D] Convert mindmaps into text to embed into Vector Database.,"Hi, I have a few detailed mindmaps for my domain and I want to know if there is a way to convert these maps and put them into a vector database for knowledge. One method I have been doing is taking screenshots of the sections and putting inside GPT4V to convert to text to embed inside the Vdb. Is there any other solution?",4,5
MachineLearning,t3_1czm3bw,1716560954.0,/r/MachineLearning/comments/1czm3bw/p_recreate_images_with_emojis/,Project,[P] Recreate Images with Emojis,"**DEMO:** [https://replicate.com/johnsutor/emoji-painter](https://replicate.com/johnsutor/emoji-painter)  


I created a DNN that can recreate a target image by successively pasting Emojis onto a ""canvas"". The code is largely inspired by the [Paint Transformer](https://github.com/wzmsltw/PaintTransformer), which sequentially pastes brush strokes onto a canvas to recreate a photo. In that paper/code base, they use a single brush type, whereas I consider emojis to be different brushes to be sampled from. A Gumbel softmax is used to select from one of N different emojis to paste onto the canvas, and a corresponding scale, rotation, and center x and y coordinate are chosen.   


I plan on training models on other shapes as well (I am open to any considerations and feedback!)",4,5
MachineLearning,t3_1czlgss,1716559311.0,/r/MachineLearning/comments/1czlgss/p_stateoftheart_open_source_computer_vision/,Project,"[P] State-of-the-art, open source, Computer Vision models that are not ultra resource intensive?","What are some leading-edge CV models (object detection, segmentation etc) that can fit on a relatively mid-tier GPU such as an A4000 or thereabouts. I'm specifically interested in inference on hardware, training is less important.

Something more interesting and performant than say a ResNet or YOLO, doesn't have to be a CNN!

Thanks in advance, just hit me with your ideas

Edit: I neglected to mention that I'm interested in FPGA inference deployment in addition, this is clearly more of a limiting factor than GPU.

Edit: My testing indicates the inference module is generally very lightweight for the majority of current CV models, I'm going to research ways to increase resource utilisation through compiler directives, scheduling and graph optimisations - Thanks!",19,46
MachineLearning,t3_1czjlyt,1716553871.0,/r/MachineLearning/comments/1czjlyt/d_stuck_in_selecting_appropriate_number_of/,Discussion,[D] Stuck in selecting appropriate number of clusters.,"I am working on a dataset that has 19 columns and 36000 rows...I am asked to perform clustering on it. So I am experimenting with KMeans. When performing the elbow method for this problem I am getting the following graph. The number of clusters were from 1 to 249. Can anyone suggest me the value of k which I should choose? And are there any other algorithms which i can try for such a dataset?

https://preview.redd.it/ftefa83x3e2d1.png?width=1355&format=png&auto=webp&s=2cee33788041b0bb7e3eb142654241e044da8188

https://preview.redd.it/4xa7ttbz3e2d1.png?width=1389&format=png&auto=webp&s=80b3bf546dc00ddd3f2ba11a00639da2cb813d85

",13,6
MachineLearning,t3_1czdmty,1716529450.0,/r/MachineLearning/comments/1czdmty/d_detecting_objects_of_same_shape_but_different/,Discussion,[D] Detecting Objects of Same Shape but Different Colors,"I'm struggling with detecting objects that have the same shape but different colors, with no other distinguishing features. When there are distinguishable patterns, CNN-based architectures like YOLO work wonders and achieve high accuracy. However, I need a method that can accurately classify objects based purely on color.

My current challenge is that these objects are not separable when I try to segment them by color in RGB space. Does anyone have suggestions or methods that achieve good accuracy in determining object classes by color?

I've included an image below for reference. Any help would be greatly appreciated!

https://preview.redd.it/83c6e7dbbb2d1.png?width=793&format=png&auto=webp&s=532c7cffcbaea96eb48d374e073bd49d5f029212

# Edit

Transforming the colorspace to HSV solved the problem. Below is the HSV colorspace representation

https://preview.redd.it/hq5yrf5wcf2d1.png?width=793&format=png&auto=webp&s=c40002fd360c1891baaed915f536aa7dae4061f5

In the first stage, I used YOLO model to detect the objects

In the second stage, I cropped the detected objects, converted the cropped images to HSV, calculated the average component value per object and then trained XGBoost model to predict color label based on 3D vector, representing the average values of H, S, and V channel.",14,54
MachineLearning,t3_1czcz42,1716526895.0,/r/MachineLearning/comments/1czcz42/p_deepfusion_a_highly_modular_deep_learning/,Project,[P] DeepFusion: a highly modular Deep Learning Framework.,"Hello all, I am a student at Stanford University, I was on a gap year due to medical conditions and to utilitze my time I was studying deep learning.

And Voila...

I've developed a deep learning library, [**DeepFusion**](https://github.com/atharvaaalok/deepfusion)!

It's customizable and has an easily accessible and highly intuitive codebase. One can just dive right in and effortlessly understand the source code.

You can download it from:

* github at [https://github.com/atharvaaalok/deepfusion](https://github.com/atharvaaalok/deepfusion)
* or install using `pip install deepfusion` (easy!)

For a series of examples explaining the usage and features refer [demo](https://github.com/atharvaaalok/deepfusion/tree/main/demo) or [tutorials](https://github.com/atharvaaalok/deepfusion-examples).

Any and all suggestions are welcome, and contributions are greatly appreciated!

https://preview.redd.it/l9ci1453ei2d1.png?width=464&format=png&auto=webp&s=96250193b39dc5042618f26ba525b4ced5b030e7

",5,6
MachineLearning,t3_1cz5zx1,1716504394.0,/r/MachineLearning/comments/1cz5zx1/p_learn_to_binarize_clip_siglip_for_multimodal/,Project,[P] Learn to Binarize CLIP (&SigLIP) for Multimodal Retrieval and Ranking,"Learning to binarize and rank with CLIP to reduce storage by 32x for text or multimodal search and recommendations.

  
Article: [https://www.marqo.ai/blog/learn-to-binarize-clip-for-multimodal-retrieval-and-ranking](https://www.marqo.ai/blog/learn-to-binarize-clip-for-multimodal-retrieval-and-ranking)

* Binary embeddings during CLIP rank-tuning preserve between 87-93% of fp32 embeddings.
* Pseudo-quantization with sigmoid with 4x scaled temperature is (almost) universally better than tanh (see next point).
* Cosine similarity on 0/1 (sigmoid) is better than -1, 1 (tanh) - pretty sure this is because cosine has better degeneracy (D vs DxN) as it penalises embeddings that are not on the same hyper-sphere (it also biases for fewer non-zero elements).
* Use L1 to approximate hamming distance during training which is marginally better than cosine (for 0/1).
* Evaluated using [GS-10M](https://github.com/marqo-ai/GCL) for multimodal retrieval using exact KNN.
* Fp32 embeddings retain full fidelity when auxiliary binary loss is addedÂ .
* Evaluated across in-domain, novel query, novel document and zero-shot settings.
* Can be combined with [Matryoshka](https://www.marqo.ai/blog/matryoshka-representation-learning-with-clip-for-multimodal-retrieval-and-ranking) if really necessary but fidelity does suffer (not shown).",0,12
MachineLearning,t3_1cz57uj,1716502238.0,/r/MachineLearning/comments/1cz57uj/d_how_to_handle_gpl_licensing_in_ml_example/,Discussion,[D] How to handle GPL Licensing in ML? Example: commercial object detection app,"Hi ML people people,  
  
Iâ€™m working on building a commercial app that involves an object detection model. Iâ€™m considering using YOLOv7 with my own training data, but Iâ€™m concerned about the licensing. YOLOv7 is licensed under GNU GPL 3.0, which would require me to make the source code of my whole app open source if YOLOv7 is integrated into it.

Iâ€™m curious about how others handle this situation. Specifically:

1. Do developers often resort to using different, older models that have more permissive licenses for commercial applications?
2. Are there any alternative approaches, such as training a model from scratch using the YOLOv7 repo (so that it is not a derivative of the repo), and build my own inference pipeline that does not use the YOLOv7 codebase for inference? For example using ONNX.

I would really appreciate any insights or experiences.

Thanks!",8,7
MachineLearning,t3_1cz3szy,1716498600.0,/r/MachineLearning/comments/1cz3szy/d_looking_for_specific_article_from_huggingface/,Discussion,[D] Looking for specific Article from HuggingFace,"Hi all, this might be a bit of a shot in the dark but here it goes:

I saw someone from HuggingFace either share/like a post about a methodology (using HuggingFace) for extreme classification problems, e.g. hundreds or even thousands of classes, in a few shot setting. I didn't save it at the time and for the life of me can't find it anymore. I tried LinkedIn search but it's terrible for finding posts. Google, HuggingFace website, Perplexity have all been of no use. Does this ring a bell to anyone? ",0,1
MachineLearning,t3_1cz3o5m,1716498267.0,/r/MachineLearning/comments/1cz3o5m/r_raster_to_graphics/,Research,[R] raster to graphics ,I want to tune hyperparameters of vtracer. Would it produce good results? If not what other techniques can I accommodate with it or what approach should I follow? ,0,0
MachineLearning,t3_1cz1yoa,1716493996.0,/r/MachineLearning/comments/1cz1yoa/r_introducing_ssamba_the_selfsupervised_audio/,Research,[R] Introducing SSAMBA: The Self-Supervised Audio Mamba!,"Hey Reddit,   

Tired of transformers? Is attention really all you need? Meet SSAMBA (Self-Supervised Audio Mamba)! ğŸâœ¨   

This attention-free, purely state-space model (SSM)-based,  self-supervised marvel doesnâ€™t just hissâ€”it roars! SSAMBA achieves  better or similar performance to its transformer-based counterparts  (SSAST) on tasks like speaker identification, keyword spotting, and  audio classification.  But here's the kicker: itâ€™s much more GPU memory  efficient and quicker at inference, especially with longer audio  lengths.   

Curious? Check out the full paper here: [SSAMBA on arXiv](https://arxiv.org/abs/2405.11831) 

Thanks for tuning in!   ",5,58
MachineLearning,t3_1cz1t4x,1716493608.0,/r/MachineLearning/comments/1cz1t4x/d_paperswithcode_relevant/,Discussion,[D] Paperswithcode relevant? ,"I feel like paperswithcode became less relevant for tracking progress in ML in general for me.

But itâ€™s hard to say, in my field (tabular ML/DL) there are not many established academic benchmarks (no need for something like papers with code yet)

In NLP and foundation model space leaderboards in hf spaces became a thing (mostly in NLP).

Overall, paperswithcode just feels less maintained and less useful.

Do you use paperswithcode often? What do you use it for? Whatâ€™s your field where it is useful?",19,44
MachineLearning,t3_1cyy7v9,1716484682.0,/r/MachineLearning/comments/1cyy7v9/dr_if_you_could_pick_3_papers_about_videoimage/,Discussion,"[D][R] If you could pick 3 papers about video/image generation models, which one would you pick?","I am doing my MSc and I've chosen to do a video generation project. I've read some papers on image and video synthesis:

* VQGAN
* Stable Diffusion
* Imagen

I also picked 3 video generation papers:

* Video-LDM
* Stable Video Diffusion fine-tuned for Multi-View generation (SVD-MV)
* Text2Video-Zero

I also read some survey papers, and those are the models I've chosen to talk about.

What i'm struggling with is to pick a logically ordered papers, so first I explain the 3 image generation papers, and the video generation papers should follow the same strategies mentioned in the image synthesis papers.

Can I ask you to suggest different set of papers to write about? I can still change all of the papers to something else.

Something that is mostly recent (2020-2024 is fine) and has some big impact. I know for example VQGAN is popular base model, the techniques and strategies used in the paper are still relevant today.

Imagen (by Google) however, is not open source, and I prefer papers with open source code. That's why I avoid OpenAI papers.

I also read that diffusion is chosen over GAN in video generation because it has better results, both in quality and training. However, diffusion is more computationally expensive.

Video-LDM for example is based on Stable Diffusion, so for me its good papers to talk about.",3,10
MachineLearning,t3_1cyve1w,1716477684.0,/r/MachineLearning/comments/1cyve1w/r_fuse_feature_vector_in_image_classification/,Research,[R] Fuse Feature Vector in image classification,"Hi everyone,  
Currently, i'm processing a image classification problem about facial emotional classification. I am using 2 extract methods: HOG and Facial Landmark. My idea is using HOG to find the gradient magnitude and oriented of the image and use facial landmark to find face keypoint. I thought i can fuse 2 method to make a better feature. But the new feature worse than HOG and better than facial landmark (same model to evaluate). I have some question:

1. I wonder how i can fuse these two method where HOG normalization before and facial landmark return 68x2 pairs point integer.
2. If can, should i normalize or something before fuse ? Which method i can try to fuse them (concat, add, multiply, ...) ?
3. Is there anyway how to measure my method will be better or evaluate it ? I am also try to fuse HOG and SIFT (Bag of visual word) too.

I had tried fuse HOG and Facial Landmark feature but it get worse than HOG and better than Facial Landmark in the same model. I also fuse (SIFT) bag of visual word and HOG but it still worse than HOG and better than bag of visual word. Here is the code i use:

    x_hogp_train = pca.transform(x_hog_train)[:,:382]
    x_hogp_valid = pca.transform(x_hog_valid)[:,:382]
    x_hogp_test = pca.transform(x_hog_test)[:,:382]
    
    scaler = StandardScaler() # scale bovw feature
    scaler.fit(x_bovw_train)
    x_scale_bovw_train = scaler.transform(x_bovw_train)
    x_scale_bovw_valid = scaler.transform(x_bovw_valid)
    x_scale_bovw_test = scaler.transform(x_bovw_test)
    
    # fuse them use concat
    x_fused_train = np.concatenate((x_hogp_train, x_scale_bovw_train), axis=1)
    x_fused_valid = np.concatenate((x_hogp_valid, x_scale_bovw_valid), axis=1)
    x_fused_test = np.concatenate((x_hogp_test, x_scale_bovw_test), axis=1)
    
    Thank in advance",0,1
MachineLearning,t3_1cyubdt,1716474978.0,/r/MachineLearning/comments/1cyubdt/r_variational_inference_reverse_kl_vs_forward_kl/,Research,[R] Variational Inference: Reverse KL vs. Forward KL,"Hi all,

I'm working on variational inference methods, mainly in the context of BNNs. Using the reverse (exclusive) KL as the variational objective is the common approach, though lately I stumbled upon some interesting works that use the forward (inclusive) KL as an objective instead, e.g \[1\]\[2\]\[3\]. Also in the context of VI for GPs both divergence measures have been used, see e.g \[4\].

While I'm familiar with the well-known difference between the objectives that the reverse KL is 'mode-seeking' and the forward KL is 'mode covering', I see some of these works making claims about downstream differences of these VI objectives such as (paraphrasing here) *""the reverse KL underestimates predictive variance"" \[4\]* and *""the forward KL is useful for applications benefiting from conservative uncertainty quantification"" \[3\]*.

I'm interested in understanding these downstream differences in the context of VI, but haven't found any works that explain these claims theoretically instead of empirically. Anyone who can point me in the right direction or have a go at explaining this?

Cheers

\[1\] Naesseth, Christian, Fredrik Lindsten, and David Blei. ""Markovian score climbing: Variational inference with KL (p|| q).""Â *Advances in Neural Information Processing Systems*Â 33 (2020): 15499-15510.

\[2\] Zhang, L., Blei, D. M., & Naesseth, C. A. (2022). Transport score climbing: Variational inference using forward KL and adaptive neural transport.Â *arXiv preprint arXiv:2202.01841*.

\[3\] McNamara, D., Loper, J., & Regier, J. (2024, April). Sequential Monte Carlo for Inclusive KL Minimization in Amortized Variational Inference. InÂ *International Conference on Artificial Intelligence and Statistics*Â (pp. 4312-4320). PMLR.

\[4\] Bauer, M., Van der Wilk, M., & Rasmussen, C. E. (2016). Understanding probabilistic sparse Gaussian process approximations.Â *Advances in neural information processing systems*,Â *29*.",18,18
MachineLearning,t3_1cytxb5,1716473973.0,/r/MachineLearning/comments/1cytxb5/d_phi3_models_compared_sidebyside/,Discussion,[D] Phi-3 models compared side-by-side.,"https://preview.redd.it/8l04pnfhq62d1.png?width=661&format=png&auto=webp&s=7fe616ca8cd7da974070c86b6b47ffab3ab545e5

---------------------------------------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------------------------------------

https://preview.redd.it/hr7fr1uiq62d1.png?width=688&format=png&auto=webp&s=bd3de359bfe4c1ed82d092be92ae38c246bdfda2

---------------------------------------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------------------------------------

https://preview.redd.it/v6k3v39kq62d1.png?width=450&format=png&auto=webp&s=c0abb0e397a498ef7ccfb35b1b1cb598198f66ad



For anyone looking to compare the Phi-3 benchmarks in one place.

Interesting comparisons for: ANLI, Hellaswag, MedQA, TriviaQA, Language understanding, Factual Knowledge and Robustness.

Note: Phi-3 mini model table have labels in different order.",18,28
MachineLearning,t3_1cyrr8c,1716467895.0,/r/MachineLearning/comments/1cyrr8c/d_whats_the_biggest_challenge_you_face_when/,Discussion,[D] What's the biggest challenge you face when deploying ML models on your own cloud (Azure/AWS/GCP)?,"Hi, This is a market research post to understand the challenges people face while deploying open-source or custom ML models in production on their own cloud (AWS/Azure/GCP).

Options:

1. Deployment complexity (K8S, Knative, Ray, etc)
2. Autoscaling wrt user demand
3. Lack of GPU availability (spot instances, quota limit)
4. Setting up CI/CD",8,11
MachineLearning,t3_1cyrhfb,1716467025.0,/r/MachineLearning/comments/1cyrhfb/d_index_of_indices/,Discussion,[D] Index of Indices,"Okay so I have been exploring the world of RAGs and I am thinking of a concept called index of indices. 

I am working on a large database of a company \[Consider a Fortune 2500\]. There are multiple functions (100+) across the company which do not really overlap too much - marketing & manufacturing for instance. 

Now for various teams within the company I want to give each department a RAG enabled chat system to play with. Someone from marketing department does some search on a topic like - ""What was CAC for Product A in the year 2023?"". Now it goes and looks for that information across vectors in an index, but in parallel it also looks for the information in 99 other indices (considering 100 departments = 100 indices). If top\_k=10, then likely, it's going to come up with 1000 results, which would be passed as context to LLM which may become very large resulting in increased latency and  pricing. 

I am thinking of creating a master index, that has top\_k = 10 (example) which then selects 10 most relevant indices based on what a user is looking for and then searches within those 10 indices, giving 100 results instead of 1000. 

I know these numbers would need to be optimized a lot, but has anyone heard about creating a vector database (on chroma or pinecone) where there's a master index and sub indices. Is it recommended for a RAG model? Is there any reading material on it? 

Thanks! ",5,0
MachineLearning,t3_1cypw7s,1716461456.0,/r/MachineLearning/comments/1cypw7s/d_better_apis_for_highload_computer_image/,Discussion,[D] Better APIs for high-load computer image inference?,"Hey everyone.

We build docker images with CV models using BentoML. But the problem is that it uses REST API, which is not the best choice for high-load CV inference.

Are there other open-source solutions for providing faster api for computer vision inference? Is there any chance we can implement them in our pipeline on BentoML?",2,3
MachineLearning,t3_1cyp308,1716458181.0,/r/MachineLearning/comments/1cyp308/r_geometry_of_data_for_ml/,Research,[R] Geometry of data for ML,"This paper Diffusion Geometry defines the geometry of data and looks like a really powerful method:  
[arxiv.org/abs/2405.10858](http://arxiv.org/abs/2405.10858)

It seems much more effective than persistent homology so maybe means people will start to use geometry and topology more in ML

https://preview.redd.it/aszt3w0qf52d1.png?width=1202&format=png&auto=webp&s=0e640e2c9fc294a32369cede18b958a58daf7f25

https://preview.redd.it/yadbcdchf52d1.png?width=748&format=png&auto=webp&s=aaffbf1e5b66def721b4ab90ec82c54c82555447",2,15
MachineLearning,t3_1cynq6x,1716452313.0,/r/MachineLearning/comments/1cynq6x/p_repromodel_open_source_ml_research_toolbox/,Project,"[P] ReproModel: Open Source ML Research Toolbox. 
","Hi, Iâ€™m a PhD in Computer Science, and Iâ€™ve just developed what I think is a great leap for Machine Learning research. I open sourced the app for everyone to check out, and all feedback and contributions are welcome.

ReproModel is a no-code toolbox that enables scientists and researchers to test and reproduce ML models efficiently. A large subset of research time is wasted trying to test models from existing papers. To replicate or test results, youâ€™d have to look into the provided code, and mimic all the config files and experiment conditions, i.e data loaders, preprocessing, optimizers etc.Â 

The toolbox takes all of that away through fetching config files from existing papers (will be available soon), loading models directly, and testing them on your data through simple checkboxes and dropdown menus. Customization is, of course, possible and encouraged.

You can find the repo here. Of course, more work has to be done, but I am approaching this step-by-step to ensure future compatibility and reusability of code.  
[https://github.com/ReproModel/repromodel](https://github.com/ReproModel/repromodel)

Appreciate your time, comments, and support with this!",7,30
MachineLearning,t3_1cymc41,1716446351.0,/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/,Project,[Project] YOLOv8 quantization project,"I quantized YOLOv8 in Jetson Orin Nano. I exported it with TensorRT (FP16, INT8) and compared the performance. Based on YOLOv8s, the mAP50-95 of the base model is 44.7 and the inference speed is 33.1 ms. The model exported with TensorRT (FP16) showed that mAP50-95 was 44.7 and the inference speed was 11.4 ms. The model exported with TensorRT (INT8) showed that mAP50-95 was 41.2 and the inference speed was 8.2 ms. There was a slight loss in mAP50-95, but the inference speed was drastically reduced. There was a problem with calibration by exporting it with TensorRT (INT8), but the loss of mAP50-95 was minimized by increasing the calibration data. I tested with all base models of YOLOv8 as well as YOLOv8s.

[https://github.com/the0807/YOLOv8-ONNX-TensorRT](https://github.com/the0807/YOLOv8-ONNX-TensorRT)",4,13
MachineLearning,t3_1cyiafd,1716431849.0,/r/MachineLearning/comments/1cyiafd/d_mamba_context_size_tradeoffs_in_multivariate/,Discussion,[D] Mamba context size tradeoffs in multivariate timeseries regression/surrogate of whitebox simulation data,"Hi all -

I am working on a problem which is a multi-variate timeseries regression problem, in particular, developing a surrogate for a (very slow) physics based simulation.

Inputs: 

34 features x 35040 timesteps (15 min interval data for one year) + 90 categorical features (ie static for entire year) with 4-56 options for each category. 

Outputs:

8 features x 35040 timesteps

My basic approach is this:

1. (b, 90 features) > embedding layer > MLP network (a few layers, intermittent skip connections etc) > (b, 96 timesteps, m features) ie create a unique latent representation of the categorical features for each interval of a day > repeat x 365 (to get to 35040 timesteps)
2. Concatenate latent timeseries representation of categorical features with the 34 timeseries feature. 
3. d_model for mamba is m + 34 (currently working with m=94 so d_model is 128)
4. Chunk the timeseries according to a desired context size and send through mamba and a final linear layer to mix down to the final number of timeseries in the output. Using a single chunk means the entire year is processed in a single pass, whereas using 73 chunks corresponds to 5 days. 

Now, the interesting question is what to set as the context/chunking size for Mamba. From a physics perspective, the maximum causal horizon (correct term?) is probably about a day - ie the state of the inputs and system over the last 24 hours are relevant to compute the next timestep in a white box solver, but beyond that the past doesnâ€™t affect the system. However, because the timeseries inputs/timesteps are not IID/uniformly random in the dataset (they theoretically could be mathematically but realistic data is used), itâ€™s of course possible (and at least somewhat likely) that there is mutual information shared between timesteps beyond the causal horizon. 

Iâ€™m getting pretty decent results with a context size of 480 timesteps (5 days) and backpropping after each chunk (and during inference, processing a whole year in chunks of the same size) but I donâ€™t have any good intuition about tradeoffs of using a larger or smaller context size here besides how it affects the VRAM requirements during training. 

Iâ€™d love to start a discussion here about it!

Or anything else in this architecture. If you have any good papers that implement a similar approach to combining categorical and timeseries data I would love to see it!",1,4
MachineLearning,t3_1cyhk1f,1716429530.0,/r/MachineLearning/comments/1cyhk1f/d_what_are_geoff_hintons_current_thoughts_on/,Discussion,[D] What are Geoff Hinton's current thoughts on backpropogation as a learning mechanism in the brain?,"I watched a lecture of his from about 4 years ago where he swats down all of the objections to backpropogation as a learning mechanism in the brain. But I recall hearing him on a podcast more recently (can't find it anymore) in which he was skeptical of backprop, and seemed to suggest that Hebbian learning was more important. I'm curious to know his current beliefs and why. What it the most recent interview or lecture where he discusses this?

/u/geoffhinton

edit: here is the lecture I am referring to, called [""Does the brain do backpropogation?""](https://youtu.be/qIEfJ6OBGj8)",87,100
MachineLearning,t3_1cydq8g,1716418114.0,/r/MachineLearning/comments/1cydq8g/failing_to_replicate_deep_residual_learning_p/,Project,"Failing to replicate 'Deep Residual Learning' ""[P]""","Hi Everyone,

for learning purposes I've been replicating the methods from Kaiming He's 2015 ['Deep Residual Learning for Image Recognition'](https://arxiv.org/pdf/1512.03385). I've built the VGG inspired plain-CNN as well as the ResNet architectures (standard & bottleneck).

However, I have been unable to replicate the degradation (saturation of accuracy) problem highlighted in the publication.  The %-error figures in the publication show clear drops in %-error as training progresses followed by stagnation.

My figures appear to stagnate, but its clear the model is generalizing horribly to the validation data. I've included one of their figures as reference. Any recommendations to better replicate the error rate saturation from this paper? Note: For Kaiming He figure, bold lines are testing error & dashed are training.

Parameters:

* 162 Epochs w/ batch size of 128 for 64k iterations.
* Lr: 0.1
* Momentum: 0.9
* Weight decay: 0.0001
* Multi-step scheduler dividing the lr by 10 at 32k and 48k iterations

[My &#37;-error](https://preview.redd.it/du9j19ay322d1.png?width=729&format=png&auto=webp&s=15d461e662e5ca823bef852b73626c5cb3f043c8)

[From Paper](https://preview.redd.it/zsvtfxb0422d1.png?width=701&format=png&auto=webp&s=019edcbf04a5eff3ffb0902fbb3057fd501e9dd4)

**Edit:** Adjustment to training transformation by implementing normalization prior to random cropping & re-ran all models. First image is change to plain 18 error rate curve, second is all error rates for tested architectures.

https://preview.redd.it/g7gpcuql482d1.png?width=1000&format=png&auto=webp&s=0db10a04a16585a0401c56a7f4c2ae25f9f8249b

https://preview.redd.it/7d35ztql482d1.png?width=1000&format=png&auto=webp&s=a83604401d1971bc7472d0b08231985a222ba8f4

**Edit 2**: First, thank you everyone so much for the suggestions you have already made!

The following recommendations were implemented: padding of 4 actually implemented, Resize() removed as images are already 32x32, horizontal flip implemented before RandomCrop. I've included the resulting Plain18, Plain34, and cumulative Error Graphs. I'm not quite sure what is happening w/ Plain34...

Here is the codebase: [https://github.com/AnotherBotIGuess/CIFAR\_ResNet](https://github.com/AnotherBotIGuess/CIFAR_ResNet)  
I apologize for the poor code cleanliness...

https://preview.redd.it/3j60d23w1e3d1.png?width=1000&format=png&auto=webp&s=335b68987921b9f58f960d6160ce990318fdecbc

https://preview.redd.it/fzbid9e02e3d1.png?width=1000&format=png&auto=webp&s=09e78d54257270c0b19e3ba3f6a2cd048cf758de

https://preview.redd.it/9y93rzu02e3d1.png?width=1000&format=png&auto=webp&s=0726489d0ad7ca68730f492ddcf3496533898783",29,16
MachineLearning,t3_1cyc0zs,1716413726.0,/r/MachineLearning/comments/1cyc0zs/research_how_can_understanding_sparse/,Research,[Research] How Can Understanding Sparse Autoencoders in Claude 3 Sonnet Influence Practical AI Applications?,"I recently read the  paper ""Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"" by Anthropic. The study explores how sparse autoencoders can extract interpretable, multilingual, and multimodal features from transformer models. 

[https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)  - paper link

Given that these features influence both the detection and generation of specific types of data (like text or images), Iâ€™m curious about the practical applications of this capability:

How can this  level of feature understanding help in customizing model outputs for specific tasks without extensive retraining? For example, could we steer a model more effectively during deployment based on identified features? Can this eliminate/identify/mitigate bias? ",9,21
MachineLearning,t3_1cyaepr,1716409620.0,/r/MachineLearning/comments/1cyaepr/thoughts_on_the_instructlab_project_community/,Discussion,Thoughts on the InstructLab project? Community driven LLM? [D],"[https://github.com/instructlab/community/blob/main/README.md](https://github.com/instructlab/community/blob/main/README.md)

""The mission of the InstructLab (**L**arge-scaleÂ **A**lignment for chat**B**ots) project is to leverage innovative techniques that overcome challenges in Large Language Model (LLM) training. InstructLab uses a taxonomy based curation process, along with synthetic data generation, that allows the open source community to submit contributions to existing LLMs in an accessible way.

InstructLab is made up of several projects that are defined as codebases and services with different release cycles. Collectively, these enable large-model development. This repository shares InstructLab's activity and collaboration details across the community and include the most current information about the project.""

Thoughts on this? I haven't seen anything like this, a community model for contributing knowledge and skills to a model to improve its performance.  Seems very new, but somewhat unique in the LLM landscape.",0,5
MachineLearning,t3_1cy7no2,1716402907.0,/r/MachineLearning/comments/1cy7no2/d_mistral7bv03_instruct_vs_llama3_8b_instruct/,Discussion,"[D] Mistral-7B-v0.3 instruct vs Llama-3 8B Instruct eval in the Medical domain 
","https://preview.redd.it/wzkhd6k1v02d1.png?width=571&format=png&auto=webp&s=0733a90b13450d48721f6eced9e70dfe2028bf93

",3,8
MachineLearning,t3_1cy6ljc,1716400390.0,/r/MachineLearning/comments/1cy6ljc/dhow_is_machine_learningdeep_learning_being_used/,Discussion,[D]How is Machine Learning/Deep Learning being used in Financial Trading?,"I love this episode, discussing the role of deep learning and machine learning for finance. Truly ML/DL in finance is now considered a key aspect of several financial services and applications, including managing assets, evaluating levels of risk, calculatingÂ credit scores, and even approving loans.

[https://podcasters.spotify.com/pod/show/ai-x-podcast/episodes/Deep-Learning-for-Financial-Trading-with-Sofien-Kaabar-e2i4q0c](https://podcasters.spotify.com/pod/show/ai-x-podcast/episodes/Deep-Learning-for-Financial-Trading-with-Sofien-Kaabar-e2i4q0c)",4,12
MachineLearning,t3_1cy6771,1716399407.0,/r/MachineLearning/comments/1cy6771/d_data_augmentation_with_ambiguous_examples/,Discussion,[D] Data augmentation with ambiguous examples?,"I feel like I'm missing a keyword here, so maybe you guys can help me figure out what to search for.

I was recently thinking about what occurs when you give a typical classifier a completely random sample that is effectively out of distribution. It will still return a label. For instance, if I train a model to classify MNIST, and I feed it vectors of Gaussian noise, the model will classify these completely random vectors as digits.

Has anyone attempted to augment their training datasets with random noise (or random combinations of training examples)? In this strategy, my intuition is that you'd be injecting essentially negative counter examples - and you'd want the label vectors to have a high amount of entropy. I think these counter examples would have a high degree of aleatoric uncertainty. ",6,1
MachineLearning,t3_1cy5ldu,1716397923.0,/r/MachineLearning/comments/1cy5ldu/d_what_would_you_like_to_see_more_or_less_of_in/,Discussion,[D] What would you like to see more or less of in /r/machinelearning?,"I can't set a poll. But thought Id check in with this community.

Ill start, I wish original researchers would post their papers and answer questions.",32,62
MachineLearning,t3_1cy515k,1716396589.0,/r/MachineLearning/comments/1cy515k/r_paper_comparing_cnn_locally_connected_and/,Research,"[R] Paper comparing cnn, locally connected, and densely connected models","A while ago I read a paper that does a comparison between a convolutional model, a locally connected one (same as cnn but without parameter sharing, so each image location has a different kernel), and a densely connected model. Specifically, they make each of these models have the same number of activations, where most comparisons keep parameters constant. This was a cool paper because it places each model on an even playing field in terms of what it can express and this shows how the benefits of a cnn can be decomposed into locality and parameter sharing. 

I'm struggling to find this paper however. Does anyone have any recommendations?",1,10
MachineLearning,t3_1cy1kn9,1716388040.0,/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/,Discussion,"[D] AI Agents: too early, too expensive, too unreliable","[**Reference: Full blog post**](https://www.kadoa.com/blog/ai-agents-hype-vs-reality)

There has been a lot of hype about the promise of autonomous agent-based LLM workflows. By now, all major LLMs are capable of interacting with external tools and functions, letting the LLM perform sequences of tasks automatically.

But reality is proving more challenging than anticipated.

The [WebArena leaderboard](https://docs.google.com/spreadsheets/d/1M801lEpBbKSNwP-vDBkC_pF7LdyGU1f_ufZb_NWNBZQ/edit#gid=0), which benchmarks LLMs agents against real-world tasks, shows that even the best-performing models have a success rate of only 35.8%.

# Challenges in Practice

After seeing many attempts to AI agents, I believe it's too early, too expensive, too slow, too unreliable.  
It feels like many AI agent startups are waiting for a model breakthrough that will start the race to productize agents.

* Reliability: As we all know, LLMs are prone to hallucinations and inconsistencies. Chaining multiple AI steps compounds these issues, especially for tasks requiring exact outputs.
* Performance and costs: GPT-4o, Gemini-1.5, and Claude Opus are working quite well with tool usage/function calling, but they are still slow and expensive, particularly if you need to do loops and automatic retries.
* Legal concerns: Companies may be held liable for the mistakes of their agents. A [recent example](https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit) is Air Canada being ordered to pay a customer who was misled by the airline's chatbot.
* User trust: The ""black box"" nature of AI agents and stories like the above makes it hard for users to understand and trust their outputs. Gaining user trust for sensitive tasks involving payments or personal information will be hard (paying bills, shopping, etc.).

# Real-World Attempts

Several startups are tackling the AI agent space, but most are still experimental or invite-only:

* [adept.ai](https://www.adept.ai/) - $350M funding, but access is still very limited
* [MultiOn](https://www.multion.ai) - funding unknown, their API-first approach seems promising
* [HypeWrite](https://www.hyperwriteai.com/personal-assistant) - $2.8M funding, started with an AI writing assistant and expanded into the agent space
* [minion.ai](https://minion.ai) - created some initial buzz but has gone quiet now, waitlist only

Only MultiOn seems to be pursuing the ""give it instructions and watch it go"" approach, which is more in line with the promise of AI agents.  
All others are going down the record-and-replay RPA route, which may be necessary for reliability at this stage.

Large players are also bringing AI capabilities to desktops and browsers, and it looks like we'll get native AI integrations on a system level:

* OpenAI announced their Mac desktop app that can interact  with the OS screen.
* At Google I/O, Google demonstrated Gemini [automatically processing a shopping return](https://www.youtube.com/watch?v=zRY_T-hBp74).
* Microsoft [announced Copilot Studio](https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio), which will let developers build AI agent bots.

Screenshot Screenshot

These tech demos are impressive, but we'll see how well these agent capabilities will work when released publicly and tested against real-world scenarios instead of hand-picked demo cases.

# The Path Forward

AI agents overhyped and it's too early.  
However, the underlying models continue to advance quickly, and we can expect to see more successful real-world applications.  
Instead of trying to have one large general purpose agent that is hard to control and test, we can use many smaller agents that basically just pick the right strategy for a specific sub-task in our workflows. These ""agents"" can be thought of as medium-sized LLM prompts with a) context and b) a set of functions available to call.

The most promising path forward likely looks like this:

1. Narrowly scoped, well testable automations that use AI as an augmentation tool rather than pursuing full autonomy
2. Human-in-the-loop approaches that keep humans involved for oversight and handling edge cases
3. Setting realistic expectations about current capabilities and limitations

By combining tightly constrained agents, good evaluation data, human-in-the-loop oversight, and traditional engineering methods, we can achieve reliably good results for automating medium-complex tasks.

Will AI agents automate tedious repetitive work, such as web scraping, form filling, and data entry? Yes, absolutely.

Will AI agents autonomously book your vacation without your intervention? Unlikely, at least in the near future.",65,295
MachineLearning,t3_1cxwqb7,1716372712.0,/r/MachineLearning/comments/1cxwqb7/p_fish_speech_tts_clone_openai_tts_in_30_minutes/,Project,[P] Fish Speech TTS: clone OpenAI TTS in 30 minutes,"While we are still figuring out ways to improve the agent's emotional response to OpenAI GPT-4o, we have already made significant progress in aligning OpenAI's TTS performance. To begin this experiment, we collected 10 hours of OpenAI TTS data to perform supervised fine-tuning (SFT) on both the LLM (medium) and VITS models, which took approximately 30 minutes. After that, we used 15 seconds of audio as a prompt during inference.

Demos Available: [here](https://firefly-ai.notion.site/OpenAI-Examples-34975ae263a9496c84e89fb7b1ea25a4?pvs=4).

As you can see, the model's emotion, rhythm, accent, and timbre match the OpenAI speakers, though there is some degradation in audio quality, which we are working on. To avoid any legal issues, we are unable to release the fine-tuned model, but I believe everyone can tune [fish-speech](https://github.com/fishaudio/fish-speech) to this level within hours and for around $20.

Our experiment shows that with only 25 seconds of prompts (few-shot learning), without any fine-tuning, the model can mimic most behaviors except details like timbre and how it reads numbers. To the best of our knowledge, you can clone how someone speaks in English, Chinese, and Japanese with 30 minutes of data using this framework.

  
Repo: [https://github.com/fishaudio/fish-speech](https://github.com/fishaudio/fish-speech)",4,27
MachineLearning,t3_1cxtf2o,1716358383.0,/r/MachineLearning/comments/1cxtf2o/d_learning_and_contributing_in_ai_agents/,Discussion,[D] Learning and Contributing in AI Agents,"I find the AI agents powered by LLMs to be quite an interesting topic to look for! I was wondering if anyone can tell me what is the theoretical as well as practical background needed to understand and contribute it? Considering I am well versed with foundational NLP, Deep Learning techniques like RNNs, LSTMs and Transformers (Only SLMs) and basic RAG pipeline .

Also, I also wanted to work/contribute in Open Source Projects related to ML as well so I would like to know what are some projects that you are aware of ? Please don't hesitate to share up your projects as well :).",3,0
MachineLearning,t3_1cxrivi,1716351218.0,/r/MachineLearning/comments/1cxrivi/discussion_metric_to_evaluate_imbalance_data/,Discussion,"[Discussion] Metric to evaluate imbalance data.
",I am processing a problem about classification images. I can augment and sampling my imbalance training set but my valid/test set got imbalance. Which metric I should use to evaluate my model (i know that F1-score exist but this is also a multiclass problem)?,13,4
MachineLearning,t3_1cxos4s,1716342308.0,/r/MachineLearning/comments/1cxos4s/d_what_is_your_favorite_way_to_expand_your/,Discussion,[D] What is your favorite way to expand your knowledge in the field post degree?,"I finished my M.S. in Statistics over a year ago, and my knowledge has gotten a bit rusty and I want to get back into pushing myself and growing my understanding of all these great concepts at our fingertips. I tend to love just picking up a textbook and working through it, or finding a cool data set and building out some models to solidify what I've been learning.

I know that beyond getting a PhD or working in a lab, it's hard to actually make meaningful strides in terms of research; however, I honestly just deeply love math/statistics/CS, so studying this stuff is probably my favorite way to relax at the end of the day. 

  
What's your favorite way to keep yourself growing?",17,27
MachineLearning,t3_1cxejcc,1716315093.0,/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/,Discussion,[D] How to combine PhD in Machine learning with Cloud and Distributed Systems?,"'m in college and I was wondering if there was any PhD focus for ML and building large efficient systems that run those models? I really like my ML research work(specifically deep learning and CV) and academic work, but I also like my cloud and distributed systems as a hobby. If I didn't want to do PhD, I would probably do something like MLops.

Points: 

-Im not sure if I'm being too vague with my topics or if I'm looking up the wrong search terms, but I can't find any info on it. 

-Would I look for labs and professors that focus on HPC? I think of HPC as more general and focused on super computer work. I might like to explore edge devices ML or other scales. 

-Is there no point in combining ML with these, and that ML on large systems can be treated as just another branch of a CS PhD focused on distributed systems?",14,3
MachineLearning,t3_1cxd3jo,1716311517.0,/r/MachineLearning/comments/1cxd3jo/r_grad_cam_on_a_data_augmentation_model/,Research,[R] GRAD CAM on a Data Augmentation model,"UPDATE (I ADDE THE LINK AND SOME SS)

hello everyone, i implemented a data augmentation model and im trying to watchh the Grad CAM of the neural network but theres a problem with the Data augmentation section and i cant solve that issue

i search some implementation on google but is still not working and a didn\`t found an implementation on a model with data augmentation, i asked to chatgpt but that code is not working

do someone knows how to do it or any advice? 

i\`ll post the link and the SS

[https://www.kaggle.com/code/luismanuelgnzalez/cnn-landuse](https://www.kaggle.com/code/luismanuelgnzalez/cnn-landuse)



data augmetation section

https://preview.redd.it/rlvdeup5bt1d1.png?width=539&format=png&auto=webp&s=d695197b20fa7552d22e722fe54d0e668592b0cb

MODEL

https://preview.redd.it/h81y76q8bt1d1.png?width=582&format=png&auto=webp&s=6fd39c251d4a2267cb552e44bfc61d57867641e1

https://preview.redd.it/02xhz5habt1d1.png?width=668&format=png&auto=webp&s=29c153ee8c3c7fd3e432e47d49997933ad7662b7



code for CAM

https://preview.redd.it/pscqz41fcu1d1.png?width=692&format=png&auto=webp&s=8dc790fcaba244940175c9fda7fb21da9cdd4bf0



https://preview.redd.it/o028579icu1d1.png?width=636&format=png&auto=webp&s=75fb6a81bdd90850a3b3217e22a1505b62e4f719



https://preview.redd.it/0a09vi1ncu1d1.png?width=889&format=png&auto=webp&s=ea91c089e9bb671dc849c3f6439a25a20724d410





and i got the following error

https://preview.redd.it/4k2ftg7rcu1d1.png?width=984&format=png&auto=webp&s=b3c21e09b314e783fbdea1703a0347d9e2575785

  
",5,0
MachineLearning,t3_1cxbsbl,1716308221.0,/r/MachineLearning/comments/1cxbsbl/supermarket_image_dataset_for_planogram/,Project,Supermarket image dataset for planogram optimization[P],"Hello,

I have been working on object detection models for planogram optimization problems in the retail industry. So far, I have been using the SKU110K dataset. The issue with this dataset is that the products are not individually labeled. All the objects to detect are labeled as â€œobjectâ€.

Do you know about a dataset similar to SKU110K that has the specific labels to each product in an image?

Thank you",2,0
MachineLearning,t3_1cx9npr,1716302831.0,/r/MachineLearning/comments/1cx9npr/p_a_post_on_probabilistic_calibration_in_blog/,Project,[P] A post on probabilistic calibration in blog series on polynomial regression ,"Another chapter in my personal learning about polynomial regression with the Bernsten basis passes through the lands of probabilistic model calibration. I certainly enjoyed learning, and I hope you'll find it interesting as well.

Series begins here: 
https://alexshtf.github.io/2024/01/21/Bernstein.html

Latest post on calibration here:
https://alexshtf.github.io/2024/05/19/BernsteinCalibration.html
",3,10
MachineLearning,t3_1cx83cs,1716298594.0,/r/MachineLearning/comments/1cx83cs/r_enabling_sparse_foundational_llms_for_faster/,Research,"[R] Enabling sparse, foundational LLMs for faster and more efficient models from Neural Magic and Cerebras","In a collaboration across Neural Magic, Cerebras, and IST Austria, we've pushed out, to the best of our knowledge, the first highly sparse, foundational LLMs with full recovery on several fine-tuning tasks, including chat, code generation, summarization, and more.

[Sparsity vs Baseline Accuracy Recovery for Popular Fine-tuning Tasks for Llama 2 7B](https://preview.redd.it/mexhej5i8s1d1.png?width=2936&format=png&auto=webp&s=3a126fe3ca7a36f25cb71cfb02924d2b26aa72f7)

Utilizing the models, we further demonstrate:

* Inference performance speedups from sparsity alone at 3x for CPUs and 1.7x for GPUs on Neural Magic's platform.
* Compounded gains with quantization for up to 8.6X faster inference performance.
* Close to theoretical gains for sparse training utilizing Cerebras's CS-3 AI accelerator.

[Prefill and Decode Llama 2 7B Performance at Various Sparsity Levels for FP32 and INT8 on an 8-core CPU.](https://preview.redd.it/jtxsxxio8s1d1.png?width=3636&format=png&auto=webp&s=ce9d228d9ff5891e043b83c27d955fb0645f3ecd)

  
Paper: [https://arxiv.org/abs/2405.03594](https://arxiv.org/abs/2405.03594)  
Models: [https://huggingface.co/collections/neuralmagic/sparse-foundational-llama-2-models-65f48cec6396309f02e74d21](https://huggingface.co/collections/neuralmagic/sparse-foundational-llama-2-models-65f48cec6396309f02e74d21)",2,37
MachineLearning,t3_1cx6pif,1716294583.0,/r/MachineLearning/comments/1cx6pif/r_llms_as_active_learning_agents/,Research,[R] LLMs as active learning agents,"**TL;DR:** LLMs can be used as active learning components because they are good at finding difficult or diverse examples - even outperforming few-shot learning methods.

&#x200B;

While LLMs such as GPT-4 are commonly used in the training process of smaller BERT-like models (pseudo-labelling or data augmentation), we wondered if they could also be used as active learning agents. In contrast to conventional active learning, we see the opportunity that LLMs are really good at capturing the diversity and difficulty of examples and do not face a cold start problem. Conventional active learning often requires many seed instances, which makes it somewhat unattractive for many tasks where BERT models already achieve good performance in few-shot scenarios.

&#x200B;

We experiment with different LLMs as active learning components and indeed show that they can significantly improve performance in few-shot scenarios:

[Few-shot results \(32 instances\) with different LLMs on GLUE tasks.](https://preview.redd.it/eyhmz4u0wr1d1.jpg?width=2306&format=pjpg&auto=webp&s=1d7a35b6e3c14d1c094283e90a9155fec3337bf1)

We also show that active learning with GPT-4 can outperform the few-shot learning method SetFit:

[Comparison on AGNews \(32 instances\)](https://preview.redd.it/ji0a0nj8wr1d1.jpg?width=1651&format=pjpg&auto=webp&s=19a75253d7e0c966563af21a0de86b63b1d84387)

**Paper:** [http://arxiv.org/abs/2405.10808](http://arxiv.org/abs/2405.10808)",0,9
MachineLearning,t3_1cx63gy,1716292675.0,/r/MachineLearning/comments/1cx63gy/research_what_are_some_great_articles_on/,Research,[Research] What are some great articles on algorithmic fairness?,I am currently preparing a presentation on algorithmic fairness and I would like to hear your article suggestions. I would love it if the articles are recent but earlier milestone articles are also appreciated. ,3,7
MachineLearning,t3_1cx3pg9,1716283507.0,/r/MachineLearning/comments/1cx3pg9/d_should_data_in_different_modalities_be/,Discussion,[D] Should data in different modalities be represented in the same space?,"As I've studied language AI primarily, I'm getting used to multimodal AI. However it seems training methodologies are so diverse, not to mention evaluating those are much more difficult imo. At least, I've thought data in different modalities should be represented different spaces. Is there any 'better method(maybe)' researchers agree?",8,21
MachineLearning,t3_1cx2u5v,1716279609.0,/r/MachineLearning/comments/1cx2u5v/d_question_about_a_derivation_in_understanding/,Discussion,"[D] Question about a derivation in ""Understanding Black-box Predictions via Influence Functions"" by Pang Wei Koh et al.","I was reading through [https://arxiv.org/abs/1703.04730](https://arxiv.org/abs/1703.04730) and was looking into the derivation of equation (1) given in the Appendix A. I understand how we get equations (6)-(10), but the jump from equation (10) to (11) is confusing me.

I also don't really get how you'd 2nd order Taylor expand the RHS of equation (10), as it's a vector-to-vector function. Do they mean 2nd order Taylor expanding the perturbed loss function R(theta) + epsilon L(theta)? If this is the case, shouldn't the hessian should be part of a quadratic form? In equation (11) they're only right multiplying by delta\_epsilon (not left multiplying), so it's not in this form. ",4,4
MachineLearning,t3_1cx2tt0,1716279566.0,/r/MachineLearning/comments/1cx2tt0/d_thoughts_on_cloninghacking_a_production_fsd_to/,Discussion,[D] Thoughts on cloning/hacking a production FSD to teach new FSD,"If a company deploys a significantly better FSD. And you feed videos from different cars into the FSD, and use those actions to supervise a new FSD.. 

Setting aside legalities and scale, this seems feasible and inevitable? Either a state actor or even a western manufacturer looking to get the last 9s ?

Thoughts ??",5,6
MachineLearning,t3_1cwvoaq,1716253521.0,/r/MachineLearning/comments/1cwvoaq/r_have_you_give_a_try_to_use_intel_and_amd_gpus/,Research,[R] Have you give a try to use Intel and AMD GPUs to train models?,"NVIDIA rules the market of GPUs for datacenter. The most adopted frameworks for ML support NVIDIA GPUs. But I'm wondering about the drawbacks and advantages of using Intel and AMD GPUs to train ML models. Have you experienced use these GPUs? What could you say about performance, usability, software stack and ecosystem?",12,52
MachineLearning,t3_1cwudpo,1716249706.0,/r/MachineLearning/comments/1cwudpo/d_wav2lip_for_anime_characters/,Discussion,[D] Wav2Lip for anime characters?,"Hello guys. I saw Wav2Lip, open-source software for creating â€œdeepfakeâ€ talking heads based on a single image and audio. Is there something like this, but explicitly for anime 2D characters? Open-source only!",0,0
MachineLearning,t3_1cwsbyw,1716243995.0,/r/MachineLearning/comments/1cwsbyw/d_has_ml_actually_moved_the_needle_on_human_health/,Discussion,[D] Has ML actually moved the needle on human health?,"We've been hearing about ML for drug discovery, precision medicine, personalized treatment, etc. for quite some time. What are some ways ML has actually moved the needle on human health?

It seems like most treatments and diagnostics are still based on decades of focused biology research rather than some kind of unbiased ML approach. Radiology is one notable exception that benefited from advances in machine vision, but even they seem slow to accept AI as clinical practice.",109,176
MachineLearning,t3_1cwraxq,1716241346.0,/r/MachineLearning/comments/1cwraxq/d_hypothetically_in_the_future_how_would_you_make/,Discussion,"[D] Hypothetically, in the future: How would you make an accurate and detailed digital twin of Earth using AI?","The reason I think it would be possible soon is that we already have so much data on the internet. As a thought experiment:

If we had billions of human 3d-artists and gave them access to all the data (text, photos, maps, video etc), I think they could rebuild the entire planet very accurately in 3d (because their brains have lots of experience from the real world and know how to relate 2d images to a 3d space etc). They could even use their massive real world experience to fill in very plausible details where data is missing. This would probably take many many years, but I think it means that (in theory) a properly trained AI-model could do it much faster (and more accurately).

**How would you tackle this challenge? How would such an AI-model be trained?**

https://preview.redd.it/9hh6hq6cin1d1.jpg?width=700&format=pjpg&auto=webp&s=4b935bc18ea4a0078ded2843ceebcb976b37204a

  


PS! I know Google street view and maps exist, but it would be cool if we could get much more details and accuracy up close. To the point of being able walk around on ground level and looking almost exactly like the real world. I imagine it could be used for lots of applications (including simulators and GTA7/8 haha).",13,0
MachineLearning,t3_1cwoh51,1716234294.0,/r/MachineLearning/comments/1cwoh51/d_can_multimodal_models_tell_images_apart_from/,Discussion,"[D] - Can multimodal models tell images apart from text? Like if a text token and an image token are close vectors, will the model be able to ""tell"" if it is reading or seeing? ","I ran into this doing some work with multimodal models. It seemed like they couldn't tell which part of the information was from the text vs the image portions of an input. 

Is there any research on this? ",14,30
MachineLearning,t3_1cwo0a7,1716233112.0,/r/MachineLearning/comments/1cwo0a7/discussion_computer_vision_lie_detection/,Discussion,[Discussion] Computer Vision Lie Detection?,"I can find lots of examples of lie detection with NLP, but I'm wondering if anyone has come across computer vision data for lie detection, or a data set that could be used for that purpose. In a perfect world, the data would probably be in video format, but I suppose it's possible it could be done with facial recognition data too.

I recall a news article I found a few years ago (can't find it now) where an ML model had been built to detect lies based on facial expressions. I did find a much more recent [video ](https://www.youtube.com/watch?v=AtmQJ6fySTY)(skip to 2:04 for the relevant bit) where Israel had developed a technique using facial muscle sensors, and this may be the original innovation I had read about, since I believe the model in the older article was also in use by the Israeli military.",20,0
MachineLearning,t3_1cwij7u,1716219460.0,/r/MachineLearning/comments/1cwij7u/d_what_is_the_advantage_and_disadvantage_of_using/,Discussion,"[D] what is the advantage and disadvantage of using custom neural network for a secondary meta learner of a stacked ensemble, than using a traditional classifier?","Is this even possible? Every journals I read they only mention the usage of classifiers like xgboost, logistic regression etc. however, when I ask on chatgpt it recommends the neural network ensemble as the most effective secondary meta learner for stacking. ",8,0
MachineLearning,t3_1cwiemd,1716219111.0,/r/MachineLearning/comments/1cwiemd/d_transliteration_translation_of_comments_on/,Discussion,[D] Transliteration + translation of comments on Instagram app,Is it just me or does anyone notice the stark improvement in the quality of translation - especially translation from languages that is written using english characters (transliteration + translation). Wonder what kind of models they are using that led to the sudden improvement,1,3
MachineLearning,t3_1cwel28,1716208784.0,/r/MachineLearning/comments/1cwel28/predictive_binary_ml_variable_selection_d/,Discussion,Predictive binary ml - variable selection [D],"Hi,

I have been working on a binary predictive model for months. 
I am trying to optimise the variable selection. 
I have around 12. 

Using lasso, best subset, trees etc is not what Iâ€™m looking for. 
I am trying to loop through all the combinations but it is not only difficult to code, but I also doubt my laptop will be able to handle it if I code it. 

Any tips ? ",3,0
MachineLearning,t3_1cwdicc,1716205359.0,/r/MachineLearning/comments/1cwdicc/p_sdg_adds_support_for_gptbased_synthetic_data/,Project,[P] SDG- adds support for GPT-based synthetic data generation for single table,[https://github.com/hitsz-ids/synthetic-data-generator](https://github.com/hitsz-ids/synthetic-data-generator),0,0
MachineLearning,t3_1cwazr2,1716195648.0,/r/MachineLearning/comments/1cwazr2/d_has_anyone_worked_with_rembg_python/,Discussion,[D] Has anyone worked with rembg (python)? ,"    def remove(
        data: Union[bytes, PILImage, np.ndarray],
        alpha_matting: bool = False,
        alpha_matting_foreground_threshold: int = 240,
        alpha_matting_background_threshold: int = 10,
        alpha_matting_erode_size: int = 10,
        session: Optional[BaseSession] = None,
        only_mask: bool = False,
        post_process_mask: bool = False,
        bgcolor: Optional[Tuple[int, int, int, int]] = None,
        *args: Optional[Any],
        **kwargs: Optional[Any]
    ) -> Union[bytes, PILImage, np.ndarray]:
        """"""
        Remove the background from an input image.
        This function takes in various parameters and returns a modified version of the input image with the background removed. The function can handle input data in the form of bytes, a PIL image, or a numpy array. The function first checks the type of the input data and converts it to a PIL image if necessary. It then fixes the orientation of the image and proceeds to perform background removal using the 'u2net' model. The result is a list of binary masks representing the foreground objects in the image. These masks are post-processed and combined to create a final cutout image. If a background color is provided, it is applied to the cutout image. The function returns the resulting cutout image in the format specified by the input 'return_type' parameter.
        Parameters:
            data (Union[bytes, PILImage, np.ndarray]): The input image data.
            alpha_matting (bool, optional): Flag indicating whether to use alpha matting. Defaults to False.
            alpha_matting_foreground_threshold (int, optional): Foreground threshold for alpha matting. Defaults to 240.
            alpha_matting_background_threshold (int, optional): Background threshold for alpha matting. Defaults to 10.
            alpha_matting_erode_size (int, optional): Erosion size for alpha matting. Defaults to 10.
            session (Optional[BaseSession], optional): A session object for the 'u2net' model. Defaults to None.
            only_mask (bool, optional): Flag indicating whether to return only the binary masks. Defaults to False.
            post_process_mask (bool, optional): Flag indicating whether to post-process the masks. Defaults to False.
            bgcolor (Optional[Tuple[int, int, int, int]], optional): Background color for the cutout image. Defaults to None.
            *args (Optional[Any]): Additional positional arguments.
            **kwargs (Optional[Any]): Additional keyword arguments.
        Returns:
            Union[bytes, PILImage, np.ndarray]: The cutout image with the background removed.
        """"""
    def remove(
        data: Union[bytes, PILImage, np.ndarray],
        alpha_matting: bool = False,
        alpha_matting_foreground_threshold: int = 240,
        alpha_matting_background_threshold: int = 10,
        alpha_matting_erode_size: int = 10,
        session: Optional[BaseSession] = None,
        only_mask: bool = False,
        post_process_mask: bool = False,
        bgcolor: Optional[Tuple[int, int, int, int]] = None,
        *args: Optional[Any],
        **kwargs: Optional[Any]
    ) -> Union[bytes, PILImage, np.ndarray]:
        """"""
        Remove the background from an input image.
    
        This function takes in various parameters and returns a modified version of the input image with the background removed. The function can handle input data in the form of bytes, a PIL image, or a numpy array. The function first checks the type of the input data and converts it to a PIL image if necessary. It then fixes the orientation of the image and proceeds to perform background removal using the 'u2net' model. The result is a list of binary masks representing the foreground objects in the image. These masks are post-processed and combined to create a final cutout image. If a background color is provided, it is applied to the cutout image. The function returns the resulting cutout image in the format specified by the input 'return_type' parameter.
    
        Parameters:
            data (Union[bytes, PILImage, np.ndarray]): The input image data.
            alpha_matting (bool, optional): Flag indicating whether to use alpha matting. Defaults to False.
            alpha_matting_foreground_threshold (int, optional): Foreground threshold for alpha matting. Defaults to 240.
            alpha_matting_background_threshold (int, optional): Background threshold for alpha matting. Defaults to 10.
            alpha_matting_erode_size (int, optional): Erosion size for alpha matting. Defaults to 10.
            session (Optional[BaseSession], optional): A session object for the 'u2net' model. Defaults to None.
            only_mask (bool, optional): Flag indicating whether to return only the binary masks. Defaults to False.
            post_process_mask (bool, optional): Flag indicating whether to post-process the masks. Defaults to False.
            bgcolor (Optional[Tuple[int, int, int, int]], optional): Background color for the cutout image. Defaults to None.
            *args (Optional[Any]): Additional positional arguments.
            **kwargs (Optional[Any]): Additional keyword arguments.
    
        Returns:
            Union[bytes, PILImage, np.ndarray]: The cutout image with the background removed.
        """"""

I'm trying to remove the background from an image, but it's not entirely successful. Pieces of the background remain.

I tried with different values for alpha\_matting\_foreground\_threshold, alpha\_matting\_background\_threshold and alpha\_matting\_erode\_size but I did not have an improved result. Most of the time it was the same result as before, when alpha\_matting was False.

I don't understand what these parameters are, how they help, what values they can take. I thought that from 0 to 255 but I can also accept 400. I really don't understand the logic and please help me with a good explanation.",1,0
MachineLearning,t3_1cw5luk,1716174105.0,/r/MachineLearning/comments/1cw5luk/r_medical_language_agent_simulation_benchmark/,Research,[R] Medical Language Agent Simulation (Benchmark),"**Website**: [https://agentclinic.github.io/](https://agentclinic.github.io/)  
**Arxiv**: [https://arxiv.org/pdf/2405.07960](https://arxiv.org/pdf/2405.07960)

**TLDR:** AgentClinicÂ turns static medical QA problems into agents in a clinical environment (doctor, patient, medical devices) in order to present a more clinically relevant challenge for medical language models.

**Abstract:** Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information---such as which tests to perform---and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection. We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. Agents in AgentClinic-MedQA are grounded in cases from the US Medical Licensing Exam\~(USMLE) and AgentClinic-NEJM are grounded in multimodal New England Journal of Medicine (NEJM) case challenges. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents.",0,5
MachineLearning,t3_1cw5dby,1716173344.0,/r/MachineLearning/comments/1cw5dby/d_why_are_non_technical_people_leading_ai/,Discussion,[D] Why are non technical people leading AI?,"Why do non-technical figures lead the charge in AI? Pichai, Nadella, Fidji (Open AI board member), Altman, Murati, and many other VPs of AI in FAANG. Despite never coding, they hold sway in AI. Meanwhile, those coding and publishing groundbreaking work are often stuck at lower levels. 

What's the deal? Why isn't the industry recognizing those publishing at Neurips, EMNLP, CVPR, or the masterminds behind creations like GPT, Gemini, or Claude? They seem to be hidden in the shadows. 

What's your take on this?

",54,0
MachineLearning,t3_1cw35by,1716166275.0,/r/MachineLearning/comments/1cw35by/r_what_is_the_stateofart_of_model_parallelism/,Research,[R] What is the state-of-art of model parallelism ? ,Is it easy to implement model parallelism with  common frameworks like PyTorch and Tensorflow? It depends on the model architecture? What are the most used approaches on model parallelism ?,14,29
MachineLearning,t3_1cw0n8b,1716158905.0,/r/MachineLearning/comments/1cw0n8b/p_simplified_pytorch_implementation_of_alphafold_3/,Project,[P] Simplified PyTorch Implementation of AlphaFold 3,,16,40
MachineLearning,t3_1cvzsqu,1716156547.0,/r/MachineLearning/comments/1cvzsqu/d_finetuning_llava_duration_and_configuration/,Discussion,[D] Fine-Tuning LLaVA: Duration and Configuration?,"Hi everyone,

I'm planning to fine-tune the LLaVA model and am curious if anyone here has experience with this. Specifically, I'm looking to understand:

1. The size of your dataset (number of images and annotations).
2. How long the fine-tuning process took.
3. Your hardware setup (GPUs, CPUs, RAM).
4. Any specific configuration settings you used.

Thanks in advance

",1,0
MachineLearning,t3_1cvxcbc,1716149967.0,/r/MachineLearning/comments/1cvxcbc/d_what_role_do_you_think_machine_learning_will/,Discussion,[D] What role do you think machine learning will play in fields like computational biology and bioinformatics in the coming years?,"I believe that computation biology and bioinformatics are going to be adopting ML work more and more, and Iâ€™m quite excited to see what advancements are made. I think it is going to open up a whole new world in terms of matching diseases to current medications that could potentially be used off label. What other things should we be on the lookout for?

Who are some researchers working in this world?",28,29
MachineLearning,t3_1cvwohz,1716148240.0,/r/MachineLearning/comments/1cvwohz/d_are_llm_observability_tools_really_used_in/,Discussion,[D] Are LLM observability tools really used in startups and companies?,"There are many LLM observability and monitoring tools launching every week. Are they actually used by real startups and companies? 

These tools seem to do one or a combination of the following:
- **monitor LLM inputs and outputs** for prompt injection, adversarial attacks, profanity, off-topic content, rtc
- **monitor LLM metrics** over time such as cost, latency, readability, output length, and custom metrics (tone, mood, etc), drift
- **prompt management**: a/b testing, versioning, gold standard set

What have you observed â€” in real companies who have their own LLM-powered features or products, do they used these tools?",22,25
MachineLearning,t3_1cvwkyq,1716147972.0,/r/MachineLearning/comments/1cvwkyq/p_title_i_created_a_neural_network_to_quickly/,Project,[P] Title: I created a Neural Network to quickly detect spoken vowels 20 times per second,"&#x200B;

Quick disclaimer: I am aware that there is an internaltional standard for labeling the diferent recognized speech sounds (phonemes), but I wanted ASCII or extended ASCII for programming simplification, so I use a different nomeclature. Besides, it's easier for me to recognize and read. -Please forgive me

&#x200B;

So I have often wondered about the real rules that govern speech that people use. For instance using something similar to a ""glottal stop"" to end words like ""don't"" and ""that"". The ""t"" is not pronounced. Or how ""r"" is almost always used as a vowel (in american english). My favorite examples are ""fur"", ""fir"", and ""-fer"". All three are pronounced identically and the typical ""i,u,e"" vowels are not pronounced at all. Its just pronounced ""fr"".

&#x200B;

One day I was looking at a spectrograph of my voice, and I noticed some patterns. Vowels like ""ah"" in ""stop"" and ""Bob"" look very different from other vowels like ""ee"" in ""green"" and ""bee"". When we speak, there is the most prominant lowest frequency called the ""fundamental"", and there are many other frequencies that are multiples of that frequency called ""harmonics"". The sound ""ah"" has high volume on many of the harmonics, but the sound ""ee"" has a big gap where the harmonics are much much smaller. Every different vowel had its own combination of different harmonic values.

&#x200B;

So I tried to create a set of rules by hand to classify different frequency patterns as different vowels. I could easily tell them apart by looking at them, but would the rules hold up to the test? So I made a computer program to guess different vowels, but it was not good. There are so many knobs to turn to create the different rules. And if there is variability, then I would also have to go through and determine all of the different ranges which would make the rules much more complex. 

&#x200B;

I started to do it by hand and tweak values, see how it worked, and then tweak the values again, etc, etc.

&#x200B;

Thats when it hit me! I'm doing what a neural network trainer does. I could use one to do this for me!

&#x200B;

So I researched the nitty gritty of getting one setup, recorded a lot of data (\~45 minutes worth) and trained the model. It took a few days to figure out some problems, but I eventually got it working.

&#x200B;

I used python and the tensoflow+keras library suite to create and train the neural network, Pyaudio for recording training data and realtime audio, numpy for data analysis. The neural network had 264 input nodes, 100 intermediate nodes, and 13 output nodes (one node for ""no vowel"", and 12 for the different vowels). The frequency calculation finishes within 1milisecond, and the neural network finishes within 2 milisecond as well on my hardware (intel i3-1115G4 at 4GHz). It spends more of its time on listening for audio than it does computing the answer. I found best results by running the loop 20 times per second (50ms) but I have also gotten it to run at 50 times per second (20ms), but it struggles on one or two vowels.

&#x200B;

Here is a list of the different vowels that it recognizes

&#x200B;

Ó‘ aa cat,  1 

Å ah stop,  2 

Ä“ = ee green,  3 

Å = oh gross,  4 

oo = oo mood blue goose,  5

Ä­ = ih sit,6 

Ä = ay stay,  7

Ä• = eh pet,  8 

Å­ = uh bump,  9 

oÍo = ou would could should took,  10 

rÌƒ = (i chose this symbol) ur fur fir fer rural,  11 

L' = LL travel left rural,  12 ",2,1
MachineLearning,t3_1cvv50l,1716144118.0,/r/MachineLearning/comments/1cvv50l/r_visual_guide_to_the_kmeans_clustering_algorithm/,Research,[R] Visual Guide to the K-Means Clustering Algorithm. ğŸ‘¥,"
TL;DR: K-Means clustering groups data points into clusters based on their similarities, making it useful for applications like customer segmentation, image segmentation, and document clustering.

[K-Means Clustering Visual Guide](https://open.substack.com/pub/codecompass00/p/k-means-clustering-a-visual-guide?r=rcorn&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)  


*Processing img 92n1nckko01d1...*

",1,0
MachineLearning,t3_1cvtx30,1716140826.0,/r/MachineLearning/comments/1cvtx30/d_softs_efficient_multivariate_time_series/,Discussion,[D] SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion,"Happy to share my latest Medium article about Time Series Forecasting.""SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion"" It is about SOFTS, an innovative MLP-based model that utilizes the novel STar Aggregate-Dispatch (STAD) module to centralize channel interactions, achieving superior forecasting performance with linear complexity. Unlike traditional methods that struggle with the trade-off between robustness and complexity, SOFTS efficiently captures channel correlations, paving the way for scalable and accurate predictions across various fields like finance, traffic management, and healthcare. 

  
[https://medium.com/towards-artificial-intelligence/softs-efficient-multivariate-time-series-forecasting-with-series-core-fusion-0ac40d2adcd2](https://medium.com/towards-artificial-intelligence/softs-efficient-multivariate-time-series-forecasting-with-series-core-fusion-0ac40d2adcd2)",2,6
MachineLearning,t3_1cvsviu,1716137920.0,/r/MachineLearning/comments/1cvsviu/d_does_dspy_actually_change_the_lm_weights/,Discussion,[D] Does DSPy actually change the LM weights? ,"I always thought it's essentially glorified and structured prompt engineering (very useful still IMO), but it also claims in the docs that it fine-tunes and changes LM weights, and then absolutely refuses to elaborate on this in any of the sections in their docs.

I don't even understand how it can change the actual parameters of the LM, especially if we're using third party API calls for the LMs. 

By LM weights, I assume it means the weights of the last layers of the transformer model. When they describe optimizers, they say ""DSPy introduces new optimizers, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a metric you want to maximize.""

Am I misunderstanding what they mean by LM weights?

I'm sorry if this is a stupid question, but I just can't seem to find any information about this. Thanks in advance!",3,12
MachineLearning,t3_1cvsmgq,1716137211.0,/r/MachineLearning/comments/1cvsmgq/p_text_to_openpose_and_weird_rnn_bugs/,Project,[P] Text to Openpose and Weird RNN bugs,"I want to create AI that generate openpose from textual description for example if input ""a man running"" output would be like the image I provided Is there any model architecture recommend for me?

my data condition is

* canvas\_width: 900px
* canvas\_height: 300px
* frames: 5 (5 person)

[expected output](https://drive.google.com/file/d/1vNcc5-zxpGodaGG3vat3TSPkbuC6olMK/view?usp=sharing)

I trying to train RNN for this task and I use sentence transformer for embedding text and then pass to RNN and the loss is look like image below

    from sentence_transformers import SentenceTransformer 
    sentence_model = SentenceTransformer(""all-MiniLM-L6-v2"")
    text = ""a man running""
    text_input = torch.tensor(sentence_model.encode(text), dtype=torch.float)

[loss image with num\_layers=3](https://drive.google.com/file/d/1580_OdM146PefgIeRrK-Cmn-mIU_7AwB/view?usp=sharing)

My RNN setting

    embedding_dim = 384
    hidden_dim = 512
    num_layers = 3
    output_dim = 180
    num_epochs = 100
    learning_rate = 0.001
    rnn_model = RNN(embedding_dim, hidden_dim, num_layers, output_dim)

but the problem is whatever I input the output is the same everytime! but when I try changing num\_layers to 1 and keep other setting the same like this

    embedding_dim = 384
    hidden_dim = 512
    num_layers = 1
    output_dim = 180
    num_epochs = 100
    learning_rate = 0.001
    rnn_model = RNN(embedding_dim, hidden_dim, num_layers, output_dim)

the loss now look like this [loss image with num\_layers=1](https://drive.google.com/file/d/1c-Q6qOr8yPWn0gYL74tnbPm7zBIPrFOz/view?usp=sharing) and now the problem is gone !!

Also I try to check the cause of the ""output is the same everytime"" problem I check dataloader and other code but no problem was found only num\_layers=3 that cause the problem num\_layers=1 fixed it

This is my training loop

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate)
    
    trainingEpoch_loss = []
    validationEpoch_loss = []
    
    for epoch in range(num_epochs):
    Â  Â  step_loss = []
    Â  Â  rnn_model.train()
    Â  Â  for idx, train_inputs in enumerate(train_dataloader):
    Â  Â  Â  Â  optimizer.zero_grad()
    Â  Â  Â  Â  outputs = rnn_model(torch.unsqueeze(train_inputs['text'], dim=0))
    Â  Â  Â  Â  training_loss = criterion(outputs, train_inputs['poses'])
    Â  Â  Â  Â  training_loss.backward()
    Â  Â  Â  Â  optimizer.step()
    Â  Â  Â  Â  step_loss.append(training_loss.item())
    
    Â  Â  Â  Â  if (idx+1) % 1 == 0: print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{len(train_dataloader)}], Loss: {training_loss.item():.4f}')
    Â  Â  trainingEpoch_loss.append(np.array(step_loss).mean())
    
    Â  Â  rnn_model.eval()
    Â  Â  for idx, val_inputs in enumerate(val_dataloader):
    Â  Â  Â  validationStep_loss = []
    Â  Â  Â  outputs = rnn_model(torch.unsqueeze(val_inputs['text'], dim=0))
    Â  Â  Â  val_loss = criterion(outputs, val_inputs['poses'])
    Â  Â  Â  validationStep_loss.append(val_loss.item())
    Â  Â  validationEpoch_loss.append(np.array(validationStep_loss).mean())

This is my Inference

    text = ""a man running""
    processed_text = torch.tensor(sentence_model.encode(text), dtype=torch.float)
    output_poses = rnn_model(processed_text.unsqueeze(0))
    print(output_poses.shape) #shape=(1, 180) 1 person is 36 (original data for 1 person is 54 but I change to 36 because I want only x and y and not z so cut out the z axis) and there's 5 person so 5*36 = 180

My question is

1. Is there any model architecture recommend for this task other than RNN?
2. Why whatever I input the output is the same everytime when num\_layers=3 I'm very confused because the loss wouldn't go down if the model was giving the same output right? that's mean it give the same output in the Inference phase

Expected Answer

1. Model architecture that suit best for my task any papers or github repo related given would be appreciated
2. Answer why whatever I input the output is the same everytime when num\_layers=3",1,1
MachineLearning,t3_1cvslyc,1716137172.0,/r/MachineLearning/comments/1cvslyc/d_how_did_openai_go_from_doing_exciting_research/,Discussion,[D] How did OpenAI go from doing exciting research to a big-tech-like company?,"I was recently revisiting OpenAIâ€™s paper on [DOTA2 Open Five](https://cdn.openai.com/dota-2.pdf), and itâ€™s so impressive what they did there from both engineering and research standpoint. Creating a distributed system of 50k CPUs for the rollout, 1k GPUs for training while taking between 8k and 80k actions from 16k observations per 0.25sâ€”how crazy is that?? They also were doing â€œsurgeriesâ€ on the RL model to recover weights as their reward function, observation space, and even architecture has changed over the couple months of training. Last but not least, they beat the OG team (world champions at the time) and deployed the agent to play live with other players online. 

Fast forward a couple of years, they are predicting the next token in a sequence. Donâ€™t get me wrong, the capabilities of gpt4 and its omni version are truly amazing feat of engineering and research (probably much more useful), but they donâ€™t seem to be as interesting (from the research perspective) as some of their previous work.

So, now I am wondering how did the engineers and researchers transition throughout the years? Was it mostly due to their financial situation and need to become profitable or is there a deeper reason for their transition?
",134,371
MachineLearning,t3_1cvs57n,1716135908.0,/r/MachineLearning/comments/1cvs57n/d_computer_vision_in_icml/,Discussion,[D] Computer vision in ICML,"Hi, this is my first year attending ICML. Based on past conferences, I was wondering how much content on computer vision typically appears at this conference, if any?",5,1
MachineLearning,t3_1cvrunk,1716135137.0,/r/MachineLearning/comments/1cvrunk/multimodal_ai_from_first_principles_most/,Discussion,Multimodal AI from First Principles - Most fundamental approaches [D],Sharing a video I made on some of the most critical and fundamental building blocks to train Multimodal models for the past decade or soâ€¦ hope you enjoy if the topic interests you!,0,6
MachineLearning,t3_1cvqxuz,1716132761.0,/r/MachineLearning/comments/1cvqxuz/p_tensorrt_cpp_codebase_for_onnx_models_dynamic/,Project,"[P] Tensorrt CPP codebase for onnx models: Dynamic batching, All models, Single file models","

[https://github.com/PrinceP/tensorrt-cpp-for-onnx/tree/main](https://github.com/PrinceP/tensorrt-cpp-for-onnx/tree/main)

Created a area for having CPP codebase for Tensorrt using ONNX models. Currently YOLOV9, YOLOV8\[Detect, Segment, Classify, OBB, POSE\] are coded. Other models are in progress.",0,2
MachineLearning,t3_1cvq77y,1716130817.0,/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/,Discussion,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",92,12
MachineLearning,t3_1cvp1m3,1716127515.0,/r/MachineLearning/comments/1cvp1m3/d_how_to_definitely_say_if_my_dataset_is_guassian/,Discussion,[D] How to definitely say if my Dataset is Guassian,"I'm following some tutorials on doing some linear regression and as I was building my notebook, I'm working on outlier detection and amongst the techniques described for doing outlier detection, one of them involved calculating the Standard Deviation, but for this I need to know if my columns are of Guassian distribution. I'm aware that there are different techniques like:

* Histograms
* KDE Plot
* Q-Q Plot
* Kolomogorov-Smirnov Test
* Shapiro-Wilk Test
* D'Agostino and Pearson's Test

And I bet there are a few more as well. So what is the best one to use? I guess Histograms just give a clue but do not show the real intention. What is the standard practice to identify if the dataset is Guassian or not?",23,0
MachineLearning,t3_1cvp0x8,1716127463.0,/r/MachineLearning/comments/1cvp0x8/d_culture_of_recycling_old_conference_submissions/,Discussion,[D] Culture of Recycling Old Conference Submissions in ML,"I work on statistical ML. I notice that many people (including myself and those that I review) often recycle their submissions for ML conferences.

E.g., if their papers got rejected by ICML, they submit to NeurIPS, and later to ICLR (or UAI/AISTATS which are also top in my field). If they did not get into ICML/NeurIPS/ICLR after 2\~3 times, they would submit them to AAAI/IJCAI/TMLR/ICDM, journals like T-NNLS/T-KDD/NN/Neurocomputing, or domain-specific venues like LoG/CoLLAs/AABI. After all these, if the paper still did not get accepted, they then simply put them or arXiv. I believe this might also be the case for CV/NLP.

As a reviewer, I often encounter conference submissions where the authors resubmit without really taking into account the previous reviews provided. Sometimes they do incorporate the reviews when resubmitting--but sometimes the work may just be not at the level of Tier 1 conferences but they just keep resubmitting and hoping that they can accepted by chance.

I think that this is consuming a lot of reviewers' time from the community to keep reviewing the same submissions (especially given that NeurIPS hits 20k submission id; I expect to see many resubmissions). This is perhaps also one of the reason TMLR was born (to emphasize correctness instead of novelty).

I do understand arguments like ""the quality of research is more important than the publication venues"" or ""OpenAI often simply just put their papers like GPT-X on arXiv these days"". However, students or junior researchers also need publications in their career, including myself. 

What do folks think about it?",17,60
MachineLearning,t3_1cvoten,1716126872.0,/r/MachineLearning/comments/1cvoten/d_how_do_you_efficiently_conduct_ablation_studies/,Discussion,[D] How Do You Efficiently Conduct Ablation Studies in Machine Learning?,"When conducting ablation studies for a model that can be pretrained and fine-tuned, do you perform a full grid search for each ablated version during both pretraining and fine-tuning? Or do you have strategies to make this process more efficient? Thank you for your insights.",11,43
MachineLearning,t3_1cvkrpf,1716112586.0,/r/MachineLearning/comments/1cvkrpf/p_nwayattention/,Project,[P] N-way-attention,"I have been playing with the concept of attending to more than two tokens in transformer models. Instead of having one query and one key for example, having two keys and one query, and for every query sum over every pair of previous tokens.

It makes the algorithm even slower ( O(n\*\*3) instead of O(n\*\*2)), but I think it is a fun concept. Some results where surprising to me, like how good it is at finding the longest increasing subsequence.

I want it to share it:  
[https://github.com/Gusanidas/n-way-attention/tree/main](https://github.com/Gusanidas/n-way-attention/tree/main)

And to ask if anyone knows of papers that treat the concept, or mention it.",17,32
MachineLearning,t3_1cvkmwe,1716112000.0,/r/MachineLearning/comments/1cvkmwe/d_what_is_the_current_state_of_llm_ops/,Discussion,[D] What Is The Current State of LLM Ops ,"Curious about how people are putting their RAG and other LLM powered applications into production today. How do you define LLM Ops? What is the process like in your team/company, and what combination of tools are you using today to implement or automate those processes and what are some of the gap areas.

I'm especially interested in what people are doing around the issue of efficiency scaling larger models across nodes in production settings. Do you apply any GPU virtualization/fractionalization and what are some good resources for these? ",1,9
MachineLearning,t3_1cviv1u,1716104133.0,/r/MachineLearning/comments/1cviv1u/d_how_to_get_word_embedding_in_word2vec_cbow/,Discussion,[D] How to get word embedding in Word2Vec CBOW method?,"I'm trying to implement CBOW algorithm using PyTorch. I know the hidden layer is the embedding of the targeted word and it's dimension is equal to dimension I want my embeddings to be in. It's quite difficult for me to understand when to get the embeddings. **Is it that after back-propagation I again need a forward pass to get the correct hidden layer output, or is it something else?** Also, please correct me if I'm wrong anywhere.

Following is the CBOW class implementation.

    class CBOW (Module):
        def __init__(self, in_channel: int, out_channel : int, winSize : int):
            super().__init__()
            self.N = in_channel
            self.V = out_channel
    
            self.lin1 = Linear(in_features= self.N, out_features= self.V)  
            self.lin2= Linear(in_features=self.V, out_features= self.N)
            self.softmax= Softmax(dim=1) 
       
        def forward(self,input : torch.Tensor): 
            assert len(input.shape) == 2, ""Input recieved is not in correct dimension""
            assert input.shape[1] == self.N, ""Word feature vector is not matching""
    
            input = self.lin1(input)
            embeddings = torch.mean(input, dim=0, keepdim= True)
            out = self.lin2(embeddings)
            return self.softmax(out)
        
        def backward(self, prediction : torch.Tensor, target : torch.Tensor):
            assert prediction.shape == target.shape , f""Input shapes not matching\nPrediction shape : {prediction.shape}\nTarget shape : {target.shape}""
            loss_fn = MSELoss()
            loss = loss_fn(prediction, target)
            loss.backward()
            return loss",4,0
MachineLearning,t3_1cvh12y,1716096900.0,/r/MachineLearning/comments/1cvh12y/create_stunning_ai_qr_code_art_in_2_minutes/,Discussion,Create Stunning AI QR Code Art In 2 Minutes! [Discussion] ,,0,0
MachineLearning,t3_1cvgn92,1716095394.0,/r/MachineLearning/comments/1cvgn92/r_kolmogorovarnold_networks_kans_explained_a/,Research,[R] Kolmogorov-Arnold Networks (KANs) Explained: A Superior Alternative to MLPs,,0,0
MachineLearning,t3_1cvfrd9,1716092172.0,/r/MachineLearning/comments/1cvfrd9/intersection_of_ml_distributed_systems_d/,Discussion,Intersection of ML & Distributed Systems [D],"What are some existing problems at the intersection of Distributed Systems and ML?

I have a decent background in both, and I want to work on projects that employ distributed computing to solve problems in ML. What are some good resources to look at? Or how to start?",5,5
MachineLearning,t3_1cvfaot,1716090589.0,/r/MachineLearning/comments/1cvfaot/p_cafusion_diffusion_model_for_generating_cat/,Project,[P] Cafusion: Diffusion model for generating cat images,"I've been working on this project for a while now. It can only generate nightmare fuel images that don't even look like cats but I'm trying to make it better

here's the repo: [https://github.com/Null-byte-00/Catfusion](https://github.com/Null-byte-00/Catfusion)

and here's the jupyter notebook: [https://nbviewer.org/github/Null-byte-00/Catfusion/blob/main/catfusion.ipynb](https://nbviewer.org/github/Null-byte-00/Catfusion/blob/main/catfusion.ipynb)",4,2
MachineLearning,t3_1cvf2jf,1716089783.0,/r/MachineLearning/comments/1cvf2jf/dwhy_dont_we_see_zero_shot_truthfulqa_performance/,Discussion,[D]why donâ€™t we see zero shot Truthfulqa performance listed on papers ?,"My intuition was itâ€™s one of the most important metric , but we normally see multi shot performance. like in phi3 paper 10 shot performance was reported.",4,5
MachineLearning,t3_1cval53,1716075734.0,/r/MachineLearning/comments/1cval53/d_is_it_possible_to_train_vitmae_with/,Discussion,[D] Is it possible to train ViTMAE with Hyperspectral Satellite Images?,I'm trying to train the ViTMAE encoder to learn representations of some Hyperspectral Satellite Images. The Images are in TIFF format and have many bands (224). Is it possible to train the ViTMAE with this high number of input bands? Any idea how I should go about it?,3,6
MachineLearning,t3_1cv6odn,1716064655.0,/r/MachineLearning/comments/1cv6odn/d_mamba_convergence_speed/,Discussion,[D] Mamba Convergence speed,"I am training mamba on sequential labelling task with an imbalanced dataset, I have nearly 800k training example. After one epoch performances on minority class are terrible near zero.
I tried to overfit one batch and couldn't achieve this.
I tried weighted loss too.
I wanted to know wether this is normal ? Does mamba star this way from the beginning and then starts to converge ?",7,6
MachineLearning,t3_1cv3n3a,1716056506.0,/r/MachineLearning/comments/1cv3n3a/p_local_rag_with_retsim_ollama_and_gemma/,Project,"[P] Local RAG with RETSim, Ollama and Gemma ","Happy Saturday,

Here is a [short write-up and notebook](https://elie.net/blog/ai/wingardium-trivia-osa-on-device-sorting-hatbot-powered-by-gemma-ollama-usearch-and-retsim) on how to use RETSim our novel near-duplicate text embedding that was present at ICLR a few days ago to create an on-device RAG by combining it with Ollama, Gemma and Usearch.

RETSim  paper,  [model and startup code](https://github.com/google/unisim) are also open-sourced - So if you need near-duplicate detection or similarity computation give it a shot!Â 

As always I hope you will find those notebooks and writeup useful and I am happy to answer questions.

I can't wait to hear what you will use RETSim for.

Have a wonderful weekend",0,4
MachineLearning,t3_1cv11oo,1716049535.0,/r/MachineLearning/comments/1cv11oo/project_tabletop_handybot_lowcost_robotic_arm/,Project,[Project] Tabletop HandyBot: low-cost robotic arm assistant for tabletop tasks,"Hi everyone, I would like to showcase the first functional version of my side project: Tabletop Handybot, a low-cost AI powered robotic arm assistant that listens to your voice commands and can carry out a variety of tabletop tasks. 

Currently it can pick and place arbitrary objects presented in front of it. It leverages some of the latest AI models like ChatGPT, Grounding DINO, Segment Anything, and Whisper. Everything is zero-shot and requires no AI training. The total BOM is $2300 USD which hopefully will be affordable to hobbyists and tinkerers.

I hope to continue adding support to handle more tasks.

Github: [https://github.com/ycheng517/tabletop-handybot](https://github.com/ycheng517/tabletop-handybot)",1,11
MachineLearning,t3_1cv0m9x,1716048342.0,/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/,Research,[R] Grounding DINO 1.5 Release: the most capable open-set detection model,"We introduce Grounding DINO 1.5, which is our most powerful open-world object detection model series. Building on the solid foundation of its predecessor, Grounding DINO, this enhanced model increases both the model size and its training dataset, enhancing its ability to understand and detect visual objects more accurately.

Github Link: [https://github.com/IDEA-Research/Grounding-DINO-1.5-API](https://github.com/IDEA-Research/Grounding-DINO-1.5-API)

Online Demo: [https://deepdataspace.com/home](https://deepdataspace.com/home)

Huggingface Demo: [https://huggingface.co/spaces/Mountchicken/Grounding-DINO-1.5](https://huggingface.co/spaces/Mountchicken/Grounding-DINO-1.5)

arXiv Link: [https://arxiv.org/abs/2405.10300](https://arxiv.org/abs/2405.10300)

# Zero-Shot Performance Compare with Grounding DINO

https://preview.redd.it/7sc0fq58k71d1.png?width=1149&format=png&auto=webp&s=3312392b5d6b820fa446f8cc2600c52ce33a6079

detailed performance:

https://preview.redd.it/kcpd3awek71d1.png?width=1280&format=png&auto=webp&s=498acee90fa95c73a87f8e9ee92a0f27430b2c51

# Fine-tuning Performance on Downstream Tasks

compare with Grounding DINO

https://preview.redd.it/d0spf0jjk71d1.png?width=1020&format=png&auto=webp&s=46073d82f6d9468852a5595b93df99b0148208f2

detailed performance

https://preview.redd.it/5m85gsylk71d1.png?width=1756&format=png&auto=webp&s=554d0b72621d79482c1ba11ba2821b51d1c65459

# Visualizations

https://preview.redd.it/lob31unpk71d1.png?width=1052&format=png&auto=webp&s=02fbffb5f23d6d8cb6b3d5cbe4e5a2146365d977

https://preview.redd.it/f1oufwnpk71d1.png?width=1039&format=png&auto=webp&s=42b906c6517181612583f89d0e69b73f3f9738d8

https://preview.redd.it/xxp98kopk71d1.png?width=1066&format=png&auto=webp&s=8dd7e47a1da43a62a41c2b78489b54816e208d7d

https://preview.redd.it/0l6te2opk71d1.png?width=1011&format=png&auto=webp&s=81c5a777a9728449b452c1cef4017fcc2e6f13e3

https://preview.redd.it/4fwp33spk71d1.png?width=2365&format=png&auto=webp&s=009c5f063fb2f0688e334d0f815e528926ce9430

https://preview.redd.it/nsgmwdspk71d1.png?width=2365&format=png&auto=webp&s=3e7125d6fa8799ad75e1db869538781c59689daf

https://preview.redd.it/o01ycarpk71d1.png?width=2367&format=png&auto=webp&s=fdffd0b2ddbff78b1b46286256e0178993304e0a

https://preview.redd.it/absfvhopk71d1.png?width=1105&format=png&auto=webp&s=d760d0082b7c271837b844c70973068db00a4e7b

https://preview.redd.it/8apy7ropk71d1.png?width=1118&format=png&auto=webp&s=8686af325641535bcd4baf12152918f5b7e60bcc

https://preview.redd.it/p65qh6qpk71d1.png?width=1106&format=png&auto=webp&s=2d9a62edf6dd6f086cd0396eca7df0c43356b6f0

https://preview.redd.it/mavta4qpk71d1.png?width=1038&format=png&auto=webp&s=d031236a761720de80e4d5e6468a41dc9f988f6a

https://preview.redd.it/th55xqppk71d1.png?width=1087&format=png&auto=webp&s=0266ebb44a9c24b5e611ce1fdbb2bcd9abf51725

https://preview.redd.it/0373ueqpk71d1.png?width=999&format=png&auto=webp&s=f1379231677d157a706ffd2a5bdf451ca61517bb



",17,27
MachineLearning,t3_1cv0hl2,1716048006.0,/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/,Discussion,[D] Foundational Time Series Models Overrated?,"I've been exploring foundational time series models like TimeGPT, Moirai, Chronos, etc., and wonder if they truly have the potential for powerfully sample-efficient forecasting or if they're just borrowing the hype from foundational models in NLP and bringing it to the time series domain.

I can see why they might work, for example, in demand forecasting, where it's about identifying trends, cycles, etc. But can they handle arbitrary time series data like environmental monitoring, financial markets, or biomedical signals, which have irregular patterns and non-stationary data?

Is their ability to generalize overestimated?",36,108
MachineLearning,t3_1cv06td,1716047167.0,/r/MachineLearning/comments/1cv06td/d_first_time_attending_icml/,Discussion,[D] First time attending ICML,"First time attending ICML

Hi everyone 

I want to attend ICML for the first time. Iâ€™m super excited but have a few questions.

Background: Iâ€™m currently doing my MSc in Artificial Intelligence.

My questions are:

1. Conference + Tutorial + Workshop tickets are kind of expensive and itâ€™s a bit inconvenient for me to attend for a whole week. Iâ€™m unsure if I should only attend workshops, or the full conference plus workshops. Could someone explain what exactly the difference between workshop sessions and conference sessions is? And does one make more sense to attend as a student than another?

2. On the ICML homepage in the schedule I cannot see the topics of the speeches at the conference. It only says â€œOral 1â€, â€œOral 2â€, etc. It says I need to be registered (~500EUR) to even see the topics of the speeches. Is this normal? Iâ€™d like to know which speeches there are before purchasing a ticket.

Thanks so much in advance for answering my noob questions! Any help/info is very much appreciated!",4,0
MachineLearning,t3_1cuzvy1,1716046324.0,/r/MachineLearning/comments/1cuzvy1/here_is_how_transformers_ended_the_tradition_of/,Discussion,Here is how Transformers ended the tradition of Inductive Bias in Neural Nets [D],"A video I made that discusses the role inductive bias and generality while comparing transformers/attention with traditional deep learning like CNNs, RNNs, and even MLPs.",0,5
MachineLearning,t3_1cuzkez,1716045432.0,/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/,Research,[R] Do Llamas Work in English? On the Latent Language of Multilingual Transformers,"**Paper**: [https://arxiv.org/abs/2402.10588](https://arxiv.org/abs/2402.10588)

**Code**: [https://github.com/epfl-dlab/llm-latent-language](https://github.com/epfl-dlab/llm-latent-language)

**Dataset**: [https://huggingface.co/datasets/wendlerc/llm-latent-language](https://huggingface.co/datasets/wendlerc/llm-latent-language)

**Colab links**:

**(1)** [https://colab.research.google.com/drive/1l6qN-hmCV4TbTcRZB5o6rUk\_QPHBZb7K?usp=sharing](https://colab.research.google.com/drive/1l6qN-hmCV4TbTcRZB5o6rUk_QPHBZb7K?usp=sharing)

**(2)** [https://colab.research.google.com/drive/1EhCk3\_CZ\_nSfxxpaDrjTvM-0oHfN9m2n?usp=sharing](https://colab.research.google.com/drive/1EhCk3_CZ_nSfxxpaDrjTvM-0oHfN9m2n?usp=sharing)

**Abstract**:

>We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in ""input space"", ""concept space"", and ""output space"", respectively. Crucially, our evidence suggests that the abstract ""concept space"" lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models.",0,16
MachineLearning,t3_1cuzbta,1716044769.0,/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/,Research,[R] Robust agents learn causal world models,"**Paper**: [https://arxiv.org/abs/2402.10877](https://arxiv.org/abs/2402.10877)

**Abstract**:

>It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.",3,24
MachineLearning,t3_1cuz1i2,1716043977.0,/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/,Discussion,[D] Library for named entity recognition,"Hi Guys, I'm needing to decide which library to use for named entity recognition. I've used spaCy, which works well, but I need a library that allows me to categorize entities and also sub-entities. Has anyone done something similar? I mean, where the same word can be more than one entity. spaCy offers the SpanCat pipeline, which theoretically allows this, but l've had trouble creating the training corpus. I think it's because they expect you to purchase an annotation text framework like Prodigy.",9,23
MachineLearning,t3_1cux80i,1716038537.0,/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/,News,[N] ICML 2024 Workshop on making discrete operations differentiable ğŸ¤–,"Hi everyone!

We are organizing the **Differentiable almost everything** workshop at ICML this year.

Many discrete operations e.g. sorting, topk, shortest paths, clustering (and many more) have null-gradients almost everywhere, and are hence not suitable for modern gradient based learning frameworks (such as deep learning). This workshop will cover research topics that aim to remedy such problems!

[https://differentiable.xyz/](https://differentiable.xyz/)

We encourage anyone who is working on relevant topics to submit their work. Even if you are not submitting, please do come by the workshop at ICML to see some of the exciting talks that will take place!

I have attached a full summary of the workshop below! All the best with your current work, L :)

*Gradients and derivatives are integral to machine learning, as they enable gradient-based optimization. In many real applications, however, models rest on algorithmic components that implement discrete decisions, or rely on discrete intermediate representations and structures. These discrete steps are intrinsically non-differentiable and accordingly break the flow of gradients. To use gradient-based approaches to learn the parameters of such models requires turning these non-differentiable components differentiable. This can be done with careful considerations, notably, using smoothing or relaxations to propose differentiable proxies for these components. With the advent of modular deep learning frameworks, these ideas have become more popular than ever in many fields of machine learning, generating in a short time-span a multitude of â€œdifferentiable everythingâ€, impacting topics as varied as rendering, sorting and ranking, convex optimizers, shortest-paths, dynamic programming, physics simulations, NN architecture search, top-k, graph algorithms, weakly- and self-supervised learning, and many more.*

*This workshop will provide a forum for anything differentiable, bringing together academic and industry researchers to highlight challenges and developments, provide unifying ideas, discuss practical implementation choices and explore future directions.*",14,85
MachineLearning,t3_1cuvwqt,1716034199.0,/r/MachineLearning/comments/1cuvwqt/d_is_an_ai_manhattan_project_possible/,Discussion,"[D] Is an AI ""Manhattan Project"" possible?","Hopefully this isn't considered too conspiratorial, but I think there is an actual interesting question here behind the click bait title. 

Historically government agencies have had secret technology that was far more advanced than the public was aware of.  Obvious examples are the airforce like the sr-71, and the decommissioned nro equipment that was loaned to hubble because it was too outdated for them to care.  In the ml space,  I had a professor claim that the cia had effectively discovered viterbi years before the public version and only admitted it years later (but I have not been able to verify this story).

Is it possible that there is secret government research into ai that is more advanced than the like of openai? The applications and demands seem obvious.  I don't want confidential information,  I'm just curious if it's even considered possible.  In my previous examples the government could fund projects at a scale that no one else would.  But public companies are already spending billions, hiring the best talent, and in many cases have massive private datasets. I'm not sure if the cia actually has any edge over the likes of alphabet.

But could they have a better technique that would not have been found by all the public competitors?  Or done something more interesting with training data that even Facebook can't acquire? ",57,0
MachineLearning,t3_1cuv6fu,1716031514.0,/r/MachineLearning/comments/1cuv6fu/d_problems_with_regards_to_database_selection_and/,Discussion,[D] Problems with regards to database selection and finding.,"Do you guys face the same problem as me where you have a brilliant idea and an way to implement a AI / ML model. But..... You seem to end all your energy in finding the relevant work towards it as well as the correct database that fits your needs.

Even if you can find the correct dataset you need to open it study it and you know look at various places to find the dataset.

Secondly, the need to download the dataset ( sadly if you dont find it for collab).

Would greatly appriciate your guys views on this. :)",6,1
MachineLearning,t3_1cut5f5,1716023065.0,/r/MachineLearning/comments/1cut5f5/r_spiking_neural_networks/,Research,[R] Spiking neural networks,"I've created a video about classical neural networks, its problems and how spiking neural networks can solve them. Will be grateful for your feedback: [https://youtu.be/gSyDlvzvhg8](https://youtu.be/gSyDlvzvhg8)  
  
For working on text i used ChatGPT:   
[https://chat.openai.com/](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbVhPc29PUFFnbC1UT1A5SEFwN2p6NXZQS3U2d3xBQ3Jtc0trS2VZV0JJZktoSXhhWWZCU252cHZRTWRjRkc3enRaTU9rWF9KSXh1UkljREVYbVdsRVU4aEs1cUtRYkc4Ym5TU2dqeVRrRW03bTVSYkIyOW1EeG9hbGRfbHZodDhmUnVORVZnYkpia3VoclVDNlJHcw&q=https%3A%2F%2Fchat.openai.com%2F&v=gSyDlvzvhg8)  
For working on video midjourney and DALL-E:  
[https://www.midjourney.com](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqblVSSmxZVDJ1YXdEdkNxWmRVWDBwZDRJcUJjZ3xBQ3Jtc0tuWE01T1ZreGx6YllRTTRLV0tOVnhRYTNrQTVJTWxQWm5qd3ZaYXl4bkFYOTRVN1lmblB6dFgxRXBVQWl0TXo4OXRzNEZycE9qUkdxOXlremdyNkl2V3VYQndEMzlwWktZaHlhb3ZfbnZmakFFaG54bw&q=https%3A%2F%2Fwww.midjourney.com%2F&v=gSyDlvzvhg8)  
[https://openai.com/index/dall-e-3/](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXk0TG9zWWdzZ2hoT3F6ZTRCSlNmZk81a0RCQXxBQ3Jtc0ttRXVzQXZOdlBYUFdHNl9wcjNSSklNRmkwMXRkWEtwT0g5d3pHU2YtcWp4Rm00MWJJV3hJRDZxTkxQNWhQdzJVbk50VVZKMU9WLVR1bmRaNW50ZGhoUWEydnh4Z3dvUlJaMlNMU010U3RHa200TEhlaw&q=https%3A%2F%2Fopenai.com%2Findex%2Fdall-e-3%2F&v=gSyDlvzvhg8)  
For generating speech - elevenlabs:  
[https://elevenlabs.io/](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2V4RTY5UXdYeWJSSDFqVjhNNFpZS29CNjZMQXxBQ3Jtc0tuQzQ0czFhUUxScU03Y2p3R29LNzBHMzBpeUZMcnZFdzY5aWFlUHJkenNCbDZLei1YRlRORTNMbXYzU3hHQW8wUTc3LV9yWFpkYXVveTBGbUtONjZpM1JzaHJhUm5FX0FJcVBFd3RRbzdVYkt1RWkxSQ&q=https%3A%2F%2Felevenlabs.io%2F&v=gSyDlvzvhg8)",4,1
MachineLearning,t3_1cut08b,1716022444.0,/r/MachineLearning/comments/1cut08b/r_are_you_human_yes_ai_am/,Research,[R] Are you human? Yes AI am!,"This article explores the use of AI to solve CAPTCHAs, a task often thought to be exclusively human. Through a controlled experiment using Claude 3 and Gemini 1.5, we demonstrate the feasibility of AI-powered CAPTCHA solutions, while underlining the importance of ethical considerations and responsible implementation.

https://medium.com/@gbasilveira/are-you-human-yes-ai-am-db649c729688",3,0
MachineLearning,t3_1cusmrp,1716020748.0,/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/,Project,[P] GPT-Burn: A simple & concise implementation of the GPT in pure Rust ğŸ”¥,,0,40
MachineLearning,t3_1curusl,1716017358.0,/r/MachineLearning/comments/1curusl/d_whats_the_best_way_to_build_ml_apps_for_macos/,Discussion,[D] What's the best way to build ML apps for MacOS?,"I'm a web dev (Python, Django) looking to get into ML. I have lots of experience in Python and have started picking up TensorFlow and PyTorch. I'm wanting to start building AI apps for MacOS and I'm trying to figure out the best workflow for this.

So far I can think of the following approaches:

* Using Python and then use packages for GUI (e.g. Tkinter) - great that everything's in Python, but Python GUIs tend to be a bit ugly... Also not sure how well these work on the app store.
* Using Electron and use TensorFlow.js - can design great UIs using HTML/CSS but limited to TensorFlow.js?
* Native MacOS apps using Swift and Core ML - never touched Swift or Core ML, so unsure of pros/cons.

A bit unsure of where to start. Any advice of people currently doing ML app dev for MacOS much appreciated.",7,4
MachineLearning,t3_1cupqed,1716008794.0,/r/MachineLearning/comments/1cupqed/d_can_mlp_layers_within_gpts_be_approximated/,Discussion,[D] Can MLP layers within GPTs be approximated using KAN layers,"This paper KAN: Kolmogorovâ€“Arnold Networks [https://arxiv.org/pdf/2404.19756](https://arxiv.org/pdf/2404.19756) (digested in [https://towardsdatascience.com/kolmogorov-arnold-networks-the-latest-advance-in-neural-networks-simply-explained-f083cf994a85](https://towardsdatascience.com/kolmogorov-arnold-networks-the-latest-advance-in-neural-networks-simply-explained-f083cf994a85) ) suggests that any 2-hidden-layer MLP layers (like used in GPTs) can be approximated by KANs:

>""KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving.""

But, can an already-pretrained GPT containing MLPs really be approximated by training a new KAN layer (plus a Low Rank parameter-efficient  PEFT adapter) as suggested by this Huggingface stub:  [https://huggingface.co/MartialTerran/GPTs\_by\_MLP-to-KAN-Transform](https://huggingface.co/MartialTerran/GPTs_by_MLP-to-KAN-Transform) KAN-based Compression of Pretrained GPT Models.

See also: The PyTorch implementation of Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling - AdityaNG/*kan*-*gpt* [https://github.com/AdityaNG/kan-gpt](https://github.com/AdityaNG/kan-gpt)  and

[https://github.com/kabachuha/nanoGPKANT/blob/main/README.md](https://github.com/kabachuha/nanoGPKANT/blob/main/README.md)",11,0
MachineLearning,t3_1cuoo2g,1716004946.0,/r/MachineLearning/comments/1cuoo2g/r_110_radio_controlled_car_autonomous_driving/,Research,[R] 1:10 Radio Controlled Car autonomous driving ,"I heavily need some advice want to be able to create a machine learning model that takes image input from 2 stereo cameras and outputs throttle and steering. I am racing this car on a track, from a platform that outlooks the track.
My thinking is to create a depth and disparity map of the racetrack and pin point the car and then track it based on the disparity map. I am getting throttle and steering data in real time from the car. How can I go about making a machine learning model that would take in image input and predict throttle/steering?",4,10
MachineLearning,t3_1cunrbn,1716001886.0,/r/MachineLearning/comments/1cunrbn/d_sft_has_higher_grad_norm_but_lower_loss/,Discussion,"[D] SFT has higher grad norm but lower loss compared to pre-trainig, why?","Recently I have been continual pre-training OpenELM-1.1B on a custom corpus w/ 2B tokens, and followed by finetuning it with custom instruction dataset w/ 4M samples.

I found that the loss (both training and eval) of PT is consistently higher than in the SFT stage when trained on similar amount of tokens, but the grad norm for SFT is higher than PT.

I assumed that the pre-trained model has a lower ppl on this custom domain, so it has a lower loss in the SFT stage. My question is, what contributes to the gradient norm being higher in the SFT stage?",1,3
MachineLearning,t3_1cumxxs,1715999209.0,/r/MachineLearning/comments/1cumxxs/cross_validation_trainvalidation_graphs_d/,Discussion,Cross validation Train/validation graphs [D],"A common trend for model evaluation that we often see is the use of Cross validation CV. Authors  often report accuracy and other metrics (f-measure, precision,..etc) derived from this approach. Alongside that, they plot training and validation graphs for both loss and accuracy as well as confusion matrices. My question is about how these graphs are generated. Are they plotted using the k folds of cross-validation, or is there another method at play ?  An example is in the paper in the link below :   [link to example](https://www.researchgate.net/publication/367565701_An_Adaptive_Batch_Size_based-CNN-LSTM_Framework_for_Human_Activity_Recognition_in_Uncontrolled_Environment)",3,4
MachineLearning,t3_1culhaw,1715994596.0,/r/MachineLearning/comments/1culhaw/d_labeling_software_advice_for_my_use_cases/,Discussion,[D] Labeling software advice for my use cases,"I'm looking for software to help me segment and categorize DICOM datasets (both images and cines (movies)), at first 2D, but later probably CT/MRI as well, and also allow me to annotate procedural videos as well.  Because I need to keep track of so many images (including which images I want to use/exclude from a given series from analysis, and keep track of patient attributes as well) the data management is a big part of what I need.  As well as the ability to assign workload to other labelers (but likely not more that 3-4 other people)...

Could you kindly share your thoughts regarding other software such as CVAT, Encore, Labelbox, etc.

Many thanks in advance.  I hope I can learn from your experiences to make this as 'easy' as possible, I don't have anyone local to get advice from...",1,0
MachineLearning,t3_1cukc34,1715991051.0,/r/MachineLearning/comments/1cukc34/p_how_to_keep_only_the_top_10k_most_common_tokens/,Project,[P] How to keep only the top 10K most common tokens (transformers library),"I am following along with the [TinyStories](https://arxiv.org/abs/2305.07759) dataset paper and it claims â€œWe use the GPT-Neo tokenizer but only keep the top 10K most common tokensâ€. I am trying to create my own tokenizer that does this, but realized I donâ€™t have a merges file- removing the top tokens doesnâ€™t handle the missing byte-pair encodings without retraining. My understanding on this isnâ€™t great, and I asked GPT and it suggested it wasnâ€™t a great idea to ignore it and use the old file without the top tokens removed. 

When looking on the huggingface for it, their merges and vocab use the full 50k tokens, so Iâ€™m a bit confused on how to accomplish this. Thanks!
",5,13
MachineLearning,t3_1cujsvt,1715989513.0,/r/MachineLearning/comments/1cujsvt/p_python_script_to_correlate_semantic_scholar/,Project,[P] Python script to correlate semantic scholar bibtex output to ACL anthology,"[https://github.com/Ghost---Shadow/anthology-porter](https://github.com/Ghost---Shadow/anthology-porter)

I built a python script which takes semantic scholar bibtex as input and correlates that with ACL anthology. EMNLP needs its citations in ACL anthology format, it is a bit of a pain to manually correlate, so I created a script for it.

Please try it out and let me know if you find any false positives/negatives.",0,3
MachineLearning,t3_1cug8iw,1715979784.0,/r/MachineLearning/comments/1cug8iw/p_improving_table_data_extraction_from_financial/,Project,[P] Improving Table Data Extraction from Financial Documents: Multimodal RAG with GPT-4o and Pathway,"Hey r/MachineLearning, I'm sharing a showcase on how we improved RAG accuracy on documents with visual elements such as tables and plots by using GPT-4o in both the parsing and answering stages.

It consists of several parts:

**Data indexing pipeline (incremental):**

1. We extract tables as images during the parsing process.
2. GPT-4o explains the content of the table in detail.
3. The table content is then saved with the document chunk into the index, making it easily searchable.

**Question Answering:**

Then, questions are sent to the LLM with the relevant context (including parsed tables) for the question answering.

**Results:**

Our method has outperformed traditional RAG toolkits on questions based on tables. To demonstrate this, we used a few sample questions derived from the Alphabet's 10K report, which is packed with many tables.

**Architecture diagram**: [https://github.com/pathwaycom/llm-app/blob/main/examples/pipelines/gpt\_4o\_multimodal\_rag/gpt4o.gif](https://github.com/pathwaycom/llm-app/blob/main/examples/pipelines/gpt_4o_multimodal_rag/gpt4o.gif)Â 

**Repo**: [https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/gpt\_4o\_multimodal\_rag/](https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/gpt_4o_multimodal_rag/)

We are working to extend this project, happy to take comments!",0,9
MachineLearning,t3_1cub74b,1715967148.0,/r/MachineLearning/comments/1cub74b/d_machine_learning_engineers_what_portion_of_your/,Discussion,"[D] Machine Learning Engineers, what portion of your work is focused on deployment pipelines vs. model building/tuning?","Iâ€™m currently a machine learning engineer, but I focus much more heavily on the pipelines in a way that is similar to when I was a data engineer. Iâ€™d love to get more into the model building side of things, but my model knowledge has gotten a bit rusty since I finished my M.S. in Statistics.

What portion of your day to day work is focused on deploying compared to model building?",46,50
MachineLearning,t3_1cu8qib,1715961201.0,/r/MachineLearning/comments/1cu8qib/r_seeking_advice_on_a_methodological_oversight/,Research,[R] Seeking Advice on a Methodological Oversight: Navigating Errors and Seeking Clarity,"Hey everyone,

I am sharing a recent experience as a PhD student and seeking advice on interpreting a methodological oversight in our study.

Upon reflection on a recent paper we submitted (I already submitted the camera-ready version), I realized we made a methodological error. Specifically, we used a validation set derived from the test set, potentially introducing bias into our results because we have heterogeneous datasets (training and test datasets are from different origins). We monitored the accuracy of the validation set and used it for early stopping. But the correct method is to make the validation set from the training set, right? I got confused because I read that in some cases, some people use validation set and test set for the same purpose.

I was surprised that neither my coauthors nor the three reviewers spotted this error during the peer-review process. Does the oversight of neither my coauthors nor the reviewers identifying the error imply that its severity can be downplayed?

Additionally, we conducted measurements on five tests to average our results - but I'm uncertain about whether this helps mitigate the impact of the error. Your input on this matter would be greatly appreciated.

I am ready to mention this error during my presentation for transparency, but I feel a bit dumb.

EDIT: Using that validation set for early stopping or not hasn't changed the capability of our model to be robust on unseen data. So... is the error negligeable?",2,3
MachineLearning,t3_1cu8is4,1715960669.0,/r/MachineLearning/comments/1cu8is4/d_how_are_subspace_embeddings_different_from/,Discussion,[D] How are subspace embeddings different from basic dimensionality reduction?,"I have been struggling to understand how more basic dimensionality reduction techniques differ from more advanced methods, mainly in whether the same intuition about subspaces, manifolds, etc. extends to the more basic methods. I understand how things like PCA, t-SNE, UMAP, etc etc work (and these are 90% of what comes up when looking for dimensionality dimensionality reduction), but when I read about subspace clustering, manifold learning, or things in this area, they rarely mention these more basic dim reduc techniques and instead opt for more advanced methods and I'm not sure why, especially given how prolific PCA, t-SNE, and UMAP seem to be.

It is unclear to me whether/how things like PCA are different from say manifold learning, particularly in their usefulness for subspace clustering. I think the goals of both are to find some latent structure, with the intuition that working in the latent space will reduce noise, useless / low info features, reduce the curse of dimensionality, and also potentially more clearly show how the features and labels are connected in the latent space. In terms of the actual algorithms, I am understand the intuition but not whether they are ""real"". For instance, in the case of manifold learning (which, FWIW, I don't really see any papers about anymore and don't know why this is), a common example is the ""face manifold"" for images, that is a smooth surface of lower dims than the original input dimensions, and smoothly transitions from every face to another. This may be a little more trivial for images, but for general time series data, does this same intuition extend? 

For instance, if I have a dataset of time series caterpillar movement, can I arbitrarily say that there exists a manifold of catepillar size (bigger catepillars move slower) or a manifold of caterpillar ability (say, some kind of ability/skill manifold, if the caterpillars are completing a task/maze)? Very contrived example, but basically the question is if it is necessarily the case that I should be able to find a latent space based on what my priors tell me should exist / may hold latent structure (given enough data)?

I know Yann LeCun is a big proponent of working in latent spaces (more so with joint embeddings, which I am not sure whether that is applicable to me and my time series data), so I am trying to take my work more in that direction, but it seems like there's a big divide between basic PCA and basic nonlinear techniques (eg the ones you would see built into scipy or sklearn or whatever) and techniques that are used in some other papers. Do PCA (or basic nonlinear methods) and the like achieve the same thing but just not as well?",13,130
MachineLearning,t3_1cu71km,1715957148.0,/r/MachineLearning/comments/1cu71km/d_weights_for_dit_finetuned_on_publaylet_or/,Discussion,[D] Weights for DiT fine-tuned on PubLayLet or DocLayNet,"Hi all,

I'm looking for weights (preferably a training checkpoint) for an object detection model built with DiT fine tuned to recognize document layouts. Something like the picture I attached:



https://preview.redd.it/mculpv0l101d1.png?width=601&format=png&auto=webp&s=2db28aa8fa05c1480418f46956fce731fbba3a1f

Thanks to anyone who can help :)",1,4
MachineLearning,t3_1cu1ox0,1715940996.0,/r/MachineLearning/comments/1cu1ox0/r_what_is_the_main_way_of_using_ns3ai_to_develop/,Research,[R] What is the main way of using ns3-ai to develop DQN algorithms?,"Hello everyone,

I am currently researching the topic of using ML algorithms for Rate Adaptation (RAA) in wireless networks, specifically 802.11ax. My research objective is to demonstrate the capability and effectiveness of ML algorithms (in this case DQN) and how they are able to surpass conventional algorithms like Minstrel or Ideal in throughput.

To do this research, I am using Network Simulator 3 (NS3) as the main training and testing environment for the DQN algorithm. Then, to solve the problem of communicating between Python and NS3 (written in C++), I am using theÂ [ns3-ai](https://github.com/hust-diangroup/ns3-ai/tree/main)Â module, which utilizes a shared memory pool approach to this situation.

The main problem I am encountering is the lack of implementation code anywhere on the internet on this specific topic. I have gone through a few research papers like ""[ns3-ai: Rate Control for Wireless LAN by Deep Q-Network](https://www.ieice.org/publications/proceedings/bin/pdf_link.php?fname=06_SS1-6_(16).pdf&iconf=SISA&year=2022&vol=69&number=SS1-6&lang=E)"", ""[NS3-AI: Enable Applying Artificial Intelligence to Network Simulation in ns-3](https://www.researchgate.net/publication/341989036_ns3-ai_Fostering_Artificial_Intelligence_Algorithms_for_Networking_Research)"", and ""[Rate Adaptation with Q-Learning in CSMA/CA Wireless Networks](https://www.researchgate.net/publication/348620562_Rate_Adaptation_with_Q-Learning_in_CSMACA_Wireless_Networks)"". While they did give me valuable insights, they never go in depth about how exactly the code was written.

So, my questions is, how would I go upon using DQN in NS3 and ns3-ai? How would I observe the states and perform actions in NS3? Given that it is not the typical gymnasium environment where the environment kind of ""accepts"" actions.

Here is what I have tried.

* I have tried to understand/reverse-engineer the sample code provided
* I have read through their tutorial, but it only describes a simple scenario of sending data back and forth between Python and C++, disregarding the NS3 simulation environment.
* The creator says that there is skeleton code but I can't find it.

I would highly appreciate any help I get!",0,1
MachineLearning,t3_1cu0qgc,1715936862.0,/r/MachineLearning/comments/1cu0qgc/r_replace_the_object_in_the_frame_with_a_similar/,Research,[R] Replace the object in the frame with a similar object,"Hello, I am doing research for my project and I need your help.  
There is a closed pizza box on the table in a 5 sec part of a movie, I want to replace this pizza box with Pizza H\*t, \*ominos, etc. There are only a few Pizza H\*t boxes in my dataset, so the artificial intelligence model will need to use a technique similar to ""Few Shot Learn"". I thought I could do it using Generative Adversarial Networks (GAN) but I couldn't go further than turning a horse into a zebra. I have enough knowledge about AI and software development but new in GAN s.

Which model should I use, how should I research, which keywords should I use etc., where can i start to search for proper result I need your help, thanks.",4,3
MachineLearning,t3_1ctzxnh,1715933253.0,/r/MachineLearning/comments/1ctzxnh/d_correct_interpretation_of_modelpredict_output/,Discussion,[D] Correct interpretation of Model.predict output.,"Currently taking the FCC Machine Learning Course.

I dont know how to correctly interpret the probabilities of the Model.predict function output. The CNN is meant to determine whether is an image of a cat or a dog. Some probabilities are negative and very low. I dont know how to interpret that data.  
Also mention the model achieves its goal with an aceptable margin.  
Here an example:

https://preview.redd.it/yyss5v202y0d1.png?width=759&format=png&auto=webp&s=b1141a6e7150fcfdf0b65dc2119de956a9fc8ec8

Here the code.

Link of images:

    wget https://cdn.freecodecamp.org/project-data/cats-and-dogs/cats_and_dogs.zip

Code:

    # 3
    train_image_generator = ImageDataGenerator(rescale=1./255)
    validation_image_generator = ImageDataGenerator(rescale=1./255)
    test_image_generator = ImageDataGenerator(rescale=1./255)
    
    train_data_gen = train_image_generator.flow_from_directory(
    Â  Â  train_dir,
    Â  Â  target_size=(IMG_HEIGHT,IMG_WIDTH),
    Â  Â  batch_size = batch_size,
    Â  Â  class_mode = 'binary')
    val_data_gen = validation_image_generator.flow_from_directory(
    Â  Â  directory = validation_dir,
    Â  Â  target_size=(IMG_HEIGHT,IMG_WIDTH),
    Â  Â  batch_size = batch_size,
    Â  Â  class_mode = 'binary')
    test_data_gen = test_image_generator.flow_from_directory(
    Â  Â  directory=test_dir,
    Â  Â  target_size=(IMG_HEIGHT,IMG_WIDTH),
    Â  Â  batch_size = batch_size,
    Â  Â  class_mode = 'binary',
    Â  Â  shuffle=False)

    # 4
    def plotImages(images_arr, probabilities = False):
    Â  Â  fig, axes = plt.subplots(len(images_arr), 1, figsize=(5,len(images_arr) * 3))
    Â  Â  if probabilities is False:
    Â  Â  Â  for img, ax in zip( images_arr, axes):
    Â  Â  Â  Â  Â  ax.imshow(img)
    Â  Â  Â  Â  Â  ax.axis('off')
    Â  Â  else:
    Â  Â  Â  for img, probability, ax in zip( images_arr, probabilities, axes):
    Â  Â  Â  Â  Â  ax.imshow(img)
    Â  Â  Â  Â  Â  ax.axis('off')
    Â  Â  Â  Â  Â  if probability > 0.5:
    Â  Â  Â  Â  Â  Â  Â  ax.set_title(""%.2f"" % (probability*100) + ""% dog"")
    Â  Â  Â  Â  Â  else:
    Â  Â  Â  Â  Â  Â  Â  ax.set_title(""%.2f"" % ((1-probability)*100) + ""% cat"")
    Â  Â  plt.show()
    
    sample_training_images, _ = next(train_data_gen)
    plotImages(sample_training_images[:5])
    
    
    

    # 5
    train_image_generator = train_image_generator = ImageDataGenerator(
    Â  Â  rotation_range = 360,
    Â  Â  horizontal_flip = True,
    Â  Â  vertical_flip = True,
    Â  Â  zoom_range = 0.2,
    Â  Â  shear_range = 60,
    Â  Â  rescale=1./255)
    
    

    # 6
    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â directory=train_dir,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â target_size=(IMG_HEIGHT, IMG_WIDTH),
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â class_mode='binary')
    
    augmented_images = [train_data_gen[0][0][0] for i in range(5)]
    
    plotImages(augmented_images)

    # 7
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(2))
    
    model.compile(optimizer='adam',
    Â  Â  Â  Â  Â  Â  Â  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    Â  Â  Â  Â  Â  Â  Â  metrics=['accuracy'])
    model.summary()

    # 8
    history = model.fit(x = train_data_gen, 
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  epochs = epochs,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  validation_data = val_data_gen)
    acc = history.history['accuracy']
    print(acc)

    # 9
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    epochs_range = range(epochs)
    print(epochs_range)
    print(acc)
    
    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')
    
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

    #10
    probabilities = model.predict(test_data_gen)
    print(probabilities)
    probabilities = np.argmax(probabilities, axis = 1)
    sample_test_images, _ = next(test_data_gen)
    plotImages(sample_test_images, probabilities=probabilities)

    # 11
    answers = Â [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
    Â  Â  Â  Â  Â  Â  1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
    Â  Â  Â  Â  Â  Â  1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
    Â  Â  Â  Â  Â  Â  1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
    Â  Â  Â  Â  Â  Â  0, 0, 0, 0, 0, 0]
    
    correct = 0
    
    for probability, answer in zip(probabilities, answers):
    Â  if round(probability) == answer:
    Â  Â  correct +=1
    
    percentage_identified = (correct / len(answers)) * 100
    
    passed_challenge = percentage_identified >= 63
    
    print(f""Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs."")
    
    if passed_challenge:
    Â  print(""You passed the challenge!"")
    else:
    Â  print(""You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!"")
    ",6,0
MachineLearning,t3_1ctzd0a,1715930722.0,/r/MachineLearning/comments/1ctzd0a/p_real_time_emotion_classification_with_fer2013/,Project,[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!",9,10
MachineLearning,t3_1ctv9li,1715915200.0,/r/MachineLearning/comments/1ctv9li/d_real_chances_to_be_accepted_in_neurips_2024/,Discussion,[D] Real chances to be accepted in NeurIPS 2024 - Other conferences,"Hey!

This is my first time submitting to NeurIPS.

Does anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?

My topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.

This year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...

Thank you!",6,27
MachineLearning,t3_1cts99m,1715905657.0,/r/MachineLearning/comments/1cts99m/d_seminal_papers_list_since_2018_that_will_be/,Discussion,[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it. ",29,77
MachineLearning,t3_1ctqzfa,1715901848.0,/r/MachineLearning/comments/1ctqzfa/d_are_pytorch_highlevel_frameworks_worth_using/,Discussion,[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",57,92
MachineLearning,t3_1ctg48v,1715874619.0,/r/MachineLearning/comments/1ctg48v/d_fridays_oxenai_water_cooler_call/,Discussion,"[D] Friday's Oxen.AI Water Cooler call: High-performance audio processing, Python vs Rust","At this Friday's [Oxen.AI](http://Oxen.AI) Water Cooler,

*  the ""Show & Tell  /  Where are you stuck?  /  What is your project?"" segment topic will be:

**High-performance audio processing**

[Oxen.ai](http://Oxen.ai) discord member Shalini Ananda, PhD, [https://www.linkedin.com/in/shalinianandaphd/](https://www.linkedin.com/in/shalinianandaphd/)  
will discuss her experimentation with Python vs Rust with audio workloads.   Preview here:  
[https://discord.com/channels/1104137825638682806/1145920256301338685/1240029110726561823](https://discord.com/channels/1104137825638682806/1145920256301338685/1240029110726561823)

* Greg Schoeninger,  CEO of Oxen.ai , u/FallMindless3563 , will share highlights from this week's SW2 Conference and his session ""Better Data, Better AI""

To join the Ai Water Cooler call or Paper Club Zoom call at Friday 10:00 AM Pacific time, click hard on the 'subscribe' button:

[https://lu.ma/oxen](https://lu.ma/oxen)",2,5
MachineLearning,t3_1ctfwdv,1715874026.0,/r/MachineLearning/comments/1ctfwdv/r_energybased_hopfield_boosting_for/,Research,[R] Energy-based Hopfield Boosting for Out-of-Distribution Detection,"https://arxiv.org/abs/2405.08766

Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94 on CIFAR-100.",3,11
MachineLearning,t3_1ctegsx,1715870288.0,/r/MachineLearning/comments/1ctegsx/d_running_large_models_on_mac_for/,Discussion,[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?",9,0
MachineLearning,t3_1ctea1i,1715869784.0,/r/MachineLearning/comments/1ctea1i/d_lwhy_are_linear_rnns_so_performant_in_terms_of/,Discussion,"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but Iâ€™m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I canâ€™t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",42,78
MachineLearning,t3_1ctdz9d,1715868973.0,/r/MachineLearning/comments/1ctdz9d/d_icml_2024_travel_grants/,Discussion,[D] ICML 2024 travel grants?,"Hello everyone,

  
Has anyone got any update about ICML 2024 Financial Aid? I saw in X that applications will be open soon but haven't heard anything yet

[https://twitter.com/icmlconf/status/1787617481034481714](https://twitter.com/icmlconf/status/1787617481034481714)

Is there anyone who got the grant in the past? Are all students eligible for this?

thanks!",0,6
MachineLearning,t3_1ctayfy,1715859914.0,/r/MachineLearning/comments/1ctayfy/d_unveiling_milebench_benchmarking_mllms_in_long/,Discussion,[D] Unveiling MileBench: Benchmarking MLLMs in Long Contexts!,"Hey everyone!

I'm excited to share our latest work on **MileBench**, a new benchmark designed to evaluate the performance of Multimodal Large Language Models (MLLMs) in long-context tasks involving multiple images and lengthy texts.

**Title:** MileBench: Benchmarking MLLMs in Long Context

**Homepage:** [https://milebench.github.io/](https://milebench.github.io/)

**Paper:** [https://arxiv.org/abs/2404.18532](https://arxiv.org/abs/2404.18532)

**Code:** [https://github.com/MileBench/MileBench](https://github.com/MileBench/MileBench)

**Data:** [https://huggingface.co/datasets/FreedomIntelligence/MileBench](https://huggingface.co/datasets/FreedomIntelligence/MileBench)

# Why MileBench?

Existing benchmarks often overlook the complexity of tasks that involve multimodal long contexts. MileBench is the first to focus on these challenging scenarios, offering a more realistic assessment of MLLMs.

# Evaluation Types:

* Diagnostic Evaluation: Tests recall in long contexts with needle-in-a-haystack and image retrieval tasks.
* Realistic Evaluation: Simulates real-world scenarios with time-sequence and semantically related image tasks.

We collected **6,440** multimodal long-context samples from **21** existing or self-constructed datasets, with an average of **15.2** images and **422.3** words per sample. The table and figure show the detailed statistics of our datasets.

# Key Findings:

* Closed-source **GPT-4o** excelled in both diagnostic and realistic evaluations, but still short of perfect 100%.
* Most open-source MLLMs struggled with long-context tasks. Only Mantis and Qwen-VL-7B managed notable scores.

These results underscore that there are **""miles to go""** towards fully-realized long-context MLLMs.

# In-depth Analyses:

**Analysis 1: How do MLLMs perform with different context lengths?**

* Most models' performance drops as the number of images increases.
* Some models, like GPT-4o, perform better with a medium number of images.

**Analysis 2: Is there a ""Lost in the Middle"" Phenomenon in Long Contexts?**

* Strong long-text processing capabilities are crucial
* Qwen-VL-Chat showed some ""Lost in the Middle"" effects.

**Analysis 3: Combining Images Helps?**

To address input limitations, we combined multiple images into a single large image.

* Closed-source models performance dropped when combining images, except for Gemini 1.0
* High-resolution capabilities are crucial
* Some open-source models showed improved performance with combined images

# Want to Dive Deeper?

Our paper includes detailed experimental analyses, covering data contamination issues and task diversity. Check it out here: [https://arxiv.org/abs/2404.18532](https://arxiv.org/abs/2404.18532)

# Looking Ahead:

* Expanding MileBench to larger contexts and other modalities.
* Developing MLLMs that can efficiently handle complex, multimodal long-context tasks.

For more details, visit our project page: [https://milebench.github.io](https://milebench.github.io)

Letâ€™s discuss and explore how we can push the boundaries of MLLMs together!",0,11
MachineLearning,t3_1ctag4h,1715858085.0,/r/MachineLearning/comments/1ctag4h/d_whats_the_best_cloud_compute_service_for_hobby/,Discussion,[D] Whatâ€™s the best cloud compute service for hobby projects?,"Hi everyone!

Iâ€™m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks Iâ€™m not an expert in as a side project, but I donâ€™t have a GPU on my personal laptop, and Iâ€™d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didnâ€™t really like the fact that I had to store stuff on my google drive and Iâ€™d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, whatâ€™s a good cloud provider at a good cost for these sort of projects?",11,10
MachineLearning,t3_1ct9nth,1715854958.0,/r/MachineLearning/comments/1ct9nth/p_needle_in_a_needlestack_nian/,Project,[P] Needle in a Needlestack (NIAN),"**Code**: [https://github.com/llmonpy/needle-in-a-needlestack](https://github.com/llmonpy/needle-in-a-needlestack)

**Website**: [https://nian.llmonpy.ai/](https://nian.llmonpy.ai/)

**Description**:

>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.

>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.",6,28
MachineLearning,t3_1ct9li6,1715854702.0,/r/MachineLearning/comments/1ct9li6/d_neural_operators_deeponet_vs_fno/,Discussion,[D] Neural Operators | DeepONet vs. FNO |,"Hi all. I am recently getting started into Neural Operators and their application in PDE driven problems. It would be great if someone with experience could share how DeepONet and Fourier Neural Operator compare to each other. More specifically when it comes to their application in a huge spatio-temporal domain, efficiency and implementation in inverse problems. 

Cheers.",0,4
MachineLearning,t3_1ct8gxb,1715849687.0,/r/MachineLearning/comments/1ct8gxb/d_computer_vision_tooling_multistage_data/,Discussion,[D] Computer Vision Tooling - Multistage data processing,"At my line of work, I have to take a picture, detect/segment tens up to hundreds of points of interest and summarize it's sizes. 

I have ML model that is mostly precise but makes few stupid mistakes so it's not perfectly reliable (as expected) and occasionaly needs manual intervention that corrects it's output.

Currently, I use 

1) CVAT to upload an image, run prediction and correct/approve the results. Then I download the image and apply

2) Python script to run postprocessing

This workflow is good for few projects and few relatively-savy users but as time passes projects pile up and team grows. At the moment, there are several different tasks, each needs a bit different posprocessing and more and more people working with it.

**Do you know any software that can help me to implement this workflow without manually showeling data from CVATÂ to scripts?** 

I looked around if it's possible to extend CVAT but it's meant as annotation tool not a link in a production chain so I didn't found anything (appart plugging my own models into it). As an alternative I was thinking about writing my own solution. I would be able to write a backend but I cannot write the frontend part. I don't know javascript and searching through github for any decent frontend supporting tools (like brushes for segmentation) and label handling(fix mislabeled stuff etc)   led nowhere so I gave up thinking about it.",2,3
MachineLearning,t3_1ct7kdn,1715845540.0,/r/MachineLearning/comments/1ct7kdn/d_apriori_algorithm/,Discussion,[D] Apriori Algorithm ,Do anyone still use Apriori in production use cases? There must be better algorithms available. ,2,8
MachineLearning,t3_1ct68hm,1715839686.0,/r/MachineLearning/comments/1ct68hm/d_we_are_forming_a_study_group/,Discussion,[D] We are forming a study group! ,"
Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",26,0
MachineLearning,t3_1ct67r4,1715839598.0,/r/MachineLearning/comments/1ct67r4/d_confusion_what_does_the_yaxis_of_calibration/,Discussion,[D] Confusion: What does the y-axis of calibration curves represent?,"I was going through the paper [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599), and saw that the authors used the following definition for the ""fraction of positives"" which shows up on the y-axis of the calibration curve. 

https://preview.redd.it/pq9eqj16bq0d1.png?width=944&format=png&auto=webp&s=be71a70ff0e6ba77b672ca9b4315c6e7ba3d1011

From my understanding, the above equation is calculating the average accuracy in the bin m.

However, my original understanding about the ""fraction of positives"" was that it was the proportion of actual positive outcomes within the bin m, which intuitively makes more sense in the context of calibration curves. I have also seen this interpretation of calibration curves.

Can you fill in the hole in my knowledge?  ",1,3
MachineLearning,t3_1ct590c,1715835750.0,/r/MachineLearning/comments/1ct590c/r_fully_neuromorphic_vision_and_control_for/,Research,[R] Fully neuromorphic vision and control for autonomous drone flight,"Arxiv: https://arxiv.org/abs/2303.08778 (15 Mar 2023)  
https://www.science.org/doi/10.1126/scirobotics.adi0591 (15 May 2024)

Also they uploaded a number of videos a few hours ago:

[Supplementary Video 1](https://www.youtube.com/watch?v=NQUv7l56r1o)  
[Supplementary Video 2](https://www.youtube.com/watch?v=0xQU7WMR1Ys)  
[Supplementary Video 3](https://www.youtube.com/watch?v=UfKr1N8mu4c)  
[Supplementary Video 4](https://www.youtube.com/watch?v=hp8Rudld3sI)

Abstract:

> Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions because of the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present a fully neuromorphic vision-to-control pipeline for controlling a flying drone. Specifically, we trained a spiking neural network that accepts raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28,800 neurons, maps incoming raw events to ego-motion estimates and was trained with self-supervised learning on real event data. The control part consists of a single decoding layer and was learned with an evolutionary algorithm in a drone simulator. Robotic experiments show a successful sim-to-real transfer of the fully learned neuromorphic pipeline. The drone could accurately control its ego-motion, allowing for hovering, landing, and maneuvering sidewaysâ€”even while yawing at the same time. The neuromorphic pipeline runs on board on Intelâ€™s Loihi neuromorphic processor with an execution frequency of 200 hertz, consuming 0.94 watt of idle power and a mere additional 7 to 12 milliwatts when running the network. These results illustrate the potential of neuromorphic sensing and processing for enabling insect-sized intelligent robots.

They have some other cool papers:

[Lightweight Event-based Optical Flow Estimation via Iterative Deblurring](https://arxiv.org/abs/2211.13726) and [Video](https://www.youtube.com/watch?v=1qA1hONS4Sw)",3,31
MachineLearning,t3_1ct4x48,1715834558.0,/r/MachineLearning/comments/1ct4x48/nanogpt_alternative_d/,Discussion,Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks",11,5
MachineLearning,t3_1ct45hk,1715831821.0,/r/MachineLearning/comments/1ct45hk/d_whats_up_with_papers_without_code/,Discussion,[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",73,227
MachineLearning,t3_1ct3knc,1715829816.0,/r/MachineLearning/comments/1ct3knc/p_jaxsplat_3d_gaussian_splatting_for_jax/,Project,[P] jaxsplat: 3D Gaussian Splatting for JAX,"I created jaxsplat which provides CUDA-accelerated 3D Gaussian Splatting for JAX.
The original INRIA code and gsplat's implementation contain dynamically shaped arrays unsuitable for usage with JAX.
Instead, I modified gsplat's CUDA implementation to expose custom XLA CUDA calls while not leaking any dynamic shapes into JAX-side code.

Take a look if you're interested in exploring 3D Gaussian Splatting with JAX:

GitHub: https://github.com/yklcs/jaxsplat

Docs: https://jaxsplat.readthedocs.io",5,34
MachineLearning,t3_1ct06gs,1715819170.0,/r/MachineLearning/comments/1ct06gs/tips_for_improving_my_vae_project/,Project,Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!",20,15
MachineLearning,t3_1csv8b6,1715805699.0,/r/MachineLearning/comments/1csv8b6/does_every_tts_tool_need_reference_voice_along/,Discussion,Does every TTS tool need reference voice along with model to run ? [D],"Does every TTS tool need reference voice along with model to run ?

does every text to speech tool need reference voice along with model to work  
even if you have model for that voice u still  need the reference voice of it   or just the model is enough ? ",4,0
MachineLearning,t3_1csqur3,1715794991.0,/r/MachineLearning/comments/1csqur3/d_acl_2024_decisions/,Discussion,[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? ğŸ‡¹ğŸ‡­ğŸ˜,47,32
MachineLearning,t3_1csp40j,1715790666.0,/r/MachineLearning/comments/1csp40j/p_new_kans_paper_just_dropped_kolmogorovarnold/,Project,[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,37,64
MachineLearning,t3_1csnxu9,1715787729.0,/r/MachineLearning/comments/1csnxu9/d_a_possible_solution_for_the_surging_submissions/,Discussion,[D] A possible solution for the surging submissions in AI/ML top confs,"Just heard that Nips has just received over 16k submissions, which is really concerning me. Such abonormal explosion in paper number is very likely to cause a largely degraded average quality of each paper, and demand more reviewers for paper review, which also could largely worsen the average review quality. Both factor would ultimately defame the conf and the paper accepted by the conf.

A straightfoward solution that came to me is to fine-tune an LLM with high-quality reviews on both good and bad papers,  giving it expertise to filter out low-quality papers, and implement a pre-review (some confs seem to arleady have that) to lower the demand for expert reviewers.

Cons are the training cost/ethics/security/accuracy, etc.

Any thoughts?",5,0
MachineLearning,t3_1csmgyd,1715784046.0,/r/MachineLearning/comments/1csmgyd/d_audio_tokenizers/,Discussion,[D] Audio Tokenizers,"The recent GPT-4O model got me thinking whether they actually tokenized the audio and trained their GPT on text + audio tokens. Are there any successful audio tokenizers that seem to work well with auto regressive models? People have used VQ-VAE\[1\] for learning discrete representation of audio samples but the encoder and decoder of such VQ-VAE uses covnets applied over Mel-Spectrogram which I think in practice cannot enable audio streaming (As it applied 1d and 2d covnets over the entire audio signal and also doing this makes the representations non casual)

\[1\] - [https://arxiv.org/pdf/1711.00937](https://arxiv.org/pdf/1711.00937)

  
Edit:

A more general question I have is that is this method of tokenizing audio even feasible(will it even work?) or it's better to incrementally sample from the audio and proj each sample to an embedding and then pre train the GPT on those embeddings instead of the embeddings learned from tokens?",3,2
MachineLearning,t3_1cslzii,1715782823.0,/r/MachineLearning/comments/1cslzii/r_classification_tasks_with_llms/,Research,[R] Classification tasks with llms,"  
Hey! as part of my thesis I have seen good results on using AutoModelForSequenceClassification for large texts with llm's (mistral) instead of bert but I have some questions I am trying to figure out:

1. is there a point for having a coherent, understandable prompt with task definition and targets while using AutoModelForSequenceClassification?
2. I have seen an improvement for the model using cot (chain of thought) prompting, when I asked him to: ""write out your reasoning step-by-step to be sure you get the right answers!"", is there any explanation why it does help? we are using a classification head instead of the decoder one so it's not auto regressive.",2,0
MachineLearning,t3_1cslctv,1715781170.0,/r/MachineLearning/comments/1cslctv/project_cant_seem_to_find_good_datasets_or_apis/,Project,[Project] Canâ€™t seem to find good datasets or APIâ€™s with European Flight information to create ML model,"Hi guys! 
Iâ€™m currently enrolled on a data science project where we have to use a ML model to predict flight prices. 
Im struggling to find good datasets or API that have European flight historical information, Iâ€™ve come across mostly Indian airlines datasets. And Iâ€™m struggling to use Sky Scanner API to gather data to create my dataset.
Any advice on how can I overcome this?",1,0
MachineLearning,t3_1cskok4,1715779328.0,/r/MachineLearning/comments/1cskok4/p_multivariate_traffic_flow_predictions/,Project,[P] Multivariate Traffic Flow Predictions,"Hi, i just started a new project, i have this traffic datasets with 5 Minutes time intervals , and flow and speed, now i tried using prediction models like Lstm and gru successfully with flow or speed but when i try to use the model on both speed and flow it doesnâ€™t work. Any inputs where can i learn this stuff ?",5,6
MachineLearning,t3_1csh8h2,1715767839.0,/r/MachineLearning/comments/1csh8h2/d_research_on_diffusion_models_with_focus_on/,Discussion,[D] Research on diffusion models with focus on efficiency and reducing computation cost,"I need help with the specific keywords,  notable research or surveys  on the mentioned topic.  I have only tried with keywords such as: ""edge device"", ""edge computing"", ""diffusion models"",  on google scholar and the results return are less than satisfactory.  Can someone give me some suggestion on where to look at ?",2,2
MachineLearning,t3_1csh3tv,1715767325.0,/r/MachineLearning/comments/1csh3tv/discussion_what_are_sota_uncertainty/,Discussion,[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",29,57
MachineLearning,t3_1csgx30,1715766589.0,/r/MachineLearning/comments/1csgx30/r_llm4ed_large_language_models_for_automatic/,Research,[R] LLM4ED: Large Language Models for Automatic Equation Discovery,"**Paper**: [https://arxiv.org/abs/2405.07761](https://arxiv.org/abs/2405.07761)

**Abstract**:

>Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.",2,18
MachineLearning,t3_1csgr7r,1715765879.0,/r/MachineLearning/comments/1csgr7r/r_the_platonic_representation_hypothesis/,Research,[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning. ",26,61
MachineLearning,t3_1csf3nf,1715758339.0,/r/MachineLearning/comments/1csf3nf/p_tips_on_training_a_transformer_model/,Project,[P] Tips on training a Transformer model,"Hello everyone,  
I am attempting to train a hybrid Resnet18 encoder-6 layer 520 dmodel Transformer decoder to do full page handwriting recognition and I am struggling to properly train it. The main issues/questions I am stuck at are:

* I have a dataset that has around 6000 samples of pages of handwriting distributed 80% training/20% validation, how would I know if this is enough to work with?
* I am currently using gradient accumulation since each sample in the dataset is quite large, as you would imagine. It proved to be beneficial, as the behaviour of the model during training is less fuzzy than the past training attempts using a fixed batch size of 2
* I am currently exploring learning rate schedulers and have used, for now, a reduce on plateau on validation loss with a learning rate starting at 1e-4 with 0.75 factor. What is the best way to approach and choose lr schedulers

Even though the place where I am at seems to suggest that I am heading in the right direction (WER is slowly decreasing across epochs), the training time is very slow (12hrs for around 3-5 epochs) on available Kaggle GPUs (P100). Any tips for going forward?",2,0
MachineLearning,t3_1cseqlo,1715756748.0,/r/MachineLearning/comments/1cseqlo/d_video_analysis_tools_for_detecting_cheating_in/,Discussion,[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.",14,0
MachineLearning,t3_1csegmq,1715755569.0,/r/MachineLearning/comments/1csegmq/training_a_2gb_of_image_dataset_on_colab_d/,Discussion,Training a 2gb of image dataset on Colab [D],"Can we train 2gb image dataset on Colab free version?

or are we going to run out of memory ?",0,0
MachineLearning,t3_1csdsje,1715752741.0,/r/MachineLearning/comments/1csdsje/d_those_in_the_industry_how_are_you_using_open/,Discussion,"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",37,55
MachineLearning,t3_1cs8fzb,1715734947.0,/r/MachineLearning/comments/1cs8fzb/r_matryoshka_representation_learning_mrl_for_clip/,Research,[R]  Matryoshka representation learning (MRL) for CLIP (& SigLip) ,"MRL \[1\] for CLIP allows smaller dimension embeddings to be used without loss in fidelity. Training is modified to optimize for truncated embeddings (multiple target dimensions at once) across both vision and text encoders.  

  
Key findings:

* Reducing embeddings size by 4x retains \~95 performance
* Projection layers for sub-embeddings did not help performance
* Works in and out (zero-shot) of domain on multi-modal retrieval
* Using too many sub-embeddings degrades performance (i.e. {512, 256, 128} vs {512, 256, 128, 64, 32, 16, 8}
* The number of sub-embeddings impacts convergence (same as above)
* Works with rank-tuning methods like GCL
* Relative importance (weighting, wi) of sub-dimensions matters (e.g. w1\*L\_512 + w2\*L\_256 + w3\*L\_128)
* MRL trained models can improve if the original sized embedding is used. i.e. performance is improved even if the smaller embeddings are not used.

Article:  
[https://www.marqo.ai/blog/matryoshka-representation-learning-with-clip-for-multimodal-retrieval-and-ranking](https://www.marqo.ai/blog/matryoshka-representation-learning-with-clip-for-multimodal-retrieval-and-ranking)

\[1\] MRL [https://arxiv.org/abs/2205.13147](https://arxiv.org/abs/2205.13147)",1,13
MachineLearning,t3_1cs6p6j,1715729943.0,/r/MachineLearning/comments/1cs6p6j/d_any_reason_not_to_submit_to_neurips/,Discussion,[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.",9,9
MachineLearning,t3_1cs4v54,1715725024.0,/r/MachineLearning/comments/1cs4v54/r_on_the_efficiency_of_convolutional_neural/,Research,[R] On the Efficiency of Convolutional Neural Networks,"Last month, I published the paper [On the Efficiency of Convolutional Neural Networks](https://arxiv.org/pdf/2404.03617) on Arxiv. It is the result of five years of research and development, and I hope you find it useful for understanding the performance of deep neural networks. At 52 pages, the paper may appear daunting, but I think you will find it readable. The figures tell the story of the paper; simply scanning the figures and captions is a good approach, then drill down into the sections you find interesting.

Although many have noticed the disconnect between model efficiency (accuracy versus FLOPs) and actual efficiency (accuracy versus latency), previous attempts to improve actual efficiency have been unsatisfying. The convnets we use today were designed to circumvent the limitations of existing inference engines. They are not the models that would perform best with optimized software.

In fact, previous works do not compare model efficiency and actual efficiency directly. The prevailing view is that these two measurements are irreconcilable, and that counting the number of model FLOPs has limited usefulness.

I set out to create a solid foundation for model efficiency research by addressing these shortcomings. The first contribution is the ""Efficiency Gap"" plot, which shows model and actual efficiency in the same graph. We divide the model FLOPs by the processor's peak arithmetic throughput, yielding the ""ideal latency"" of the model. Ideal latency is the time that model inference would take if the software ran with 100% computational efficiency (i.e. 100% arithmetic utilization).

We plot ideal and actual latency as two curves on the same accuracy versus latency graph. We use log-scale on the latency axis, and the horizontal gap between the two curves equals the logarithm of the computational efficiency (i.e., the ratio between ideal and actual latency).

Efficiency gap plots of well known baseline models give us a precise analytical view of model performance. For example, the efficiency gap plots for EfficientNet show that its ideal latency is much lower than its actual latency, a result of impressive model efficiency and poor computational efficiency.

The plots also show us where modern convnets fall short. ConvNeXt has worse model efficiency than EfficientNet, yet still has lower actual latency because of better computational efficiency. This suggests that we could do better by finding a model that has low ideal latency and that affords high computational efficiency.

The next chapter of the paper creates a new performance model for neural networks to better understand the sources of computational efficiency. We correct the roofline model, which was never meant to be applied to a sequence of parallel kernels. Previous works overestimate the operational intensity of neural networks  by computing a single op: byte ratio for the entire application, despite the fact that this measurement only has meaning for operations that run in parallel. Our ""Waterline Model"" computes the minimum attainable latency for each kernel separately, ensuring an accurate performance estimate for a sequence of parallel kernels.

Waterline analysis shows that the performance of modern convnets suffer because the traditional conv2d layer has been replaced with a sequence of degenerate conv2d layers, each having less operational intensity than the original conv2d. Thus layer-by-layer execution of modern convnets is memory bound for many of the individual layers.

The intuition behind models like ConvNeXt that increased performance by making the models wider helps somewhat, but does not solve the computational efficiency problem. Also, these adaptations caused the model to use more operations to achieve the same accuracy.

In the next chapter, we address the computational efficiency head-on by writing GPU kernels that fuse all the degenerate layers of a convnet block into a single kernel. We also replace depth-wise convolutions with grouped convolutions with group-width equal to eight to fully utilize the NVIDIA tensor core 16m x 8n x 8k matrix multiply instruction. Block fusion replaces a sequence of low operational intensity, degenrate conv2d layers with a single, high operational intensity kernel. Block fusion algorithms create temporal locality, reduce workspace size, and avoid communication to DRAM.

Our block-fusion kernels run 3x to 14x faster than PyTorch Inductor kernels on an NVIDIA Ampere A5000 GPU with float16 tensor core arithmetic. We implemented MBConv with Squeeze & Excitation (MBConv+SE), the block which created the impressive model efficiency of EfficientNet. We also implemented a new block, ConvFirst, which is very similar to ConvNeXt, but better suited for tensor-core arithmetic.

(An aside: I created the ConvFirst block in 2019, but did not publish it until now due to engineering workload. ConvNeXt was discovered independently).

In the final chapter, we created a new convnet called ConvFirstNet using ConvFirst blocks in the early stages and MBConv+SE block in the late stages. This separation between light-weight blocks for early vision and heavy blocks for late vision was used in EfficientNetV2. Our ConvFirst block is a light-weight alternative to the FusedMBConvBlock used by EfficientNetV2, producing better model efficiency.

The result is that ConvFirstNet has better model efficiency than EfficientNet and better computational efficiency than ConvNeXt. Using our kernels, ConvFirstNet runs 4x as fast as ConvNeXt using PyTorch Inductor. In fact, the actual latency of ConvFirstNet is less than ideal latency of ConvNeXt.

I hope that this work contributes to the progress of model efficiency research. I look forward to all of the wonderful applications of efficient convnets.",1,1
MachineLearning,t3_1cs132k,1715715596.0,/r/MachineLearning/comments/1cs132k/d_optical_flow_for_video_classification/,Discussion,[D] Optical Flow For Video Classification,Can anyone help me with the Optical flow for video classification. For eg. Human Activity Classification. I didnt find any tutorials from scratch for code all I just found were papers. ,1,1
MachineLearning,t3_1cs0smx,1715714868.0,/r/MachineLearning/comments/1cs0smx/discussion_event_sequence_order_prediction/,Discussion,[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:",9,1
MachineLearning,t3_1cs0isb,1715714185.0,/r/MachineLearning/comments/1cs0isb/d_lightzero_custom_environment/,Discussion,[D] LightZero custom environment,"I want the create a custom environment and do benchmarks on it using varius MCTS algorithms from LightZero [https://github.com/opendilab/LightZero](https://github.com/opendilab/LightZero) . However, I find the implementation confusing and complicated, does somenone have a clean extension of this algorithm that defines its own environment and runs varius algorithms on it ?

",0,1
MachineLearning,t3_1crzj4o,1715711769.0,/r/MachineLearning/comments/1crzj4o/d_kolmogorov_arnold_networks_a_visual_paper/,Discussion,[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)",24,39
MachineLearning,t3_1crzdhd,1715711386.0,/r/MachineLearning/comments/1crzdhd/d_gpt4o_natively_multimodal_what_does_this/,Discussion,"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",44,147
MachineLearning,t3_1crxhfp,1715706812.0,/r/MachineLearning/comments/1crxhfp/d_is_bert_still_relevant_in_2024_for_an_emnlp/,Discussion,[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",24,55
MachineLearning,t3_1crwh0q,1715704345.0,/r/MachineLearning/comments/1crwh0q/r_building_an_observable_arxiv_rag_chatbot_with/,Research,"[R] Building an Observable arXiv RAG Chatbot with LangChain, Chainlit, and Literal AI","HeyÂ r/MachineLearning, I published a new article where I built an observable semantic research paper application.

This is an extensive tutorial where I go in detail about:

1. Developing a RAG pipeline to process and retrieve the most relevant PDF documents from the arXiv API.
2. Developing a Chainlit driven web app with a Copilot for online paper retrieval.
3. Enhancing the app with LLM observability features from Literal AI.

You can read the article here:Â [https://medium.com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8](https://medium.com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8)

Code for the tutorial:Â [https://github.com/tahreemrasul/semantic\_research\_engine](https://github.com/tahreemrasul/semantic_research_engine)

",0,3
MachineLearning,t3_1crt8ax,1715696110.0,/r/MachineLearning/comments/1crt8ax/r_embedding_learning_new_idea_for_calculating/,Research,[R] Embedding Learning: New idea for calculating ideal margin penaltys,"Hi everyone,
i was experimenting with facial recognition, during my masters thesis and therefore was learning embeddings (using triplet loss, ArcFace, AdaCos... ect.). Intention was creating an efficient (and non gdpr violating) face unlock.

The ArcFace method appears to be the SOTA still. Works like AdaCos have tried eliminating the annoying hyperparameters by eliminating the margin and dynamicly adapting the scale during training, though in reality this doesn't seem to work as well as ArcFace when optimally tuned.

I subsequently came up with a different idea of adapting the margin during training instead of completely eliminating it, and in my tests it seemed to work very well, better than AdaCos and somewhere between equally well and slightly better than ArcFace. I would love to hear if someone could validate my findings, here is a pytorch implementation and explaination of the method:
https://github.com/VBambi/AdaAcos-the-self-adjusting-implementaion-of-ArcFace",0,7
MachineLearning,t3_1crromv,1715692000.0,/r/MachineLearning/comments/1crromv/p_seeking_advice_on_retrievalaugmented/,Project,[P] Seeking advice on retrieval-augmented classification for seasonal prediction tasks,"I'm working on a project to train a binary multi-modal classifier for predicting political content. Since political content tends to have seasonal trends, I want to use a retrieval-augmented classification setting. This way, whenever a new trend emerges, I can incorporate new features into my retrieval dataset and improve the model's precision. Additionally, I'd like the ability to override the model's decisions based on high similarity in the retrieval dataset. Can anyone recommend relevant papers or techniques for this approach? Any guidance or resources would be greatly appreciated!",0,1
MachineLearning,t3_1crr0fa,1715689986.0,/r/MachineLearning/comments/1crr0fa/d_how_do_you_get_better_at_reading_proof_in_the/,Discussion,"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",35,58
MachineLearning,t3_1crqdzt,1715688048.0,/r/MachineLearning/comments/1crqdzt/p_time_series_forecasting/,Project,[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?  ",13,1
MachineLearning,t3_1crojwd,1715681359.0,/r/MachineLearning/comments/1crojwd/d_the_usefulness_of_the_last_linear_layer_of_each/,Discussion,[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.",22,35
MachineLearning,t3_1crno38,1715677647.0,/r/MachineLearning/comments/1crno38/r_how_well_can_transformers_emulate_incontext/,Research,[R] How Well Can Transformers Emulate In-context Newton's Method?,"**Paper**: [https://arxiv.org/abs/2403.03183](https://arxiv.org/abs/2403.03183)

**Code**: [https://anonymous.4open.science/r/transformer\_higher\_order-B80B/](https://anonymous.4open.science/r/transformer_higher_order-B80B/)

**Abstract**:

>Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve Ïµ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent.",1,4
MachineLearning,t3_1crnika,1715677005.0,/r/MachineLearning/comments/1crnika/p_a_dataset_for_the_global_artificial/,Project,[P] A Dataset for The Global Artificial Intelligence Championship Math 2024,"**Dataset and code**: [https://github.com/protagolabs/odyssey-math](https://github.com/protagolabs/odyssey-math)

**AGI Odyssey**: [https://www.agiodyssey.org](https://www.agiodyssey.org)

**Description**:

>The Global Artificial Intelligence Championship(GAIC) Math 2024 presents a collection of 387 meticulously crafted math problems, meticulously curated by professional math problem writers from both universities and high schools. The compilation includes high school competition questions with 148 problems, followed by a series of 138 high school mathematics questions, and concluding with 101 university-level mathematics questions.

>The GAIC Math 2024 problem setters are composed of mathematics professors hailing from esteemed institutions such as Arizona State University, Johns Hopkins University, Drexel University, National University of Singapore, Tsinghua University, and Central China Normal University. These professors were formally invited by AGI Odyssey to contribute their expertise to the competition. The problem setter committee aligned with the mission of AGI Odyssey, which aims to advance innovative research in artificial general intelligence (AGI) and foster interdisciplinary collaboration, and ensure that AGI development benefits humanity as a whole. To maintain the integrity and fairness of the competition, the problem setter committee ensured that all problems were original and kept confidential. Responsibilities of the problem setter committee included problem generation, review, formatting, testing, and revisions for GAIC Math 2024.

>A new dataset of 387 questions and solutions from high school competition questions, high school mathematics questions, and university-level mathematics questions.

https://preview.redd.it/2alzha4ewc0d1.jpg?width=1193&format=pjpg&auto=webp&s=fa9735ddce1aea5f44b6a06d1fe2e4908526c80b",0,8
MachineLearning,t3_1crn6iw,1715675504.0,/r/MachineLearning/comments/1crn6iw/d_language_model_for_timeseries_forecasting_from/,Discussion,[D] Language model for TimeSeries Forecasting from Amazon,"Time series forecasting is super important for many industries, like retail, energy, finance, etc.Â 

I delivered many projects in this area with statistical models, deep learning models (LSTM, CNN) and always it was a challenge.Â 

With a great development in language model space I was thinking how LLM architecture could be used for forecasting and while I was exploring this idea I found that Amazon already delivered multiple **pretrained time series forecasting models** based on language model architectures.Â 



If you are interesting check following resources:Â 

[https://github.com/amazon-science/chronos-forecasting](https://github.com/amazon-science/chronos-forecasting)

[https://www.amazon.science/blog/adapting-language-model-architectures-for-time-series-forecasting](https://www.amazon.science/blog/adapting-language-model-architectures-for-time-series-forecasting)



What do you think, will a such models make a forecasting more accurate? Â ",1,5
MachineLearning,t3_1crm54y,1715670879.0,/r/MachineLearning/comments/1crm54y/d_have_someone_tried_to_implement_kans_from/,Discussion,[D] Have someone tried to implement KANs from scratch?,"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. 

Since many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.

If you still don't know about it, you could take help of following resources ğŸ‘‡ğŸ»

Here is the research paper:
https://arxiv.org/abs/2404.19756

And the explanation video of this paper:
https://youtu.be/-PFIkkwWdnM

And if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments. ",7,17
MachineLearning,t3_1crih6f,1715657054.0,/r/MachineLearning/comments/1crih6f/r_memory_mosaics/,Research,[R] Memory Mosaics,"""[Memory Mosaics](https://arxiv.org/pdf/2405.06394) are networks of associative memories working in  concert to achieve a prediction task of interest. Like transformers,  memory mosaics possess compositional capabilities and in-context  learning capabilities. Unlike transformers, memory mosaics achieve these  capabilities in comparatively transparent ways. We demonstrate these  capabilities on toy examples and we also show that memory mosaics  perform as well or better than transformers on medium-scale language  modeling tasks.""  
\-----   

This paper takes an approach that combines elements of [Augmenting Self-Attention with Persistent Memory](https://arxiv.org/abs/1907.01470) and [Hopfield Networks is All You Need](https://arxiv.org/abs/2008.02217)  ; the resulting block seems to have similar capabilities to a  transformer block at parameter parity but (interestingly to me at least)  can function as a self-contained induction head.   ",0,1
MachineLearning,t3_1cri6h6,1715656093.0,/r/MachineLearning/comments/1cri6h6/d_full_causal_selfattention_layer_in_onlogn/,Discussion,"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",41,107
MachineLearning,t3_1crh21y,1715652616.0,/r/MachineLearning/comments/1crh21y/discussion_miccai_2024_decisions/,Discussion,[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",21,14
MachineLearning,t3_1crgu6y,1715651968.0,/r/MachineLearning/comments/1crgu6y/whats_your_favorite_paper_at_iclr2024_d/,Discussion,What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,9,50
MachineLearning,t3_1crahli,1715634440.0,/r/MachineLearning/comments/1crahli/d_neurips_2024_submissions/,Discussion,[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year itâ€™s not the case! 7000 submissions already?!",24,38
MachineLearning,t3_1cra98l,1715633879.0,/r/MachineLearning/comments/1cra98l/d_lora_with_cross_validation/,Discussion,[D] LoRA with Cross Validation,Is there a way to do k-fold cross validation with low rank adaptation? Iâ€™m not sure how to implement and evaluate with the PEFT library.,0,4
MachineLearning,t3_1cra5mc,1715633629.0,/r/MachineLearning/comments/1cra5mc/d_best_performing_light_weight_qa_llm_in_english/,Discussion,[D] Best performing light weight Q&A LLM in English,I am looking for the SOTA light weight answer generating open source LLM from context (disorganized multiple paragraphs) and question in English in HuggingFace. Can anyone suggest any from HuggingFace. The best performing are seems like eating up all the storage even the sharded versions. I am looking for something whichs model/weight file is around 20GB in total.,0,0
MachineLearning,t3_1cra384,1715633458.0,/r/MachineLearning/comments/1cra384/d_moving_my_threshold_using_few_shot_examples/,Discussion,[D] Moving my threshold using few shot examples,"I have a BERT based classifier and have decided that I want a different threshold for my modelâ€™s decision boundary. I have a only a few (dozen) examples of labels that exemplify this new threshold. It seems to me shifting the last layer predictions to this new decision boundary without gradient training should be easy and wouldnâ€™t need many examples.
Any ideas on how to implement this?
",3,3
MachineLearning,t3_1cr8swg,1715630358.0,/r/MachineLearning/comments/1cr8swg/d_data_labeling_tools/,Discussion,[D] Data Labeling Tools,"What are some of your favorite data labeling tools? I know of the following:

https://github.com/cleanlab/cleanlab
This is for noisy labels  

https://github.com/voxel51/fiftyone
This one is an image search engine

But would like to know what everyone else is using",3,5
MachineLearning,t3_1cr78zr,1715626655.0,/r/MachineLearning/comments/1cr78zr/d_time_series_forecasting_with_extremely_limited/,Discussion,[D] Time series forecasting with extremely limited amount of data,"Hey everyone,

I am looking for some suggestions to work on this task. I have a few time series with only 30/40 observations and of course we all agree this is a really limited amount data. 
I want to forecast some financial metrics and I have only these few observations because data were collected on monthly basis.

Do you have any suggestions? Of course I must try with a simple regression as first, but it would be highly appreciated if you know some other methods that I may try.
I read something related to few shot learning, but it seems to me that many applications use LSTM o other neural networks and I think that although they're thought to address these kind of problems, all the papers I've read so far use series with 100/120 observations and I don't know if it might work for me.

Thanks for sharing your knowledge ğŸ™‚",2,5
MachineLearning,t3_1cr6wm4,1715625822.0,/r/MachineLearning/comments/1cr6wm4/n_padri_tts_plan_ahead_dont_rush_it_texttospeech/,News,"[N] PADRI TTS â€” 'Plan Ahead, Don't Rush It' Text-to-Speech","Blog: [https://picovoice.ai/blog/orca-true-streaming-tts/](https://picovoice.ai/blog/orca-true-streaming-tts/)

Doc: [https://picovoice.ai/docs/orca/](https://picovoice.ai/docs/orca/)

GitHub: [https://github.com/Picovoice/orca/](https://github.com/Picovoice/orca/)

https://i.redd.it/afefdw5eo80d1.gif

  
",2,2
MachineLearning,t3_1cr5lv8,1715622696.0,/r/MachineLearning/comments/1cr5lv8/n_gpt4o/,News,[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web ",161,210
MachineLearning,t3_1cr5llt,1715622677.0,/r/MachineLearning/comments/1cr5llt/d_what_python_package_do_you_prefer_for_classical/,Discussion,[D] What Python package do you prefer for classical diffusion maps and why?,"
Iâ€™m trying to decide between using pydiffmap https://github.com/DiffusionMapsAcademics/pyDiffMap/tree/master and mapalign https://github.com/satra/mapalign/tree/master

Have you used either? If so, which do you prefer and why? 

Thereâ€™s a similar user base for each package. 

Im mainly interested in classical diffusion maps over diffusion pseudotime. 
",0,3
MachineLearning,t3_1cr1dn7,1715612277.0,/r/MachineLearning/comments/1cr1dn7/ml_feature_compression_d/,Discussion,ML Feature Compression [D],"Hey All,

We know that feature reduction/Compression can be used via AutoEncoders, SVD, PCA, etc. 

- Are there any methods that anyone can think of other than these that have worked for them?
- When using feature reduction, are there any techniques/gotchaâ€™s that youâ€™ve learned over the years that youâ€™d want to share? ",6,9
MachineLearning,t3_1cr14ki,1715611608.0,/r/MachineLearning/comments/1cr14ki/d_time_series_anomaly_detection_with_diffusion/,Discussion,[D] Time series Anomaly detection with diffusion models ,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.",9,4
MachineLearning,t3_1cqzkh8,1715607608.0,/r/MachineLearning/comments/1cqzkh8/d_looking_for_research_on_point_cloud/,Discussion,[D] Looking for Research on Point Cloud Understanding in Remote Sensing," 

Hi everyone,

I'm interested in learning more about research applying point cloud understanding techniques (like classification and segmentation and etc.) to remote sensing data.

**Are there any recent papers you'd recommend that explore this field?**

any area: forestry, urban environments, disaster response....

&#x200B;",0,3
MachineLearning,t3_1cqv5y4,1715592805.0,/r/MachineLearning/comments/1cqv5y4/r_our_new_classification_algorithm_outperforms/,Research,"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time
","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:Â [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",53,229
MachineLearning,t3_1cqq1in,1715572047.0,/r/MachineLearning/comments/1cqq1in/d_thoughts_on_dspy/,Discussion,[D] Thoughts on DSPy,"I have been tinkering with DSPy and thought I will share my 2 cents here for anyone who is planning to explore it:

The core idea behind DSPy are two things:

1.	â Separate programming from prompting
2.	â incorporate some of the best practice prompting techniques under the hood and expose it as a â€œsignatureâ€

Imagine working on a RAG. Today, the typical approach is to write some retrieval and pass the results to a language model for natural language generation. But, after the first pass, you realize itâ€™s not perfect and you need to iterate and improve it. Typically, there are 2 levers to pull:

1.	â Document Chunking, insertion and Retrieval strategy
2.	â Language model settings and prompt engineering

Now, you try a few things, maybe document the performance in a google sheet, iterate and arrive at an ideal set of variables that gives max accuracy.

Now, letâ€™s say after a month, model upgrades, and all of a sudden the accuracy of your RAG regresses. Again you are back to square one, cos you donâ€™t know what to optimize now - retrieval or model? You see what the problem is with this approach? This is a very open ended, monolithic, brittle and unstructured way to optimize and build language model based applications.

This is precisely the problem DSPy is trying to solve. Whatever you can achieve with DSPy can be achieved with native prompt engineering and program composition techniques but it is purely dependent on the programmers skill. But DSPy provides native constructs which anyone can learn and use for trying different techniques in a systematic manner.

DSPy the concept:

Separate prompting from programming and signatures

DSPy does not do any magic with the language model. It just uses a bunch of prompt templates behind the scenes and exposes them as signatures. Ex: when you write a signature like â€˜context, question -> answerâ€™, DSPy adds a typical RAG prompt before it makes the call to the LLM. But DSPy also gives you nice features like module settings, assertion based backtracking and automatic prompt optimization.

Basically, you can do something like this with DSPy,

â€œGiven a context and question, answer the following question. Make sure the answer is only â€œyesâ€ or â€œnoâ€â€. If the language model responds with anything else, traditionally we prompt engineer our way to fix it. In DSPy, you can assert the answer for â€œyesâ€ or â€œnoâ€ and if the assertion fails, DSPy will backtrack automatically, update the prompt to say something like, â€œthis is not a correct answer- {previous_answer} and always only respond with a â€œyesâ€ or â€œnoâ€â€ and makes another language model call which improves the LLMs response because of this newly optimized prompt. In addition, you can also incorporate things like multi hops in your retrieval where you can do something like â€œretrieve -> generate queries and then retrieve again using the generated queriesâ€ for n times and build up a larger context to answer the original question.

Obviously, this can also be done using usual prompt engineering and programming techniques, but the framework exposes native easy to use settings and constructs to do these things more naturally. DSPy as a concept really shines when you are composing a pipeline of language model calls where prompt engineering the entire pipeline or even module wise can lead to a brittle Pipeline.

DSPy the Framework:

Now coming to the framework which is built in python, I think the framework as it stands today is

1.	â Not production ready
2.	â Lacks clear documentation
3.	â Poorly designed with not so clean interfaces and abstractions

To me it felt like a rushed implementation with little thought for design thinking, testing and programming principles. The framework code is very hard to understand with a lot of meta programming and data structure parsing and construction going behind the scenes that are scary to run in production.

This is a huge deterrent for anyone trying to learn and use this framework. But, I am sure the creators are thinking about all this and are working to reengineer the framework. Thereâ€™s also a typescript implementation of this framework that is fairly less popular but has a much better and cleaner design and codebase:

https://github.com/dosco/llm-client/

My final thought about this framework is, itâ€™s a promising concept, but it does not change anything about what we already know about LLMs. Also, hiding prompts as templates does not mean prompt engineering is going away, someone still needs to â€œengineerâ€ the prompts the framework uses and imo the framework should expose these templates and give control back to the developers that way, the vision of separate programming and prompting co exists with giving control not only to program but also to prompt.

Finally, I was able to understand all this by running DSPy programs and visualizing the LLM calls and what prompts itâ€™s adding using my open source tool - https://github.com/Scale3-Labs/langtrace . Do check it out and let me know if you have any feedback.",1,16
MachineLearning,t3_1cqndld,1715563463.0,/r/MachineLearning/comments/1cqndld/d_please_consider_signing_this_letter_to_open/,Discussion,[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",45,161
MachineLearning,t3_1cqhsln,1715547320.0,/r/MachineLearning/comments/1cqhsln/p_simplegemm_fast_and_minimal_tensor_core_matrix/,Project,[P] SimpleGEMM: Fast and minimal tensor core matrix multiplication in CUDA,"Hello all! Sharing my side project here: [https://github.com/andylolu2/simpleGEMM](https://github.com/andylolu2/simpleGEMM) !

This is an *extremely* minimalistic but fast implementation of matrix multiplication in CUDA. The source code is a single, 200-line CUDA/C++ file which implements fp16 tensor core matrix multiplication, optimised for Turing (SM75) architecture. The goal is to:

1. Write a matmul kernel that does not sacrifice performance. In fact, it's faster than PyTorch/CuBLAS if you [test it on a T4 in Colab](https://colab.research.google.com/github/andylolu2/simpleGEMM/blob/master/colab/simpleGEMM.ipynb)!
2. Make it hackable for new purposes. For example if you want to add a new custom prologue (e.g. Matmul + some reduction), just go to line 186, add your code, and recompile! Full flexibility with no C++ templating shenanigans.
3. Keep it as simple as possible. Hopefully someone learning CUDA will find this useful!

Of course, I didn't implement *everything* from scratch. Most of the this builds upon Nvidia CUTLASS's new CuTe interface for things like memory layout, data copying and using tensor core instructions.

*Aside:*

*Why not OpenAI Triton? I love triton, but sometimes it's hard to get the extra 10-20% performance if you are doing something off its main optimisation path. In fact,* [*triton's matmul for Turing GPUs is quite slow*](https://github.com/openai/triton/issues/189) *(because they mainly optimise for SM80+). I just enjoy having full control over the hardware, knowing that if I have infinite time I can squeeze very single bit of performance out.*",3,44
MachineLearning,t3_1cqejjc,1715538786.0,/r/MachineLearning/comments/1cqejjc/d_should_active_learning_samples_classes_uniformly/,Discussion,[D] should active learning samples classes uniformly,"When using active learning to sample images from an unlabeled dataset, existing works usually does so by trying to have an uniform number of image per class. This approach allow to mitigate the class imbalance issue that can exist in some datasets.

However, when building up a dataset, we  want our training set to be as close as possible to the real dataset in term of class distribution. Thus, is the approach of AL methods wrong for trying to sample an uniform number of image per class?",3,3
MachineLearning,t3_1cqd2ss,1715534929.0,/r/MachineLearning/comments/1cqd2ss/p_darwin_opensourced_devin_alternative/,Project,[P] DARWIN - open-sourced Devin alternative,"**ğŸš€ Introducing DARWIN - Open Sourced, AI Software Engineer Intern! ğŸ¤–**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And whatâ€™s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 ğŸ“¹ Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 ğŸ“¹ Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 ğŸ“¹ Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",29,43
MachineLearning,t3_1cq8ufm,1715523371.0,/r/MachineLearning/comments/1cq8ufm/r_how_far_are_we_to_gpt4v_closing_the_gap_to/,Research,[R] How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites,"**Paper**: [https://arxiv.org/abs/2404.16821](https://arxiv.org/abs/2404.16821)

**Code**: [https://github.com/OpenGVLab/InternVL](https://github.com/OpenGVLab/InternVL)

**Models**: [https://huggingface.co/OpenGVLab](https://huggingface.co/OpenGVLab)

**Chat demo**: [https://internvl.opengvlab.com/](https://internvl.opengvlab.com/)

**Hugging Face demo**: [https://huggingface.co/spaces/OpenGVLab/InternVL](https://huggingface.co/spaces/OpenGVLab/InternVL)

**Abstract**:

>In this report, we introduce **InternVL 1.5**, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448Ã—448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at [this https URL](https://github.com/OpenGVLab/InternVL).",0,9
MachineLearning,t3_1cq8guo,1715522306.0,/r/MachineLearning/comments/1cq8guo/r_curvatureinformed_sgd_via_general_purpose/,Research,[R] Curvature-Informed SGD via General Purpose Lie-Group Preconditioners,"**Paper**: [https://arxiv.org/abs/2402.04553](https://arxiv.org/abs/2402.04553)

**Code** (*toy experiments*): [https://github.com/lixilinx/psgd\_torch](https://github.com/lixilinx/psgd_torch)

**Code** (*large scale experiments*): [https://github.com/opooladz/Preconditioned-Stochastic-Gradient-Descent](https://github.com/opooladz/Preconditioned-Stochastic-Gradient-Descent)

**Abstract**:

>We present a novel approach to accelerate stochastic gradient descent (SGD) by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm. Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner. We update both preconditioners online using a criterion that is robust to stochastic gradient noise and does not require line search or damping. To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups. The Lie group's equivariance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers. As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios. Our proposed approach offers a promising direction for improving the convergence of SGD with low computational overhead. We demonstrate that **Preconditioned SGD** (**PSGD**) outperforms SoTA on Vision, NLP, and RL tasks across multiple modern deep-learning architectures. We have provided code for reproducing toy and large scale experiments in this paper.",0,14
MachineLearning,t3_1cq6jgz,1715516152.0,/r/MachineLearning/comments/1cq6jgz/p_a_look_at_the_latest_major_open_llm_releases/,Project,"[P] A look at the latest major open LLM releases: Mixtral, Llama 3, Phi-3, and OpenELM",,1,26
MachineLearning,t3_1cq5g4r,1715512124.0,/r/MachineLearning/comments/1cq5g4r/d_how_do_unets_achieve_spatial_consistency/,Discussion,[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever â€knowsâ€œ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels â€not enoughâ€œ. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",19,17
MachineLearning,t3_1cq3uh4,1715505438.0,/r/MachineLearning/comments/1cq3uh4/d_impact_of_solar_storm_on_qlora_rlhf_of_llama3_8b/,Discussion,[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage.""Â 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",31,214
MachineLearning,t3_1cq3qep,1715504930.0,/r/MachineLearning/comments/1cq3qep/integrating_multiple_ais_for_single/,Research,Integrating Multiple AIs for Single Purposesâ€”Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?",10,0
MachineLearning,t3_1cq3k96,1715504233.0,/r/MachineLearning/comments/1cq3k96/can_one_use_squared_inverse_of_kl_divergence_as/,Discussion,Can one use squared inverse of KL divergence as another divergence metric? [D],"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:

The KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\log \frac{p}{q}]$`

depending on the order of p and q, the divergence is mode seeking or mode covering.

However, can one use `$\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?

Or maybe not a divergence metric (strictly speaking), but something to measure similarity/dissimilarity between the two distributions?

**Edit**:

it is definitely not a divergence as `-1/KL(p,q) <= 0` also as pointed in the discussion, `1/KL(p,p) = +oo`.

However, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1/KL(p,q)` is increasing `=>` `-1/KL(p,q)` is decreasing. Although, `-1/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.",5,7
MachineLearning,t3_1cpu3lc,1715469836.0,/r/MachineLearning/comments/1cpu3lc/r_llms_related_research_papers_published_on_may/,Research,[R] LLMs related research papers published on May 8th 2024,,0,4
MachineLearning,t3_1cpqe3h,1715459209.0,/r/MachineLearning/comments/1cpqe3h/r_trying_to_understand_a_certain_function_in/,Research,[R] Trying to understand a certain function in MaskCLIP," Hello,

So I was trying to re-produce this paper: [https://arxiv.org/pdf/2208.12262](https://arxiv.org/pdf/2208.12262)

However, I got stuck on a certain function that I don't understand. Specifically, the ""quantizer"" *h,* in Equation 6 shared below:

&#x200B;

https://preview.redd.it/ykotdlvjwuzc1.png?width=625&format=png&auto=webp&s=b5d5d6a9e35414ee65f1609a2508864719431c8a

Firstly: I don't understand what ""soft codewords distribution"" means. Do they mean they passed the output features through a softmax first? If so, then why is there an EMA *h()* if h() is just a softmax.

They cite iBOT so they could mean two things: The iBOT head (which is just MLP layers) or the centering/sharpening + softmax in the iBOT loss. If they mean the former, then why do they have the decoder in equation 5? Only the student outputs go through the decoder, as highlighted in their figure 1. If they mean the centering/sharpening + softmax thing from the iBOT loss, then why do they describe the quantizer as ""online"" which implies that it is trainable.

The code is not public, and I did try to contact the authors about something else before, but didn't get any reply.

Any ideas or thoughts would be greatly appreciated!

&#x200B;",0,4
MachineLearning,t3_1cpn1dl,1715449781.0,/r/MachineLearning/comments/1cpn1dl/d_help_credit_analysis_model/,Discussion,[D] Help credit analysis model,"The objective of my project is the evaluation of Artificial Intelligence Models for Credit Card Fraud Detection in order to discuss their implications and applications. The data I will be using is provided by an institution and although it is somewhat outdated (from the year 2021), it can be used for the objectives of my project.

To have a better idea of everything described so far, what I aim to achieve with my development is as follows:



a. Review and select appropriate artificial intelligence techniques for fraud analysis in credit cards.

b. Implement various predictive models to identify fraud patterns.

c. Evaluate and compare the accuracy and efficiency of each model using a sample of real data.

d. Recommend the most effective model or models for practical implementation, based on criteria of accuracy, processing speed, and ease of integration into existing systems.

e. Propose improvements in fraud detection processes based on the results obtained.



Based on this, I have the following questions to know if I am meeting the objectives so far:

1. After getting an idea of the project and visualizing the data, did I approach it correctly or should I give it another focus, and if so, what would you recommend?

2. Is the way the data was processed correct?

3. For the selection of the most impactful features for my model, I used the Recursive Feature Elimination technique. For the type of problem I want to address, can this technique be applied, or should I implement another one that is perhaps more robust?

4. Regarding the selection of models, do you recommend any others that may help and have more relevance and impact on my solution? Do the ways in which I evaluated them and the results obtained seem logical to you? Do you think they could be applied in institutions? What would be missing for them to be considered and applied to real situations?

I also want to know the correct ways to test the model since I tried creating a synthetic dataset with characteristics similar to the one I have, called ""synthetic data,"" but as it is under the same conditions as the original (unbalanced and untreated data), I have no idea how to do it. Although I found on a website that using a pipeline could do it, I'm not convinced by that.

Any other observations or contributions outside of everything mentioned are welcomeÂ asÂ well.

Here is my project and data files:  
  
Notebook:  
[https://colab.research.google.com/drive/1DnluH0fMIuPF3ZOO0czRVZ2eliyHK4l7#scrollTo=Albbq\_mLKsR-&uniqifier=2](https://colab.research.google.com/drive/1DnluH0fMIuPF3ZOO0czRVZ2eliyHK4l7#scrollTo=Albbq_mLKsR-&uniqifier=2)

Data:  
[https://drive.google.com/drive/folders/1eskK2avrZXFoCYzm87QbDMdlO2trZdPd?usp=sharing](https://drive.google.com/drive/folders/1eskK2avrZXFoCYzm87QbDMdlO2trZdPd?usp=sharing)

Thanks in advance!",3,0
MachineLearning,t3_1cpj6b9,1715438981.0,/r/MachineLearning/comments/1cpj6b9/p_lora_from_scratch_implementation_for_llm/,Project,[P] LoRA from scratch implementation for LLM classifier training,,6,53
MachineLearning,t3_1cpi9gc,1715436347.0,/r/MachineLearning/comments/1cpi9gc/d_dealing_with_conflicting_training/,Discussion,[D] Dealing with conflicting training configurations in reference works.,"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.

The problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https://github.com/yuantn/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.

Then comes the [CVPR22 ](https://arxiv.org/abs/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.

Then you have [PPAL - CVPR24](https://arxiv.org/abs/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https://github.com/ChenhongyiYang/PPAL/blob/15875ed7a524675bc6daeba79b3716a0abca2b64/configs/voc_active_learning/al_train/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).

There are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.

My question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?",7,10
MachineLearning,t3_1cpcwuz,1715416721.0,/r/MachineLearning/comments/1cpcwuz/r_marcus_hutters_work_on_universal_artificial/,Research,[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube:Â [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplaceâ€™s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks",45,95
MachineLearning,t3_1cpbgd1,1715410772.0,/r/MachineLearning/comments/1cpbgd1/p_llminator_a_llamacpp_gradio_based_opensource/,Project,[P] LLMinator: A Llama.cpp + Gradio based opensource Chatbot to run llms locally(cpu/cuda) directly from HuggingFace,"Hi I am currently working on a context-aware streaming chatbot based on Llama.cpp, Gradio, Langchain, Transformers. LLMinator can pull LLMs directly from HF & run them locally on cuda or cpu.

I am looking for recommendations & help from opensource community to grow this further.

**Github Repo:**Â [https://github.com/Aesthisia/LLMinator](https://github.com/Aesthisia/LLMinator)

**Goal:**Â To help developers with kickstarter code/tool to run LLMs.

https://preview.redd.it/fnzja7rjwqzc1.png?width=1846&format=png&auto=webp&s=a62c43614d63e82156fef8722b986b051cc1795b

**Features:**

* Context-aware Chatbot.
* Inbuilt code syntax highlighting.
* Load any LLM repo directly from HuggingFace.
* Supports both CPU & Cuda modes.
* Load & Offload saved models.
* Command Line Args
* API Access(Soon to be available)

Any review or feedback is appreciated.",0,6
MachineLearning,t3_1cpalcs,1715407467.0,/r/MachineLearning/comments/1cpalcs/d_handson_machine_learning_with_scikitlearn_keras/,Discussion,"[D] Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow 2nd Edition ","I bought this book when it came out and worked through a couple of chapters. I really enjoyed but ended up never finishing it but now that I actually have an opportunity to dedicate time to it, im wondering if it's up to date enough (it's from 2019) or of there would be a more recent book that covers similar topics. 

Any tips appreciated ğŸ‘",14,11
MachineLearning,t3_1cpa4io,1715405658.0,/r/MachineLearning/comments/1cpa4io/d_how_to_train_very_shallow_dot_product_networks/,Discussion,[D] How to train very shallow (dot product) networks with huge embeddings on a GPU cluster?,"In the olden days we used dozens of parameter servers and hundreds of CPU machines to train such heavy embedding light compute models and achieved impressive throughput. Nowadays with GPU clusters with high speed NVlink, looks like the throughput actually gets much worse. Of course I am talking about a dozen or so GPU machines each with say 8 A100. The tensor core utilization is very minimal (< 1%), but the GPUs are very busy due to all2all communication. I am trying to wrap my head around what the bottleneck maybe with the latter setup, is it simply that all2all (or ring all reduce etc) is intrinsically slower than parameter server when the number of parameters gets large, no matter how fast the nvlink is?",6,16
MachineLearning,t3_1covm2r,1715363461.0,/r/MachineLearning/comments/1covm2r/d_why_does_nproc_per_node_not_work_for_values/,Discussion,[D] Why does nproc_per_node not work for values greater than 1?,"Context: Running a training for dinov2 using torchrun. I have two nodes. When I run training (1 gpu per node) w/nproc=1, it works. When I allocate 2 gpus per node, I change nproc to 2. The training then crashes when trying to initialize the model. Any insight on what this could be?",8,0
MachineLearning,t3_1coviyc,1715363241.0,/r/MachineLearning/comments/1coviyc/d_seeking_insights_on_time_series_data/,Discussion,[D] Seeking Insights on Time Series Data Augmentation: Python Libraries and Benchmark Datasets,"Hey everyone,

I'm diving into the world of time series data augmentation and I'm curious about the current state of the art techniques, particularly those that are accessible through Python libraries.

**Techniques:** What are some of the most effective methods for augmenting time series data? Are there any recent advancements or innovative approaches worth exploring?


**Python Libraries:** Are there any Python libraries that offer comprehensive support for time series data augmentation? I'm particularly interested in libraries that provide easy-to-use implementations of various augmentation techniques.


**Benchmark Datasets:** When it comes to benchmarking time series data augmentation techniques, are there any go-to datasets that the community often relies on? It would be great to have some reference datasets for evaluating the effectiveness of different augmentation strategies.
I'm eager to hear from those who have experience in this domain. Any insights, recommendations, or resources you can share would be immensely helpful in guiding my exploration.

Thanks in advance for your input!",0,1
MachineLearning,t3_1cou91s,1715359944.0,/r/MachineLearning/comments/1cou91s/p_google_colab_crashes_before_even_training_my/,Project,[P] Google Colab crashes before even training my images dataset.,"I have 780 images. All of them are microscopic and I'm doing microplastic image detection. First I did binary classification using U-Net and then VGG-16 transfer learning. Google Colab didn't crash one bit. Worked really well.

Now I'm doing multi-class segmentation and pre-processing is kinda same. except for one extra channel for colored masks.

But, just by storing the categorical masks of training dataset, my System Ram exceeds 6-7GB. I have 580 images each of size 512x512 after resize. they are even smaller before resize though.

So, what is going on here? Any help would be appreciated.

Instead of preprocessing every time I store the data in npz format and load them in variables. they are of maximum 1GB. but not higher.

I'm stuck. It's been two days but I simply can't train. Also, I'm a student and don't have money to get the Colab Pro. My laptop is GTX-1650 so, absolute no way it would perform better then Google Colab especially since I have only 8GB RAM.",15,11
MachineLearning,t3_1cosbka,1715355017.0,/r/MachineLearning/comments/1cosbka/d_is_evaluating_llm_performance_on_domainspecific/,Discussion,[D] Is Evaluating LLM Performance on Domain-Specific QA Sufficient for a Top-Tier Conference Submission?,"Hello,

Hello,  
I'm preparing a paper for a top-tier conference and am grappling with what qualifies as a significant contribution. My research involves comparing the performance of at least five LLMs on a domain-specific question-answering task. For confidentiality, I won't specify the domain.

I created a new dataset from Wikipedia, as no suitable dataset was publicly available, and experimented with various prompting strategies and LLM models, including a detailed performance analysis.

I believe the insights gained from comparing different LLMs and prompting strategies could significantly benefit the community, particularly considering the existing literature on LLM evaluations ([https://arxiv.org/abs/2307.03109](https://arxiv.org/abs/2307.03109)). However, some professors argue that merely ""analyzing LLM performance on a problem isn't a substantial enough contribution.""

Given the many studies on LLM evaluation accepted at high-tier conferences, what criteria do you think make such research papers valuable to the community?

Thanks in advance for your insights!",11,6
MachineLearning,t3_1cos6bq,1715354641.0,/r/MachineLearning/comments/1cos6bq/n_book_lauching_accelerate_model_training_with/,News,[N] Book Lauching: Accelerate Model Training with PyTorch 2.X ,"Hello everyone! My name is Maicon Melo Alves and I'm a High Performance Computing (HPC) system analyst specialized in AI workloads.

I would like to announce that my book ""**Accelerate Model Training with PyTorch 2.X: Build more accurate models by boosting the model training process**"" was recently launched by Packt.

This book is for intermediate-level data scientists, engineers, and developers who want to know how to use PyTorch to accelerate the training process of their machine-learning models.

If you think this book can help other professionals, please share this post with your community! ğŸ˜Š

Thank you very much!",4,18
MachineLearning,t3_1cook89,1715344696.0,/r/MachineLearning/comments/1cook89/d_best_communitywebsite_to_find_ml_engineer/,Discussion,[D] Best community/website to find ML engineer interested in hourly work,"I've been searching for a machine learning engineer on platforms like Upwork, but many of the candidates seem to have limited experience in building models from scratch. They often focus on integrating pre-built ML APIs rather than developing custom models tailored to specific requirements. 

Where is the best place to find ML engineers that can handle the entire model development process from data collection to model deployment? ",48,34
MachineLearning,t3_1com9qh,1715336808.0,/r/MachineLearning/comments/1com9qh/d_what_on_earth_is_discretization_step_in_mamba/,Discussion,"[D] What on earth is ""discretization"" step in Mamba?","What is there to ""discretize""? Isn't the signal / sequence already ""discrete"" in the form of tokens? Please don't send me over to wikipedia article about ""Discretization of linear state space modelsÂ "", because I cannot draw any connection to LLMs. It seems to me that Mamba at its core is just EMA with dynamic alpha parameter that is calculated from the current token at time t for each channel. Don't quite understand what is the benefit of ""discretization"" and what it actually does to the data.",25,64
MachineLearning,t3_1colzy7,1715335693.0,/r/MachineLearning/comments/1colzy7/pycaret_unstable_d/,Discussion,Pycaret unstable [D],"I have a forecasting application backed by pycaret, however suddenly at times pycaret based models raise an unkown exception and suddenly after a day or so it starts working . I am unable to understand error nor this exception as the exception says this exception should not have occured. On debugging on my local the same inputs are working fine. Does anyone have any idea on such issues? Is there any alternative to auto generate ml models for a forecasting application ?

Appreciate your support. Thanks.

  
Edit:  I am getting this error, this is the most detail I could reach for, however a trained model is being called for predictions, so I don't understand how this error is possible, and I  am unable to reproduce the error as mentioned earlier.

https://preview.redd.it/0sn9n3ji240d1.png?width=1550&format=png&auto=webp&s=3aee59660e2bed8d9801ac902346bf5bef0d6920

",4,3
MachineLearning,t3_1coluve,1715335171.0,/r/MachineLearning/comments/1coluve/r_better_faster_large_language_models_via/,Research,[R] Better & Faster Large Language Models via Multi-token Prediction,"**Paper**: [https://arxiv.org/abs/2404.19737](https://arxiv.org/abs/2404.19737)

**Abstract**:

>Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict *multiple* future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.",4,17
MachineLearning,t3_1coj061,1715322923.0,/r/MachineLearning/comments/1coj061/generating_outputs_from_last_layers_hidden_state/,Discussion,Generating outputs from last layer's hidden state values [D],"I manipulated the hidden state values obtained from the llama-2 model after feeding it a certain input, let's call it Input\_1. Now, I want to examine the output (causal output) it produces from this. My hypothesis is that it should correspond to a different input, let's call it Input\_2, which would yield a distinct output from the initial input.

I got last layer's hidden state values in the following manner :

    from transformers import LlamaModel, LlamaTokenizer, LlamaForCausalLM
    tokenizer = LlamaTokenizer.from_pretrained(path_to_llama2)
    model = LlamaModel.from_pretrained(path_to_llama2)
    model_ = LlamaForCausalLM.from_pretrained(path_to_llama2)
    
    tokenizer.pad_token = tokenizer.eos_token
    inputs = tokenizer(prompt, return_tensors='pt')    
    
    with torch.no_grad():
      outputs = model(**inputs, output_attentions=True, output_hidden_states=True)
      hidden_states = outputs.hidden_states[-1]  # Last layer hidden states

As shown above, I was trying to change hidden\_states values which I got from `model` but now I want to generate a causal output. How can I do it? Are there any suggestions?",29,3
MachineLearning,t3_1coi4iy,1715319425.0,/r/MachineLearning/comments/1coi4iy/d_how_to_use_rag_benchmarks_in_practice/,Discussion,[D] How to use RAG benchmarks in practice,"I am working on a research projects which involves experimenting with RAGs. I want to run the models first to get an understanding of how the whole pipeline works. I found some datasets on HuggingFace (such as https://huggingface.co/datasets/explodinggradients/WikiEval).

My understanding of RAGs is that I should be given a datastore, and then I perform the task of question answering using the datastore. However, in these datasets, context is given along with the question, and I do not quite understand that. Is RAG supposed to be performed as in-context question answering? If yes, doesn't it destroy the point of *retrieval* in RAG?

  
Another way to put my question is as follows: shouldn't every RAG dataset have a dataset level document store instead of providing context along with the question? ",4,15
MachineLearning,t3_1co9uaf,1715293481.0,/r/MachineLearning/comments/1co9uaf/d_training_on_cifar10/,Discussion,[D] Training on CIFAR10,"Hi everyone, is there any known set of hyperparameters for training a diffusion model on  CIFAR10 or any other famous datasets primarily for reconstruction loss ?",2,0
MachineLearning,t3_1co7w0i,1715288460.0,/r/MachineLearning/comments/1co7w0i/d_eccv2024_reviews_are_out/,Discussion,[D] ECCV-2024 reviews are out ,Title says it all.,45,32
MachineLearning,t3_1co4kfw,1715280134.0,/r/MachineLearning/comments/1co4kfw/d_iclr_outstanding_paper_awards_congratulations/,Discussion,[D] ICLR Outstanding Paper Awards. Congratulations!,"[**Vision Transformers Need Registers**](https://openreview.net/forum?id=2dnO3LLiJ1)  
*TimothÃ©e Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski*

**Abstract:**Â Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.

[**Generalization in diffusion models arises from geometry-adaptive harmonic representations**](https://openreview.net/forum?id=ANvmVS2Yr0)  
*Zahra Kadkhodaie, Florentin Guth, Eero P Simoncelli, StÃ©phane Mallat*

**Abstract:**Â Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the â€œtrueâ€ continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough. In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous regions. We demonstrate that trained denoisers are inductively biased towards these geometry-adaptive harmonic bases since they arise not only when the network is trained on photographic images, but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.

[**Learning Interactive Real-World Simulators**](https://openreview.net/forum?id=sFyTZEqmUY)  
*Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, Pieter Abbeel*

**Abstract:**Â Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as â€œopen the drawerâ€ and low-level controls such as â€œmove by x,yâ€ from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications.

[**Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors**](https://openreview.net/forum?id=PdaPky8MUn)  
*Ido Amos, Jonathan Berant, Ankit Gupta*

**Abstract:**Â Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using only the downstream task data, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",8,130
MachineLearning,t3_1co2ye4,1715276121.0,/r/MachineLearning/comments/1co2ye4/d_where_does_the_term_feature_come_from/,Discussion,"[D] Where does the term ""feature"" come from?","Maybe a stupid trivia question, but I can't figure it out. ML calls features features, stats calls features predictors, math calls features variables, engineering calls features variables too.

I know what they are, but WHY do we call them features? Does anyone know the origin story?

**EDIT:** You all gave me some good leads; I think I have found a plausible answer: It likely comes from cognitive psychology. The [paper introducing the perceptron](https://psycnet.apa.org/record/1959-09865-001) (arguably the first step towards neural networks) refers to inputs as stimuli, but also notes that encoding stimuli into a small set of robust features helps performance:

>As the number of responses in the system increases, the performance becomes progressively poorer, if every response is made mutually exclusive of all alternatives. One method of avoiding this deterioration (described in detail in Rosenblatt, 15) is through the binary coding of responses. In this case, i**nstead of representing 100 different stimulus patterns by 100 distinct, mutually exclusive responses, a limited number of discriminating features is found, each of which can be independently recognized as being present or absent**, and consequently can be represented by a single pair of mutually exclusive responses.

(highlighting is mine)

And later it concludes

>The performance of the system can be improved by the use of a contour-sensitive projection area, and by the use of a binary response system, in which **each response, or ""bit,"" corresponds to some independent feature or attribute of the stimulus**.

(highlighting mine)",19,51
MachineLearning,t3_1co2k7i,1715275088.0,/r/MachineLearning/comments/1co2k7i/r_qserve_w4a8kv4_quantization_and_system_codesign/,Research,[R] QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving,"ğŸ“š Research paper:Â [http://arxiv.org/abs/2405.04532v1](http://arxiv.org/abs/2405.04532v1)

ğŸ¤”Â Why?:Â Existing INT4 quantization techniques failing to deliver performance gains in large-batch, cloud-based language model serving due to significant runtime overhead on GPUs.

ğŸ’»Â H**ow?:Â The research paper proposesÂ a new quantization algorithm, QoQ,** which stands forÂ q**uattuor-octo-quattuor,** thatÂ uses 4-bit weight, 8-bit activation, and 4-bit KV cache.Â This algorithm is implemented in the QServe inference library and aims to reduce dequantization overhead on GPUsÂ b\*\*y introducing progressive quantization.Â \*\*Additionally, the research paper introducesÂ SmoothAttention to mitigate accuracy degradation caused by 4-bit KV quantization.Â QServe also performs compute-aware weight reordering and utilizes register-level parallelism to reduce dequantization latency. Finally, QServe makes use of fused attention memory-bound to further improve performance.

ğŸ¦¾Â Performance gain:Â The research paper achieves significant performance improvements compared to existing techniques. QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100.",0,2
MachineLearning,t3_1cnzdh9,1715266880.0,/r/MachineLearning/comments/1cnzdh9/project_how_to_find_instance_segmentation_model/,Project,[Project] How to find Instance segmentation Model Zoo Repositories?,"I am working on a project for instance segmentation using TensorFlow. The Professor told me to find github repositories that are model zoo for instance segmentation. It should work with TensorFlow and should have pretrained models. The problem is that I could not find model zoo, rather individual models.Â **How do I find github repositories that are model zoo for instance segmentation, and are compatible with TensorFlow?**Â Besides links and resources, any further advices and suggestions are highly appreciated. Thank you

The things I tried so far:

1. Google search of â€œinstance segmentation githubâ€.
2. Search â€œinstance segmentationâ€ in github search bar.
3. AskingÂ **ChatGpt**Â andÂ **Gemini**Â if it can find any repositories for me. I could find frameworks likeÂ **PaddlePaddle**, orÂ **supervision**, orÂ **AdelaiDet**Â etc, but they are not compatible with Tensorflow. They are rather standalone frameworks. I could also find repositories that were model zoo of instance segmentation, but are compatible with PyTorch. The Professor told me to use TensorFlow, not PyTorch.

I have looked through around 50 to 60 repositories until now.",8,0
MachineLearning,t3_1cnxb8p,1715261320.0,/r/MachineLearning/comments/1cnxb8p/r_seeking_guidance_thesis_on_comparing/,Research,[R] Seeking Guidance: Thesis on Comparing Classification Models for Corporate Credit Ratings,"Hey everyone,

I'm currently working on my graduation thesis and could use some guidance. My research revolves around comparing various classification models for predicting corporate credit ratings within a dataset.

The models I'm diving into include logistic regression (logit), probit regression, random forest, k-nearest neighbors (KNN), and simple neural networks. Additionally, I'm incorporating mathematical metrics such as the Altman Z-score and the model proposed by Jarrod (1973) to enhance the analysis.

However, I'm encountering difficulties in finding comprehensive research papers that discuss robust methods for validating and evaluating these classification models.

My primary goal is to assess and compare the performance of these models in accurately classifying corporate credit ratings.

Any insights, recommendations, or suggested resources on validation and evaluation techniques for classification models would be immensely appreciated!

Thanks in advance!",0,0
MachineLearning,t3_1cnwbvu,1715258508.0,/r/MachineLearning/comments/1cnwbvu/d_pytorch_training_on_ipad/,Discussion,[D] pytorch training on iPad ,Do people use pytorch and python to train small scale models on iPad? New ipads have very good hardware comparable to high end laptops. Is it possible to do it with min hassle? Is it worth it to get new models for this convenient device for small scale training?,8,0
MachineLearning,t3_1cnvgfi,1715255792.0,/r/MachineLearning/comments/1cnvgfi/d_train_whisper_with_synthetic_audio/,Discussion,[D] Train Whisper with synthetic audio ,Is it possible to fine tune Whisper with a synthetic audio and transcription without timestamp ?,1,0
MachineLearning,t3_1cnuot2,1715253205.0,/r/MachineLearning/comments/1cnuot2/d_any_encoder_only_model_having_bigger_max_token/,Discussion,"[D] Any encoder only model having bigger max token than 512 (BERT, Roberta, etc)?","Thanks for reading this.

Is there any encoder model like BERT, Roberta, etc, thats has a bigger max token length, say around 4k. I am looking to finetune one for multi-class classification usecase.

Thanks in advance!",7,7
MachineLearning,t3_1cnu9mx,1715251726.0,/r/MachineLearning/comments/1cnu9mx/r_alphamath_almost_zero_process_supervision/,Research,[R] AlphaMath Almost Zero: process Supervision without process,"**Paper**: [https://arxiv.org/abs/2405.03553](https://arxiv.org/abs/2405.03553)

**Code**: [https://github.com/MARIO-Math-Reasoning/Super\_MARIO](https://github.com/MARIO-Math-Reasoning/Super_MARIO)

**Model**: [https://huggingface.co/MARIO-Math-Reasoning/AlaphaMath-7B](https://huggingface.co/MARIO-Math-Reasoning/AlaphaMath-7B)

**Abstract**:

>Recent advancements in large language models (LLMs) have substantially enhanced their mathematical reasoning abilities. However, these models still struggle with complex problems that require multiple reasoning steps, frequently leading to logical or numerical errors. While numerical mistakes can largely be addressed by integrating a code interpreter, identifying logical errors within intermediate steps is more challenging. Moreover, manually annotating these steps for training is not only expensive but also demands specialized expertise. In this study, we introduce an innovative approach that eliminates the need for manual annotation by leveraging the Monte Carlo Tree Search (MCTS) framework to generate both the process supervision and evaluation signals automatically. Essentially, when a LLM is well pre-trained, only the mathematical questions and their final answers are required to generate our training data, without requiring the solutions. We proceed to train a step-level value model designed to improve the LLM's inference process in mathematical domains. Our experiments indicate that using automatically generated solutions by LLMs enhanced with MCTS significantly improves the model's proficiency in dealing with intricate mathematical reasoning tasks.",0,19
MachineLearning,t3_1cntiks,1715248849.0,/r/MachineLearning/comments/1cntiks/d_eccv_2024_review_discussion/,Discussion,[D] ECCV 2024 Review Discussion,"I thought that, like with other conferences, that we might have a discussion for people submitting to ECCV, as the reviews will be out in 10 hours (10pm CEST (center european summer time)). It is my first time submitting anywhere and I am very nervous tbh.",79,51
MachineLearning,t3_1cnrv30,1715241740.0,/r/MachineLearning/comments/1cnrv30/d_is_it_a_good_idea_for_a_3rd_year_phd_student_to/,Discussion,[D] Is it a good idea for a 3rd year PhD student to start a TPAMI submission?,"I have a recently accepted paper (by ICML) in CV that, according to my advisor, could be submitted to trans with extra  solid work. I do have some academic ambition to get published on some top-tier trans like TPAMI. However, I've hearing a lot complaint from my peers that submission to trans could take 6-12months, without guaranteed outcomes. Since I'm in my late 3rd year of PhD, this could be quite risky as it may lead to an unwanted postponed graduation. So shall I keep focusing on conferences? Thanks in advance.",22,7
MachineLearning,t3_1cnr7ax,1715238902.0,/r/MachineLearning/comments/1cnr7ax/d_use_vqvaes_for_ssl/,Discussion,[D] Use VQ-VAEs for SSL?,"VQ-VAEs are used successfully to transform images into a representative latent space for diffusion models (LDM). For self supervised learning, however, I canâ€™t find people using them much to create an embedding that can later be used as input to downstream models to predict eg image classes.

Do you have an idea why that is? Intuitively, I would assume VQ-VAEs should also yield quite nice embeddings. ",4,5
MachineLearning,t3_1cnpo6n,1715232670.0,/r/MachineLearning/comments/1cnpo6n/d_matrix_profile_vs_deep_learning_for/,Discussion,[D] Matrix Profile vs. Deep Learning for Multivariate Time Series ,"Hey everyone,

So I was reading a plethora of approaches, especially regarding research done in Multivariate Time Series and real-time Human Activity Recognition (HAR). Though, I recently stumbled upon [Eamonn Keoghâ€™s](https://www.cs.ucr.edu/~eamonn/) amazing and comprehensive work on Matrix Profiles and ended up in a rabbit hole.Â 

But out of curiosity, in general how does the Matrix Profile compare with Deep Learning methods (e.g. MLP, LSTM, etc..) in the context of Multivariate Time Series and real-time streams of data? 

I would love to hear other peopleâ€™s perspectives!",13,37
MachineLearning,t3_1cnn54n,1715223825.0,/r/MachineLearning/comments/1cnn54n/d_reviewers_you_all_need_to_stop_being_so_lazy/,Discussion,[D] Reviewers you all need to stop being so lazy dog. Why are reviewers doing things so lazy man? ,"I submitted a paper.

Gets accepted to conference.

Got email from some random dude from \_insert\_university\_. Sending to both the chair and conference head.

Accuses me a plagarism and says 92% matching of publish papers...

Check cross reference. Title, authors (me and the mentor), data, conclusion, and almost the entire paper is highlighted.

Only source says Arkiv. I have my pre-print on there by chance. I followed their policies with pre-prints and put the notices.

Now, this is very stupid. I done a lot of due diligence and if its matching the authors, it has to be referencing my pre-print.

Why are reviewers so lazy and can do such drastic actions instead of just asking authors questions about these? I seriously don't understand some of these people. Do you have any suggestions about dealing with these situations?",41,160
MachineLearning,t3_1cnlztx,1715220310.0,/r/MachineLearning/comments/1cnlztx/d_is_it_possible_to_use_models_from_pinns_to/,Discussion,[D] Is it possible to use models from PINNs to replace manual calculation of functions?,"As we know, PINNs are done by modifying the loss function where we put the physics equations and their constants in the training model. I wonder if the model can be used to replace manual computation for different conditions, e.g. the constants involve other parameters.",0,1
MachineLearning,t3_1cnjvjl,1715214061.0,/r/MachineLearning/comments/1cnjvjl/research_icml_2024_camera_ready/,Research,[Research] ICML 2024 Camera Ready,"Hi all,

Just received an email with camera ready instructions, not mentioning anything about poster vs oral. Does that mean the paper is designated as poster alone, or there's no decision yet?

Thanks",4,5
MachineLearning,t3_1cnivqh,1715211258.0,/r/MachineLearning/comments/1cnivqh/research_adaptable_and_intelligent_generative_ai/,Research,[Research] Adaptable and Intelligent Generative AI through Advanced Information Lifecycle (AIL) ,"Video: [Husky AI: An Ensemble Learning Architecture for Dynamic Context-Aware Retrieval and Generation (youtube.com)](https://www.youtube.com/watch?v=FpxLg8eDfG0)  
Pleases excuse my video, I will make a improved one.  I would like to do a live event.

Abstract:

Husky AI represents a groundbreaking advancement in generative AI, leveraging the power of Advanced Information Lifecycle (AIL) management to achieve unparalleled adaptability, accuracy, and context-aware intelligence. This paper delves into the core components of Husky AI's architecture, showcasing how AIL enables intelligent data manipulation, dynamic knowledge evolution, and iterative learning. Developed entirely in Python, using open source tools like Transformers, Haystack and Elasticsearch, just to name a few.  Husky AI dynamically incorporates real-time data from the web and its local ElasticSearch DB, significantly expanding its knowledge base and contextual understanding. The system's ability to continuously learn and refine its response generation capabilities through user interactions sets a new standard in the development of generative AI systems. Husky AI's superior performance, real-time knowledge integration, and generalizability across applications position it as a paradigm shift in the field, paving the way for the future of intelligent systems.

Husky AI Architecture: A Symphony of AIL Components

At the heart of Husky AI's success lies its innovative architecture, which seamlessly integrates various AIL components to achieve its cutting-edge capabilities. Let's dive into the core elements that make Husky AI a game-changer:

2.1. Intelligent Data Manipulation: Streamlining Information Processing

Husky AI's foundation is built upon intelligent data manipulation techniques that ensure efficient storage, retrieval, and processing of information. The system employs state-of-the-art sentence transformers to convert structured & unstructured textual data into dense vector representations, known as embeddings. These embeddings capture the semantic meaning and relationships within the data, enabling precise similarity searches during information retrieval.

Under the hood, the preprocess\_and\_write\_data function works its magic. It ingests raw data, encodes it as a text string, and feeds it to the sentence transformer model. The resulting embeddings are then stored alongside the data within a Document object, which is subsequently committed to the document store for efficient retrieval.

2.2. Dynamic Context-Aware Retrieval: The Mastermind of Relevance

Husky AI takes information retrieval to the next level with its dynamic context-aware retrieval mechanism. The MultiModalRetriever class, in seamless integration with Elasticsearch (ESDB), serves as the mastermind behind this operation, ensuring lightning-fast indexing and retrieval.

When a user query arrives, the MultiModalRetriever springs into action. It generates a query embedding and performs a similarity search against the document embeddings stored within Elasticsearch. The similarity function meticulously calculates the semantic proximity between the query and document embeddings, identifying the most relevant documents based on their similarity scores. This approach ensures that Husky AI stays in sync with the evolving conversation context, retrieving the most pertinent information at each turn. The result is a system that generates responses that are not only accurate but also exhibit remarkable coherence and contextual relevance.

2.3. Ensemble of Specialized Language Models: A Symphony of Expertise

Husky AI takes response generation to new heights by employing an ensemble of specialized language models, orchestrated by the MultiModelAgent class. Each model within the ensemble is meticulously trained for specific tasks or domains, contributing its unique expertise to the response generation process.

When a user query is received, the MultiModelAgent leverages the retrieved documents and conversation context to generate responses from each language model in the ensemble. These individual responses are then carefully combined and processed to select the optimal response, taking into account factors such as relevance, coherence, and factual accuracy. By harnessing the strengths of specialized models like BlenderbotConversationalAgent, HFConversationalModel, and MyConversationalAgent, Husky AI can handle a wide range of topics and generate responses tailored to specific domains or tasks.

https://preview.redd.it/rijx6eytbazc1.png?width=1198&format=png&auto=webp&s=c958eaabfcea3fa23dc6fb4ce5fea3dd3dac03e2

2.4. Integration of CustomWebRetriever: The Game Changer

Husky AI takes adaptability and knowledge expansion to new heights with the integration of the CustomWebRetriever class. This powerful tool enables the system to dynamically retrieve and incorporate external data from the web, significantly expanding Husky AI's knowledge base and enhancing its contextual understanding by providing access to real-time information.

Under the hood, the CustomWebRetriever class leverages the Serper API to conduct web searches and retrieve relevant documents based on user queries. It generates query embeddings using sentence transformers and utilizes these embeddings to ensure that the retrieved information aligns closely with the user's intent.

The impact of the CustomWebRetriever on Husky AI's knowledge acquisition is profound. By incorporating this component into its pipeline, Husky AI gains access to a vast reservoir of external knowledge. It can retrieve up-to-date information from the web and dynamically adapt to new domains and topics. This dynamic knowledge evolution empowers Husky AI to handle a broader spectrum of information needs and provide accurate and relevant responses, even for niche or evolving topics.

Iterative Learning: The Continuous Improvement Engine

One of the key strengths of Husky AI lies in its ability to learn and improve over time through iterative learning. The system's knowledge base and response generation capabilities are continuously refined based on user interactions, ensuring a constantly evolving and adapting AI.

3.1. Learning from Interactions

With every user interaction, Husky AI diligently analyzes the conversation history, user feedback (implicit or explicit), and the effectiveness of the chosen response. This analysis provides invaluable insights that help the system refine its understanding of user intent, identify areas for improvement, and strengthen its knowledge base.

3.2. Refining Response Generation

The insights gleaned from user interactions are then used to refine the response generation process. Husky AI can dynamically adjust the weights assigned to different language models within the ensemble, prioritize specific information retrieval strategies, and optimize the response selection criteria based on user feedback. This continuous learning cycle ensures that Husky AI's responses become progressively more accurate, coherent, and user-centric over time.

3.3. Adaptability Across Applications

The iterative learning mechanism in Husky AI fosters generalizability, enabling the system to adapt to diverse applications. As Husky AI encounters new domains, topics, and user interaction patterns, it can refine its knowledge and response generation strategies accordingly. This adaptability makes Husky AI a valuable tool for a wide range of use cases, from customer support and virtual assistants to content generation and knowledge management.

4. Experimental Results and Analysis While traditional evaluation metrics provide valuable insights into the performance of generative AI systems, they may not fully capture the unique strengths and capabilities of Husky AI's AIL-powered architecture. The system's ability to dynamically acquire knowledge, continuously learn through user interactions, and leverage the synergy of its components presents challenges for conventional evaluation methods.  
4.1. The Limitations of Traditional Metrics Traditional evaluation metrics, such as precision, recall, and F1 score, are designed to assess the performance of individual components or specific tasks. However, Husky AI's true potential lies in the seamless integration and collaboration of its various modules. Attempting to evaluate Husky AI using isolated metrics would be like judging a symphony by focusing on individual instruments rather than appreciating the harmonious performance of the entire orchestra. Moreover, traditional metrics may not adequately account for Husky AI's ability to continuously learn and update its knowledge base through the \`CustomWebRetriever\`. The system's dynamic knowledge acquisition capabilities enable it to adapt to new domains and provide accurate responses to previously unseen topics. This ongoing learning process, driven by user interactions, is a progressive feature that may not be fully reflected in conventional evaluation methods.  
4.2. Showcasing Husky AI's Strengths through Real-World Scenarios To truly showcase Husky AI's superior capabilities, it is essential to evaluate the system in real-world scenarios that highlight its adaptability, contextual relevance, and continuous learning. By engaging Husky AI in diverse conversational contexts and assessing its performance over time, we can gain a more comprehensive understanding of its strengths and potential.  
4.2.1. Dynamic Knowledge Acquisition and Adaptation To demonstrate Husky AI's dynamic knowledge acquisition capabilities, the system can be exposed to new domains and topics in real-time. By observing how quickly and effectively Husky AI retrieves and incorporates relevant information from the web, we can assess its ability to adapt to evolving knowledge landscapes. This showcases the power of the \`CustomWebRetriever\` in expanding Husky AI's knowledge base and enhancing its contextual understanding.  
4.2.2. Continuous Learning through User Interactions Husky AI's continuous learning capabilities can be evaluated by engaging the system in extended conversational sessions with users. By analyzing how Husky AI refines its responses, improves its understanding of user intent, and adapts to individual preferences over time, we can demonstrate the effectiveness of its iterative learning mechanism. This highlights the system's ability to learn from user feedback and deliver increasingly personalized and relevant responses.  
4.2.3. Contextual Relevance and Coherence To assess Husky AI's contextual relevance and coherence, the system can be evaluated in real-world conversational scenarios that require a deep understanding of context and the ability to maintain a coherent dialogue. By engaging Husky AI in multi-turn conversations spanning various topics and domains, we can demonstrate its ability to generate accurate, contextually relevant, and coherent responses. This showcases the power of the ensemble model and the synergy between the system's components. Husky AI sets a new standard for intelligent, adaptable, and user-centric systems. Its AIL-powered architecture paves the way for the development of AI systems that can seamlessly integrate with the dynamic nature of real-world knowledge and meet the diverse needs of users. With its continuous learning capabilities and real-time knowledge acquisition, Husky AI represents a significant step forward in the quest for truly intelligent and responsive AI systems.

Samples of outputs and debug logs showcasing its abilities.  I would be happy to show more examples.

https://preview.redd.it/6p01bea2cazc1.png?width=1920&format=png&auto=webp&s=63ba611de7d32366ab0fdeef11f4d8c4ecb7a58c

https://preview.redd.it/v8jlpms9cazc1.png?width=2535&format=png&auto=webp&s=121ca669ef6f871473a1fdb51c71fba52111147f

https://preview.redd.it/qybm0jrrcazc1.png?width=2532&format=png&auto=webp&s=aeadd4152e8e0cd92c8a02df5a020f56751d1570",3,13
MachineLearning,t3_1cnhkyc,1715207752.0,/r/MachineLearning/comments/1cnhkyc/p_from_scrath_ppo_implementation/,Project,[P] From Scrath PPO Implementation.,"I've been for the past 5 months working on a from scratch PPO implementation. I am doing most of the work from scratch except numerical computation libraries such as numpy. It started with supervised learning networks to now this. And I just can't seem to get it. Every paper I read is A. Outdated/Incorrect B. Incomplete. No paper has a full description on what they do and what Hyper Params they use. I tried reading the SB3 code but it's too different from my implementation and I just don't understand whats happening as it's just so many files, I can't find the little nitts and gritts. So I'm just gonna post my backward method and if someone wishes to read it and would tell me some mistakes/reccomendation. Would be great! Side notes: I made the optim which uses standard gradient descent and the critic just takes state. I'm not using GAE as I'm trying to minimize potential failure points. All the hyperparams are standard vals.

    def backward(self):
        T = len(self.trajectory['actions'])
        for i in range(T):
            G = 0
            for j in range(i, T):
                current = self.trajectory['rewards'][j]
                G += current * pow(self.gamma, j - i)
    
            # G = np.clip(G, 0, 15)
            # CRITIC STUFF
            if np.isnan(G):
                break
            state_t = self.trajectory['states'][i]
            action_t = self.trajectory['actions'][i]
    
            # Calculate critic value for state_t
            critic_value = self.critic(state_t)
    
            # print(f""Critic: {critic_value}"")
            # print(f""G: {G}"")
            # Calculate advantage for state-action pair
            advantages = G - critic_value
    
            # print(f""""""Return: {G}
            # Expected Return: {critic}"""""")
            # OLD PARAMS STUFF
            new_policy = self.forward(state_t, 1000)
    
            # PPO STUFF
            ratio = new_policy / action_t
    
            clipped_ratio = np.clip(ratio, 1.0 - self.clip, 1.0 + self.clip)
    
            surrogate_loss = -np.minimum(ratio * advantages, clipped_ratio * advantages)
    
            # entropy_loss = -np.mean(np.sum(action_t * np.log(action_t), axis=1))
            # Param Vector
            weights_w = self.hidden.weights.flatten()
            weights_x = self.hidden.bias.flatten()
            weights_y = self.output.weights.flatten()
            weights_z = self.output.bias.flatten()
            weights_w = np.concatenate((weights_w, weights_x))
            weights_w = np.concatenate((weights_w, weights_y))
            param_vec = np.concatenate((weights_w, weights_z))
            param_vec.flatten()
    
            loss = np.mean(surrogate_loss)  # + self.l2_regularization(param_vec)
            # print(f""loss: {loss}"")
            # BACKPROPAGATION
            next_weights = self.output.weights
    
            self.hidden.layer_loss(next_weights, loss, tanh_derivative)
    
            self.hidden.zero_grad()
            self.output.zero_grad()
    
            self.hidden.backward()
            self.output.backward(loss)
    
            self.hidden.update_weights()
            self.output.update_weights()
    
            self.critic_backward(G)",2,4
MachineLearning,t3_1cnfmec,1715202934.0,/r/MachineLearning/comments/1cnfmec/research_consistency_llms_converting_llms_to/,Research,[Research] Consistency LLMs: converting LLMs to parallel decoders accelerates inference 3.5x,"Hey all! We are here to share our latest work: consistency large language models (CLLMs), which is a new family of models capable of reducing inference latency by efficiently decodingÂ ğ‘›Â tokens in parallel. Your new friends for LLM serving/local deployment with faster inference speed! ğŸ”¥ Please check our blog post for demo with 3.1x speedup:

[https://hao-ai-lab.github.io/blogs/cllm/](https://hao-ai-lab.github.io/blogs/cllm/)

Compared with existing fast decoding techniques, CLLMs achieve fast parallel decodingÂ **without the need for**:

* Draft models
* Architectural modifications/auxiliary model components

This introduces a number of advantages for CLLMs:

* CLLMs don't have to deal with the complexity of obtaining 'good' draft models and managing two different models in a single system.
* CLLMs share the same architecture with target LLMs and require no additional engineering efforts when adopting the technique to different models.
* CLLMs can be integrated seamlessly with other techniques for efficient LLM inference (e.g. Lookahead Decoding) to achieve even more significant speedup.

This decoding method CLLMs use is calledÂ [Jacobi decoding](https://arxiv.org/abs/2305.10427), which improves inference efficiency in comparison with conventional auto-regressive decoding. CLLMs are trained with the objective of performing efficient Jacobi decoding by mapping any randomly initializedÂ ğ‘›-token sequence to the same result as AR decoding in as few steps as possible.

Experiment results have demonstrated the effectiveness of CLLMs, showingÂ 2.4Ã—Â toÂ 3.4Ã—Â improvements in generation speed on a variety of tasks.

[In comparison with Medusa2, CLLMs achieve comparable or better performance, but \*\*need no extra parameters or tree-style verification\*\*](https://preview.redd.it/fta39wapq9zc1.png?width=640&format=png&auto=webp&s=af39917a635c57a3d8886f8db3a952c0716909c9)

[CLLMs training objective visualization](https://preview.redd.it/nb7pulpqq9zc1.png?width=2560&format=png&auto=webp&s=d4b1ac36e92050579350bdae1e5225a9a5dc6387)

Please seeÂ [our paper](http://arxiv.org/abs/2403.00835)Â for more details. Feel free to try outÂ [our codebase](https://github.com/hao-ai-lab/Consistency_LLM)Â andÂ [CLLM checkpoints](https://huggingface.co/cllm)!

If you found our work interesting, please subscribe, like or repost, thanks! Learn more and engage with us on Twitter:

[https://x.com/haoailab/status/1788269848788869299](https://x.com/haoailab/status/1788269848788869299)",3,54
MachineLearning,t3_1cnflqd,1715202888.0,/r/MachineLearning/comments/1cnflqd/d_tips_and_tricks_for_performing_large_model/,Discussion,[D] Tips and tricks for performing large model checkpointing,"Checkpoints are super important during LLM training as they could help to restart a failed job from a last known good state. In the same time it is also a big challenge for a team, mostly because of checkpoints size and a fact that you want to save them ASAP without blocking a training process. For example, LLaMa 70B model checkpoint in training format is 782 gigabytes in size.

**How you will save them every hour?**

Based on our team (Nebius AI) experience we prepared a summary of tips and tricks for performing large model checkpointing:

Blog [https://nebius.ai/blog/posts/model-pre-training/large-ml-model-checkpointing-tips](https://nebius.ai/blog/posts/model-pre-training/large-ml-model-checkpointing-tips)

Video from last meetup in Amsterdam ([**https://www.youtube.com/watch?v=8HmORvLbh\_o**](https://www.youtube.com/watch?v=8HmORvLbh_o))

MLOps Community podcast: handling multi-terabyte large model checkpoints. The audio ([**https://podcasters.spotify.com/pod/show/mlops/episodes/Handling-Multi-Terabyte-LLM-Checkpoints--Simon-Karasik--228-e2j32c4**](https://podcasters.spotify.com/pod/show/mlops/episodes/Handling-Multi-Terabyte-LLM-Checkpoints--Simon-Karasik--228-e2j32c4)) is available across popular podcast platforms, and hereâ€™s the video ([**https://www.youtube.com/watch?v=6MY-IgqiTpg**](https://www.youtube.com/watch?v=6MY-IgqiTpg)).

**If you know more best practices around checkpoints, please add them as comments and let's discuss them!**",0,8
MachineLearning,t3_1cne918,1715199374.0,/r/MachineLearning/comments/1cne918/p_seeking_advice_on_finetuning_ssd_object/,Project,[P] ğŸ” Seeking Advice on Fine-tuning SSD Object Detection for My Custom Dataset ğŸ¯,"Hey everyone! I'm diving into the world of object detection, and I've set my sights on fine-tuning an SSD (Single Shot Multibox Detector) for my custom dataset. After doing some research, it seems like SSD's architecture aligns perfectly with what I need for my project.

Does anyone have recommendations for tutorials, notebooks, or resources that can help me on this mission? Specifically, I'm looking for tips on grabbing an SSD detector with pre-trained feature selection models, and then tweaking it to fit my dataset.",0,0
MachineLearning,t3_1cne766,1715199247.0,/r/MachineLearning/comments/1cne766/d_how_do_transformers_memorize_facts_after_a/,Discussion,[D] How do transformers memorize facts after a single gradient update?,"I guess I would first like a citation of that fact, or for somebody to tell me I made it up. But folk knowledge is that a transformer trained for a single epoch can recall facts that only appear a single time in the training dataset. This implies that a single update is enough to modify the weights to produce the correct output (without catastrophically forgetting other facts).

This is really surprising to me. I would think a single update large enough to substantially modify an output would be quite destructive, and possibly just not do what you want given the non-monotonicity of the loss landscape. Is there a good answer to how/why this happens, and if so can anyone provide a link to research that investigates this question? Is it a feature of large models (something like NTK), a feature of the transformer architecture, or something else? Note that I'm not asking about in-context learning, but the change from a single gradient step.",21,60
MachineLearning,t3_1cndn2u,1715197861.0,/r/MachineLearning/comments/1cndn2u/d_segmentation_of_land_plots_in_hand_drawn_maps/,Discussion,[D] Segmentation of Land Plots in Hand Drawn Maps,"I am currently working on a personal project on segmenting regions of land between roads and property lines from black and white maps with text on them. The first phase of the process is to segment and label each land plot. I know that some models exist through tools like Qgis and ArcGis but for reasons I am trying to avoid using them directly. 

My experience is mostly in medical image segmentation and classification but those have fixed classes and additionally models that I worked with wont translate to this data as well. As such I wanted to ask for guidance regarding existing models in this field that I can reference and fine tune or train from scratch. 

 As a baseline something like a Unet with a ResNet or VGG backbone could be used to segment the land vs road but I am not sure how to make sure it notes each plot as a separate object.  Additionally any pretrained model that I can fine tune would also be helpful",8,1
MachineLearning,t3_1cn9wlk,1715188401.0,/r/MachineLearning/comments/1cn9wlk/d_how_to_select_reliable_xai_methods_and_make/,Discussion,[D] How to select reliable XAI methods and make sense of conflicting explanations?,"https://preview.redd.it/baife9rhc8zc1.png?width=858&format=png&auto=webp&s=149789e027657cb292398427be23ae4e8f09a817

The figure is from [http://arxiv.org/abs/2206.04394](http://arxiv.org/abs/2206.04394) for a library called xplique. It is intended to display all the different attribution methods the authors have included in the library. However, it seems to me that different methods may highlight different regions and pixel structures in these images, with partial or little overlap in some cases. At the end of the day, I will have one model, a prediction and a ground truth (which may or not be known to me). Which method should I be looking at to explain a given prediction? And how would I know which one to choose beforehand? Is there one method that will always work even if I change the model architecture and data domain? (Assuming only post hoc methods here)

  
I'm currently doing a comparative analysis of different XAI methods for tabular data from the perspective of practicality and clarity for the users. I'm interested in understanding what is the current consensus on reliable methods, both for global and local explanations. That is, given the wide variety of XAI methods and libraries out there today, are there methods that are widely preferred? or conversely, generally avoided? Among other things, I want to consider robustness of the method (does it work for different types of models and different data domains?), and interpretability of the explanation itself (how easy is it to understand and communicate the explanation itself to non-technical users). From this, I'm also wondering, is there a way to measure the quality (or applicability) of the explanation? That is, how would I know if the explanation I'm looking at actually makes sense/if I'm using the right method for a given use case? Although the example above relates to computer vision, I suspect I will find similar issues in the case of tabular data.

Any recommendations on methods/libraries that are highly trusted or generally avoided?",4,11
MachineLearning,t3_1cn9ejf,1715187132.0,/r/MachineLearning/comments/1cn9ejf/d_fun_little_discovery_gemini_is_surprisingly_bad/,Discussion,[D] Fun little discovery: Gemini is surprisingly bad at following simple number sequences,"Try this: tell it to reply to you with the next number in a sequence, starting from 1, then you reply with the next one, and so on. After a few messages, it starts to output text instead of numbers, and after about 20 messages, it completely fails to keep following the sequence. 

The original goal was to find out how good is its long-term memory, which seems to be pretty bad. Given its huge context window, how can it lose focus so quickly?",19,21
MachineLearning,t3_1cn94yq,1715186469.0,/r/MachineLearning/comments/1cn94yq/d_strange_loss_curve_while_training/,Discussion,[D] Strange Loss Curve while training,"https://preview.redd.it/z5wmyi0nb8zc1.png?width=599&format=png&auto=webp&s=97e108bd749f9cf0874759f7ba0b8aafb3260640

Today I was training a small (11.07 Million) parameter GPT model on text dataset and I came across this loss curve while training, is there any explaination as to why the loss first plataeus around 2.4 and then starts to exponentially fall there after? Also why is there a sudden spike in between at around 1200 steps?

- The dataset I am using is the entire novel ""one hundred years of solitude""  
- The total token count in the train dataset is 0.82 million, with a vocabulary size of 77 (I am using a character level tokenizer)  
- 6 transformer layer with 6 attention heads each no bias in any proj and no dropout layer a context length of 1024 trained with a batch size of 32",16,25
MachineLearning,t3_1cn1f4o,1715164807.0,/r/MachineLearning/comments/1cn1f4o/d_how_valuable_is_a_diverse_and_highdefinition/,Discussion,[D] How valuable is a diverse and high-definition (quality) dataset of tens of thousands of visual content for research in Computational Aesthetics?,"# Researchers in Computational Aesthetics are exploring how computers can analyze and potentially create aesthetically pleasing visual experiences.  A key component of this research is having access to large, diverse datasets of high-quality visual content.

I'm curious to hear from the community:

* How valuable would a dataset of tens of thousands of diverse and high-quality images and videos be for research in Computational Aesthetics?
* What specific benefits might such a dataset offer (e.g., training AI models, evaluating aesthetic quality, exploring different artistic styles)?

I'm interested in learning from researchers, artists, and anyone else interested in the intersection of AI and aesthetics.

**Additionally:**

* A specific example of a research area in Computational Aesthetics that could benefit from such a dataset (e.g., style transfer, artistic quality assessment).
* A link to an article or resource that explains Computational Aesthetics for those unfamiliar with the field (optional).",3,3
MachineLearning,t3_1cmwljs,1715144800.0,/r/MachineLearning/comments/1cmwljs/research_xlstm_extended_long_shortterm_memory/,Research,[Research] xLSTM: Extended Long Short-Term Memory,"Abstract:

In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.

Link: [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517)",48,172
MachineLearning,t3_1cmntyr,1715118744.0,/r/MachineLearning/comments/1cmntyr/non_technical_ml_podcasts_d/,Discussion,Non Technical ML Podcasts? [D],"Hey everyone. For context, Iâ€™m a recent CS graduate and current entry level Data Engineer, and Iâ€™ve always loved learning about ML models and techniques and how to implement, deploy, and scale them. Iâ€™m looking for a good podcast to keep my knowledge of ML trends up to date, but the challenge is that I donâ€™t really like listening to podcasts that are technical as I am still a newbie and generally understand complexities better if I read them. Iâ€™ve tried some podcasts but most of the time the stuff goes over my head and I get lost. Looking for something I can listen to without having to think too hard on my way to work. Would love any suggestions!",14,23
MachineLearning,t3_1cmirbu,1715106192.0,/r/MachineLearning/comments/1cmirbu/d_peft_techniques_actually_used_in_the_industry/,Discussion,[D] PEFT techniques actually used in the industry,"A lot of works on parameter efficient fine tuning of transformers are coming out, but how much of them are actually being applied? Also I was curious what techniques do you normally use in the industry?",20,62
MachineLearning,t3_1cmiqub,1715106157.0,/r/MachineLearning/comments/1cmiqub/d_weighted_pruning_question/,Discussion,[D] weighted pruning question,"
Hi I'm doing weighted pruning, but I have one issuse here , so let's say I have a tensor so most of the tensors are nearly to zero so I changed that to zero , so nearly 40percent of the tensors zero now, does that mean my matrix is a sparse one or is it still dense , if it's not a sparse matrix , the computation will be same right , all row and column gets multipled , so then what is the purpose of weighted pruning then !! ",5,0
MachineLearning,t3_1cmh0a6,1715101717.0,/r/MachineLearning/comments/1cmh0a6/d_can_anyone_with_the_expertise_speak_to_the/,Discussion,"[D] Can anyone with the expertise speak to the overlap, or not, between Nvidia's hardware and Apple's hardware? ","I'm curious to understand how much realistic potential there is that Apple can compete with Nvidia IF we make an assumption that they're starting with what we know about in the M series chips. Could they pull some of this IP to make purpose built ""AI"" chips that might compete?

Context: Rumors that Apple might try to do this..",26,25
MachineLearning,t3_1cmfbzc,1715097487.0,/r/MachineLearning/comments/1cmfbzc/p_skyrim_opensource_model_zoo_for_large_weather/,Project,[P] Skyrim - Open-source model zoo for Large Weather Models,"[Github link](https://github.com/secondlaw-ai/skyrim) 

Hey all, I'm Efe from Secondlaw AI. We are building physics-informed large AI models. Currently, we are focusing on weather modelling.

To benchmark SOTA, we had to build a forecasting infra for all available large weather models and we could not find a solid tooling to do so, so we built [Sykrim](https://github.com/secondlaw-ai/skyrim). Within <5 mins and <5 LOC you can run forecasts on par with global weather models that are run on 100K+ CPU HPCs! You can check out examples [here](https://github.com/secondlaw-ai/skyrim/blob/master/notebooks/01_quickstart.ipynb).

We are implementing more models & fine-tuning capabilities. Let us know if anything more we can add, happy to answer any questions!",29,78
MachineLearning,t3_1cmcfv4,1715089985.0,/r/MachineLearning/comments/1cmcfv4/p_identify_toxic_underwater_air_bubbles_lurking/,Project,[P] Identify toxic underwater air bubbles lurking in the substrate with aquatic ultrasonic scans via Arduino Nano ESP32 (Ridge classification) and assess water pollution based on chemical (color-coded) water quality tests via UNIHIKER (NVIDIA TAO RetinaNet) simultaneously.,,11,50
MachineLearning,t3_1cmb0x4,1715085872.0,/r/MachineLearning/comments/1cmb0x4/p_yari_yet_another_rag_implementation_hybrid/,Project,[P] YARI - Yet Another RAG Implementation. Hybrid context retrieval,"
I made YARI. 

It features a hybrid fusion search between BM25 and Cosine Similarity and is built on top of Redis.

Uses: FastAPI, Celery and Redis.
OpenAIâ€™s API support for embedding generation and prompt completion.

Please give me your feedback on it. 
Source: https://github.com/fighterbay/YARI",6,17
MachineLearning,t3_1cm9r0y,1715081779.0,/r/MachineLearning/comments/1cm9r0y/d_limiting_llm_output_to_certain_words/,Discussion,[D] limiting LLM output to certain words,"Suppose I want to do a multi-class classification on text. One approach is to prompt engineer, however, this can output labels different to what I want. Here is an example:

    Extract the following labels from the text. Labels: Apples, Oranges. 
    Text: I ate an apple and then a few oranges. 
    Answer: Apples, Oranges

The answer shown above being simply the expected answer. If we were to use prompts, some other possibilities would be \[Apple, Orange\], \[Oranges, Apples\] etc.

In my case I do have an extensive set of labels that I can fine tune a model on. While I can train `BERT` to do this, I want to be able to add labels in the future, so want to try finetuning an LLM. Is there a way to train this so that we limit the words that can be output after Answer? One way I can think of is looking at the logits of the word, but this depends on the tokenization (eg. apple could be ap\_, \_ple).

There is also the `instructor` library, but this doesn't work with transformer library models (eg. Llama-3) to my understanding, (at least not without hosting it separately).

Would appreciate any hints/ thoughts about this. TIA",14,6
MachineLearning,t3_1cm82xu,1715075514.0,/r/MachineLearning/comments/1cm82xu/p_agent_cloud_opensource_gui_platform_to_build/,Project,[P] Agent Cloud - Open-source GUI platform to build private LLM apps," Hey everyone, We're building Agent Cloud and weâ€™ve been working in the RAG space since last couple of months and weâ€™re open-source.

>[Agent Cloud](https://github.com/rnadigital/agentcloud) is an open-source platform enabling companies to build and deploy private LLM chat apps, empowering teams to securely interact with their data. AgentCloud internally uses Airbyte to build data pipelines allowing us to split, chunk, and embed data from over 300 data sources, including NoSQL databases like MongoDB. It simplifies the process of ingesting data into the vector store for the initial setup and subsequent scheduled updates, ensuring that the vector store information is always updated. AgentCloud uses Qdrant as the vector store to efficiently store and manage large sets of vector embeddings. For a given user query the RAG application fetches relevant documents from vector store by analyzing how similar their vector representation is compared to the query vector.  
>  
>You can find more info about how it works and how to use it in the projectâ€™s README and We're launching cloud version by end of this week. 

Weâ€™re also very open to contributions and added good first issues for beginners.

&#x200B;

* Sync strategies - we still need to implement ability to change to incremental append instead of full overwrite
* Chunking strategies - We have semantic chunking, we want to implement custom strategies that would work well with Airbyte connections - currently chunking message by message (Rust)
* Retrieval strategies - Currently we use agents to craft the query, we would either like more standard retrieval strategies that can be added out of the box in our RAG connector (TS, Python, Mongo)
* Conversation app ease of setup - we have a design pattern we would like to employ to make setup of conversation apps simpler. 
* APIs - Publish our current Web App APIs as open API spec and more. 

Happy to answer any questions. \[GitHub repo\]([https://github.com/rnadigital/agentcloud](https://github.com/rnadigital/agentcloud))",0,1
MachineLearning,t3_1cm7frx,1715072786.0,/r/MachineLearning/comments/1cm7frx/d_recognizing_uncommon_terms_with_whisper/,Discussion,[D] Recognizing uncommon terms with whisper ,"Hello everyone
I'm currently working on Whisper to specialize it in French railway language. I'm facing some issues with transcribing ambigous words, and recognizin station names. Initially, i tried training it with audio file totaling 2 hours, but the results didn't meet my expectations. I then turned to usings prompts, which solved the ambiguity problÃ¨me, however since the context size is limited to 244 tokens, i can't include all station names.

Could you please provide me with some tips? I'm new to this field. 
Thank you",4,5
MachineLearning,t3_1cm7eqw,1715072668.0,/r/MachineLearning/comments/1cm7eqw/d_is_eos_token_crucial_during_pretraining/,Discussion,[D] Is EOS token crucial during pre-training?,"The EOS token used during pretraining marks ""end of sequence"", but it does not prevent information to flow across potentially unrelated documents. If so why to even include it during pretraining when we can add it later in SFT phase?",7,22
MachineLearning,t3_1cm64jk,1715066992.0,/r/MachineLearning/comments/1cm64jk/d_stack_overflow_partnership_with_open_ai/,Discussion,[D] Stack Overflow partnership with OPEN AI," [https://stackoverflow.co/company/press/archive/openai-partnership](https://stackoverflow.co/company/press/archive/openai-partnership)

A couple of thoughts:

\- Pretty sure OPEN AI has already scraped Stack Overflow while training ChatGPT (if you don't believe it - please watch again the famous interview with Mira Murati) - so why do this? Maybe to have legal access to the content?

\- Since Chat GPT has been released, StackOverflow is declining in popularity (see chart below from Google trends) - so it makes sense for SO owners

\- Very interesting from the community perspective: developers created the entire content for free which will now be used to replace them, and they don't get the profit share 

&#x200B;

https://preview.redd.it/fudrujkniyyc1.png?width=968&format=png&auto=webp&s=e116159e61394557e03a6cad431aadc77f88807b",27,63
MachineLearning,t3_1cm4h4i,1715060200.0,/r/MachineLearning/comments/1cm4h4i/d_how_does_fast_inference_work_with_state_of_the/,Discussion,[D] How does fast inference work with state of the art LLMs?,"Iâ€™ve read that inference speed for models like Llama-2 70B is ~10 t/s at best. So that left me wondering how the extremely large models like GPT-4 (1T params?) do their fast 20 t/s inference. With 10x the params, they gotta have at least 3x the layers(?) So that should make its inference much slower. Am I missing anything? What kind of further improvements might these companies be doing to power their fast APIs?

Edit: I must mention that you cannot parallelize across GPUs to help with latency of a single example when the data has to pass through model layers sequentially.

And with the large model sizes, model parallelism, with its inter-GPU communication should make it even slowerâ€¦",33,38
MachineLearning,t3_1clzh29,1715044056.0,/r/MachineLearning/comments/1clzh29/concerns_regarding_building_out_nodes_for_ai_gpu/,Project,Concerns regarding building out nodes for AI GPU cluster [P],"Here are some options that are available in my region, I want to go with the 2011, because of how cost-effective the CPUs were for the amount of cores and threads, so there were 2 platform the X79 and the X99. DDR3 was significantly cheaper than DDR4 even though offering little to no performance drop, x99 boards were available with only DDR4 and didn't have any DDR3 boards. As for the GPU, I went with the mi50 16gb because it was available here for just around $130. So after some researching here is what I found:

Concerns:

* I'm planning to do Video Generative Model Training, and I'm still relatively unsure whether or not Ram matters a lot, it seems like having a lot of ram you could do less streaming data on disk, and offload it to Ram for faster access from GPU. If you don't I assume it would just hinder data reading speed?
* As for storing Data, I don't know if I would actually need to build out a Storage Cluster for this? It seems like it's also possible to tream data to the nodes though it would be very slow? Or potentially just do data slicing so that the amount of data isn't too large for any node? Can I potentially train let say with 10TB of data first, then because my disk is full, delete the current batch data and get another 1OTB of data to then continue training, is that possible?
* As for MI50 as well, it seems like rocm has dropped support for this card, I was planning to use Zluda, basically a drop-in driver on top of CUDA for AMD, which uses the Rocm 5.7, is this going to affect the stability of the GPU at all if I'm training on Pytorch with Zluda?

Option #1: Potentially Ram Restricted But less?

* Main: X79 5 slot 3.0 x8
* Ram: 32gb DDR3
* CPU: 2696v2
* GPU: 5x MI50 16GB

Option #2: - Ram Restricted?

* Main: X79 9 slot 3.0 x8
* Ram: 32gb DDR3
* CPU: Dual 2696v2
* GPU: 9x MI50 16GB

Option #3: Pcie Lanes Restricted?

* Main: X79 8 slot 2.0 \* x1
* Ram : 64gb DDR3
* CPU: Dual 2696v2
* GPU: 8x Mi50 16GB",16,0
MachineLearning,t3_1clx2bi,1715037116.0,/r/MachineLearning/comments/1clx2bi/d_data_preparation_for_gpt2_training/,Discussion,[D] Data Preparation for GPT-2 Training,"Hi guys,

I am creating a dataset to train/fine-tune GPT-2 with nanoGPT repository. I have this formating in a txt file:

SENTENCE

TREE PARSING

<|endoftext|>

SENTENCE

TREE PARSING

<|endoftext|>

And so on (the special token is added in the tokenization process, \\n\\n is replaced by it).

Is this correct? Or should I use other formating for the model to learn parsing sentences?

Thank you!ğŸ˜Š

[Some examples](https://preview.redd.it/4mzif9ws1wyc1.png?width=2080&format=png&auto=webp&s=6167a20ff85bc19bfcaf6803d4b63f3b0c77c5fe)

",0,0
MachineLearning,t3_1cloxpt,1715016808.0,/r/MachineLearning/comments/1cloxpt/d_icml_participation_grant/,Discussion,[D] ICML Participation Grant,"As a PhD student in Canada with an accepted paper at ICML, I'm curious about funding options for attending these costly conferences. While my supervisor covers some costs, the total can reach 3500-4000 CAD, including a 700 CAD registration fee. Are there other external funding sources available to cover the remaining expenses?",2,4
MachineLearning,t3_1cljvpa,1715004290.0,/r/MachineLearning/comments/1cljvpa/d_llama_3_monstrosities/,Discussion,[D] Llama 3 Monstrosities,"I just noticed some guy created a 120B Instruct variant of Llama 3 by merging it with itself (end result duplication of 60 / 80 layers). He seems to specialize in these Frankenstein models. For the life of me, I really don't understand this trend. These are easy breezy to create with mergekit, and I wonder about their commercial utility in the wild. Bud even concedes its not better than say, GPT-4. So what's the point? Oh wait, he gets to the end of his post and mentions he submitted it to Open LLM Leaderboard... there we go. The gamification of LLM leaderboard climbing is tiring.",23,44
MachineLearning,t3_1clhvi0,1714998700.0,/r/MachineLearning/comments/1clhvi0/r_academic_survey_about_diversity_in_ai/,Research,[R] academic survey about diversity in AI development/research teams,"For my PhD research (social sciences) I am looking for respondents for this survey on diversity and how it affects trustworthy AI development, particularly on trustworthiness (as defined by the EU, AI HLEG ethics guidelines for trustworthy AI). My target population are people working on artificial intelligence (machine learning and algorithms as well), preferably as developers, but researchers are welcome, too! There are no further restrictions/criteria for participation.Â [Link to survey](https://erasmusuniversity.eu.qualtrics.com/jfe/form/SV_6xGGjbwZb9eokZg)

The focus of this study is the role of diversity within an organization/team, how diversity is perceived (diversity perspectives and diversity climate), and how it affects development of Trustworthy AI. The survey considers aspects such as gender, age, and cultural background, as well as so-called functional aspects, e.g., educational background or specialization.

This study is part of my PhD project so if you fit the criteria, please consider filling out this survey. Otherwise, if you know anyone who fits the criteria and is willing to participate, please share this post with them!

If you have any questions or comments, don't hesitate to message me!",25,0
MachineLearning,t3_1clg7mu,1714993214.0,/r/MachineLearning/comments/1clg7mu/d_get_paid_for_peer_reviews_on_researchhub/,Discussion,[D] Get paid for peer reviews on ResearchHub,"ResearchHub is rewarding peer reviews on various topics, including AI, for which I'm an editor. The paiement is \~150$ per peer review (paid in their cryptocurrency but easily exchangeable for dollars). Here are some papers for which a peer review bounty is currently available, but keep in mind new papers are often added (and you can also upload papers you'd find interesting to review):

- [Physics Of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://www.researchhub.com/paper/6393562/physics-of-language-models-part-33-knowledge-capacity-scaling-laws)

- [Scaling Laws For Fine-Grained Mixture Of Experts](https://www.researchhub.com/paper/6402478/scaling-laws-for-fine-grained-mixture-of-experts)

- [Mixture-Of-Depths: Dynamically Allocating Compute In Transformer-Based Language Models](https://www.researchhub.com/paper/6402477/mixture-of-depths-dynamically-allocating-compute-in-transformer-based-language-models)

- [Interpretability Analysis And Attention Mechanism Of Deep Learning-Based Microscopic Vision](https://www.researchhub.com/paper/6415227/interpretability-analysis-and-attention-mechanism-of-deep-learning-based-microscopic-vision)

To get the bounty, simply post your review in the Peer Review tab of the paper, and you'll get the bounty if the quality is sufficient. I'd be happy to answer any question you have!",8,8
MachineLearning,t3_1clfe3o,1714989976.0,/r/MachineLearning/comments/1clfe3o/d_ner_for_large_text_data/,Discussion,[D] NER for large text data,"Hello people 
I am currently working as a data scientist at startup. We have a requirement of extracting entities from the text of 10 billion tokens. I am not aware how to do it at this much scale. What should be the pipeline and so on. It would be helpful if you guys share your knowledge or good research paper/blog. Currently we are working on 18 entities and my boss wants me to get 93% accuracy.
Thankyou ",3,8
MachineLearning,t3_1clf2sg,1714988668.0,/r/MachineLearning/comments/1clf2sg/p_table_extraction_text_extraction/,Project,"[P] Table Extraction , Text Extraction","
The input is a blueprint design presented as a PDF. Currently, my dataset consists of four different samples, each with a unique title name for the table and column names. I need to extract the title block and dimensions for each layout and put them into an Excel file.


Footings	Quantity	Length	Width	Height	Reo type
PF1	4	1.9	1.9	1.1	N16 @ 200 C/C EACH WAY TOP & BOTTOM
PF2	5	1.5	1.5	1.1	N16 @ 200 C/C EACH WAY TOP & BOTTOM
PF3	3	1.2	1.2	0.8	N16 @ 200 C/C EACH WAY TOP & BOTTOM",8,5
MachineLearning,t3_1cldfy2,1714981723.0,/r/MachineLearning/comments/1cldfy2/p_lerobot_hugging_faces_library_for_realworld/,Project,[P] LeRobot: Hugging Face's library for real-world robotics,"MeetÂ [LeRobot](https://github.com/huggingface/lerobot), a library hosting state-of-the-art deep learning for robotics.

The next step of AI development is its application to our physical world. Thus, we are building a community-driven effort around AI for robotics, and it's open to everyone!  
Take a look at the code:Â [https://github.com/huggingface/lerobot](https://github.com/huggingface/lerobot)

https://preview.redd.it/ugf4l8lfgryc1.png?width=3794&format=png&auto=webp&s=222825e897ba48eb07acedffb0662d5794af04e8

LeRobot is to robotics what the Transformers library is to NLP. It offers clean implementations of advanced AI models with pre-trained checkpoints. We also reimplemented 31 datasets from academia, and some simulation environments, allowing to get started without a physical robot.

https://preview.redd.it/1d8qnrpggryc1.png?width=2563&format=png&auto=webp&s=3e0d6c10440c9d0970b867813d3a210dd9a7bab9

Additionally, the same models can be trained on real-world datasets. Here is a cool data visualization withÂ [rerun.io](http://rerun.io/)Â which is fully integrated with our video format optimized for training. The data originally comes from theÂ [Aloha project](http://tonyzhaozh.github.io/aloha).  
[\[LINK TO VIDEO\]](http://remicadene.com/assets/videos/battery-720p.mov)

https://preview.redd.it/86ihkcwhgryc1.png?width=2506&format=png&auto=webp&s=4f2ca7522a012d00d7327d90335d069dd099a321

Another visualization with LeRobot, this time onÂ [Mobile Aloha](http://mobile-aloha.github.io/)Â data, to learn navigation and manipulation totally end-to-end. Both datasets have been collected onÂ [trossenrobotics](https://www.trossenrobotics.com/)Â robot arms.Â [\[LINK TO VIDEO\]](http://remicadene.com/assets/videos/wipe-420p.mov)

https://preview.redd.it/qqtncqligryc1.png?width=1900&format=png&auto=webp&s=4f83c675b5c6f9dbded4b5b90a7a1c9f531c4086

LeRobot codebase has been validated by replicating state-of-the-art results in simulations. For example, here is the famous ACT policy which has been retrained and made available as a pretrained checkpoint:  
[\[LINK TO HF HUB\]](http://huggingface.co/lerobot/act_aloha_sim_transfer_cube_human/blob/main/README.md)

LeRobot also features theÂ [Diffusion Policy](http://diffusion-policy.cs.columbia.edu/), a powerful imitation learning algorithm, andÂ [TDMPC](http://yunhaifeng.com/FOWM), a reinforcement learning method that includes a world model, continuously learning from its interactions with the environment.

https://preview.redd.it/br9ibrylgryc1.png?width=1684&format=png&auto=webp&s=8e5595f1dff5381e5f60c6776126f48187ec58d9

Come join ourÂ [Discord channel](http://discord.com/invite/s3KuuzsPFb). We are building a diverse community from various backgrounds, software and hardware, to develop the next generation of smart robots in the real-world!

Thanks to the AI and robotics community without whom LeRobot won't have been possible.",1,46
MachineLearning,t3_1cld406,1714980277.0,/r/MachineLearning/comments/1cld406/d_volunteers_for_mlcv_conferences/,Discussion,[D] - Volunteers for ML/CV conferences,"Hi everyone,

I want to get some information about volunteering for ML/CV conferences (e.g. CVPR, ECCV, ICML, etc). In particular:

* Is there a selection process?
* What are volunteers doing in these conferences?
* In general, was it worth it? Especially from a network point of view.

Thanks",4,4
MachineLearning,t3_1clcu5i,1714979066.0,/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/,Discussion,[D] Kolmogorov-Arnold Network is just an MLP,"It turns out, that you can write Kolmogorov-Arnold Network as an MLP, with some repeats and shift before ReLU.

[https://colab.research.google.com/drive/1v3AHz5J3gk-vu4biESubJdOsUheycJNz](https://colab.research.google.com/drive/1v3AHz5J3gk-vu4biESubJdOsUheycJNz)",92,305
MachineLearning,t3_1clcluq,1714978129.0,/r/MachineLearning/comments/1clcluq/d_why_gemma_has_such_crazy_big_mlp_hidden_dim_size/,Discussion,[D] Why Gemma has such crazy big MLP hidden dim size?,,18,149
MachineLearning,t3_1clbmz2,1714974189.0,/r/MachineLearning/comments/1clbmz2/r_why_can_llama3_work_with_32k_context_if_it_only/,Research,[R] Why can Llama-3 work with 32K context if it only had 8K context length?,"Hello folks! See post here:Â [https://twitter.com/abacaj/status/1785147493728039111](https://twitter.com/abacaj/status/1785147493728039111)

I didn't understand what he meant by ""with zero-training (actually just a simple 2 line config) you can get 32k context out of llama-3 models""

Does someone know what thisÂ **dynamic scaling trick**Â is? Much appreciated! :)",8,45
MachineLearning,t3_1clbipd,1714973721.0,/r/MachineLearning/comments/1clbipd/d_classification_with_gradient_boosting/,Discussion,[D] Classification with Gradient boosting,"Classification with Gradient boosting

I am trying to classify objects in urban area, mainly buildings and vegetations. I am using geometric features from lidar data like planarity, linearity verticality, omnivariance, smallest eigenvalues, change of curvatures, sphericity. I have five classes low vegetation, mid vegetation, high vegetation and buildings.and use the radius from [1...12] I did a randomisedsearch to find the parameters, n_estimator=100, learning rate =0.1, min_sample_split=2, min_sample_leaf= 1. The model gets an accuracy of 98%. When i predict the model in larger scale, there are few problems. The edges of the building in some buildings and the straight lines in a triangle shape roof(mostly found in european urban area) . In these two cases the model predicts them as high vegetation. 
Now i dont know how to move forward whether to increase the n estimator and learning rate or to find features that help differentiate vegetation and the edge cases.

Any advice would be appreciated, thanks

",6,3
MachineLearning,t3_1cl4tg1,1714952030.0,/r/MachineLearning/comments/1cl4tg1/d_is_there_a_formal_name_for_dialogue/,Discussion,"[D] Is there a formal name for ""dialogue classification?""","I'm trying to classify dialogues into categories. Specifically, if we have customer service chatting data where clients ask questions and CS agents answer, I want to categorize those to have labels like ""product inquiry,"" ""delivery inquiry,"" etc.

Is there a formal name for this? It doesn't seem to be normal text classification because we have to take into account speaker information. There also doesn't seem to be a task called dialogue classification. I thought intent classification may come closest but the typical datasets seem to only be using the initial query as the input text rather than the entire conversation.

I'm thinking that maybe using the entire dialogue may not be appropriate, and perhaps there could be an initial phase that extract key queries from the conversation. Afterwards maybe those could be used for intent classification but I'm not sure if this is an ideal approach.",7,9
MachineLearning,t3_1cl4oc7,1714951644.0,/r/MachineLearning/comments/1cl4oc7/d_llm_use_case_for_qa_and_reasoning/,Discussion,[D] LLM use case for QA and reasoning.,"Consider a use case, where we have textual data. we have to extract information from it. Some of the data is direct and can be assigned directly. Others are not so-direct, like total weight, total quantity, these values are supposed to be calculated after extracting individual data from the data.

Since RAG provides contextual information, so I am planning to inform the LLM about the labels to be extracted. I am also planning to fine-tune Llama3 on annotations so model learns about what how information extraction is actually taking place.

What else can be done to improve the output performance of model.",2,4
MachineLearning,t3_1cl3zmi,1714949739.0,/r/MachineLearning/comments/1cl3zmi/how_large_language_models_play_video_games_d/,Discussion,How Large Language Models play video games [D],"A video from my YT channel that talks about how LLMs are used to play video games like Crafter (Minecraft-lite) and Atari, among others. Some of these are solo LLM prompt engineering works, while some help RL agents explore or provide better reward signals. Link here in case anyone is interested.",0,11
MachineLearning,t3_1cl3vtp,1714949436.0,/r/MachineLearning/comments/1cl3vtp/d_python_libraries_that_support_gaussian/,Discussion,[D] Python libraries that support Gaussian processes with derivative information,"Hi everyone, I am looking for Python libaries (other than GPyTorch) that support Gaussian processes with derivative information. I am currently using GPyTorch and want to compare the results I get with other libraries. I have looked at the docs of GPflow and GPy but couldn't find if they support this or any examples in the docs. If you happen to have links to examples that would be great!",0,4
MachineLearning,t3_1cl3fs3,1714948228.0,/r/MachineLearning/comments/1cl3fs3/project_an_llmpowered_web_app_for_sec_filing/,Project,[Project] An LLM-Powered Web App for SEC Filing Insights,"I built an app that analyzes 10-K filings using large language model (LLM) APIs and generates insights to provide a comprehensive understanding of a company's financial performance and strategic direction through user-friendly visualizations and segment-wise breakdowns.

Here is the link to the GitHub repo:Â [https://github.com/astonishedrobo/sec-llm-insights](https://github.com/astonishedrobo/sec-llm-insights)

In future, I also plan to add RAG to avoid hallucination by LLM.Â ***Any suggestion to make this better/accurate will be appreciable.***",0,5
MachineLearning,t3_1cl1gs2,1714943118.0,/r/MachineLearning/comments/1cl1gs2/r_timeseries_predictive_ml_validation_set/,Research,[R] Time-series predictive ML validation set ,"Iâ€™ve been working on a project. Simply put, predicting the future time period, eg, 1 month ahead as Iâ€™ve used monthly data. 

As Iâ€™m working with time series data, is it logical/necessary to keep it in chronological order ? 

Critically, validating the model. 
If I now want to tune/optimise the model on validation data, how do I choose the length of the validation set as logically it would be the most recent data right ??? 
Should it be 1 month or for example 10 months ?
I have tried a brute force method, but that it not possible with my laptop.

Any insights or relevant stories would be great. 
Cheers ",1,0
MachineLearning,t3_1ckxzti,1714934095.0,/r/MachineLearning/comments/1ckxzti/research_understanding_the_attention_mechanism_in/,Research,[Research] Understanding The Attention Mechanism In Transformers: A 5-minute visual guide. ğŸ§ ,"TL;DR: Attention is a â€œlearnableâ€, â€œfuzzyâ€ version of a key-value store or dictionary. Transformers use attention and took over previous architectures (RNNs) due to improved sequence modeling primarily for NLP and LLMs.

[What is attention and why it took over LLMs and ML: A visual guide](https://open.substack.com/pub/codecompass00/p/visual-guide-attention-mechanism-transformers?r=rcorn&utm_campaign=post&utm_medium=web)

https://preview.redd.it/8aoqz10hjnyc1.png?width=1903&format=png&auto=webp&s=234b7aa38e9eee56d9d91f70f69ff81a7c666ff7

",1,11
MachineLearning,t3_1ckx3c6,1714931749.0,/r/MachineLearning/comments/1ckx3c6/d_problem_framingmodel_selection_for_marketing/,Discussion,[D] Problem Framing/Model Selection for Marketing Analytics,"Hello

We are in the process of selecting, training and using an AI model to determine the best sequence of marketing actions for the next few weeks to maximize INCREMENTAL sales for each customer segment for a B2B consumable product (i.e. one that needs to be purchased on a periodic basis). Many of our customers are likely to buy our products even without promotions - however, we have seen that weekly sales increase significantly when we have promotions

Historically, we have executed campaigns that include emails, virtual meetings and in-person meetings.

We have the following data for each week for the past 2 years

1. Total Sales (this is the target variable) for each segment
2. Campaign type

Our hypothesis is that INCREMENTAL weekly sales depend on a variety of factors including the customer segment, the channel (in-person, phone call, email) as well as the SEQUENCE of actions.

Our initial assumption is that promotions during any 4 week period has an impact on INCREMENTAL sales over the next 4 weeks. So campaigns in February have a significant impact in March but not much in April or May.

In general we have only one type of connect in any specific week (so either in-person, or phone or email). Therefore, in any 4 week period we have 3x3x3x3 = 81 combinations. (There are some combinations that are extremely unlikely such as in-person meetings every week for 4 weeks - so that actual number of combinations is probably slightly less than 81).

We are considering a 2 step process

1. For each segment and for each of the 81 combinations predict sales for the next 4 weeks. Subtract Predicted Sales from the Actual Sales for current 4 week period to find INCREMENTAL sales for next 4 weeks
2. Select the combination with the highest INCREMENTAL sales

For step 1, two of my data scientists are proposing different options.

Bob proposes Option A: Use regression. As per Bob, there is very limited temporal relationship between sales in different time periods so a linear regression model should be sufficient. He wants to try out linear regression, random forest and XGBoost. He thinks this approach can be tested quite quickly (\~8 weeks) and should give decent results.

Susan proposes Option B: As per Susan, we should use a time series method since sales for any segment for a given 4 week period should have some temporal relationship with prior 4 week periods. She wants to try smoothing techniques, ARIMA as well as deep learning methods such as vanilla RNN, LSTM and GRU. She is asking for about 12-14 weeks but says that this is a more robust method and is likely to show higher performance.

We have some time pressures to show some results and don't have resources to try both in parallel.

Any advice regarding how I should choose between the 2 options?",1,3
MachineLearning,t3_1ckwqj4,1714930789.0,/r/MachineLearning/comments/1ckwqj4/p_simple_captcha_reader/,Project,[P] Simple Captcha Reader,"This is a simple captcha reader model I made:

repo: [https://github.com/Null-byte-00/CaptchaReader](https://github.com/Null-byte-00/CaptchaReader)",0,0
MachineLearning,t3_1ckt6k4,1714921221.0,/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/,Discussion,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",91,11
MachineLearning,t3_1cksxm2,1714920533.0,/r/MachineLearning/comments/1cksxm2/r_postdoc_developing_medical_machine_learning_in/,Research,[R] Postdoc developing medical machine learning in patient with blood cancer,"We have created a multimodal large-scale data resource for Danish Lymphoid Cancer Research (DALY-CARE) including 65,000+ individuals from 13 nationwide registers + detailed electronic health record data. We collaborate with AZ who is hiring a fellow postdoc to develop medical machine learning algorithms to predict clinical outcomes on targeted therapies. Applications may be submitted here https://careers.astrazeneca.com/job/gothenburg/postdoc-fellow-machine-learning-for-predicting-adverse-events-in-blood-cancer-treatments/7684/64381401040",1,8
MachineLearning,t3_1ckrzq6,1714917845.0,/r/MachineLearning/comments/1ckrzq6/d_is_there_a_more_systematic_way_of_choosing_the/,Discussion,[D] Is there a more systematic way of choosing the layers or how deep the architecture goes when creating a neural network?,"So I'm learning about deep learning and neural networks and I'm really a bit confused on this part. I'm generally familiar with the layers available and how they work (at least those that are widely used) But I'm still having a hard time trying to figure out what to use on what. Is there a more logical or a systematic way of doing this? like mathematically or something? I'm down for experimenting but I'm just trying to avoid the rabbit hole since this projects on a deadline and I'm not down with that

  
\`\`\` EDIT \`\`\`\`

Thank you for all the responses especially for giving reading material and suggestions. ",42,105
MachineLearning,t3_1ckrd1u,1714915943.0,/r/MachineLearning/comments/1ckrd1u/research_creative_problem_solving_in_large/,Research,[Research] Creative problem solving in large language and vision models,"Code: [https://github.com/lnairGT/creative-problem-solving-LLMs](https://github.com/lnairGT/creative-problem-solving-LLMs)

The code provided in this repository prompts LLMs (image + text prompts) to identify creative object replacements (object substitution) when the required objects are missing, e.g., substituting a bowl for a scoop.Â This work shows that prompts that are augmented with relevant object features (i.e., affordances) enable LLMs to effectively reason about object substitutions.",0,4
MachineLearning,t3_1ckpgfd,1714909653.0,/r/MachineLearning/comments/1ckpgfd/d_efficient_muzero_value_prefix/,Discussion,[D] Efficient MuZero value prefix,"I am looking for a more clear explanation of ow value prefix works in efficient muzero. Does the value prefix completely replace the reward predictions or if not how are they working together? Is the value prefix meant to predict future values or past? At what exact MCTS step is the value prefix used? To me it seems that it should be used in backpropagaion, but it's unclear to me. How is the value prefix trained?",0,2
MachineLearning,t3_1cknrka,1714902881.0,/r/MachineLearning/comments/1cknrka/d_where_does_the_real_value_of_a_data_scientist/,Discussion,[D] Where does the real value of a data scientist come from?,"Companies care about what you can do for them with high regard to profit. A typical software engineer's value in my opinion is that:

1. They can deliver code fast

2. They're smart enough

i.e. they are extremely expendable, which is something you never want to be. Are data scientists as expendable as software engineers? What makes a data scientist irreplaceable and desired in the marketplace? ",57,28
MachineLearning,t3_1cklpyd,1714893933.0,/r/MachineLearning/comments/1cklpyd/d_nvidia_gpu_benchmarks_comparison/,Discussion,[D] NVIDIA GPU Benchmarks & Comparison,"[https://tensordock.com/benchmarks](https://tensordock.com/benchmarks)

Spent the past few hours putting together some data on vLLM (for both Llama 7B and OPT-125M) and Resnet-50 training performance on the TensorDock cloud. 

vLLM data is 100% out of the box, with 2048 batch sizes from [this repository](https://github.com/vllm-project/vllm/tree/main/benchmarks). 

My learnings:

* H100 and A100 performance is unbeatable, but the price-to-performance of lower-end RTX cards is pretty darn good. Even the L40 and RTX 6000 Ada outperform the A100 at some tasks, as they are 1 generation newer than the A100. **If your application does not need 80GB of VRAM, it probably makes sense to not use an 80GB VRAM card**
* Standalone H100 performance isn't as strong as I would have imagined. H100 performance is bottlenecked by memory bandwidth for LLM inference, hence **H100s are only 1.8x faster than A100s for vLLM**. H100s really perform better when interconnected together, but I didn't benchmark that today. 
* **CPU matters more than I expected**. The OPT-125M vs Llama 7B performance comparison is pretty interesting... somehow all GPUs tend to perform similar on OPT-125M, and I assume that's because relatively more CPU time is used than GPU time, so the GPU performance difference matters less in the grand scheme of things.  
* The marketplace prices itself pretty well. If cohort GPUs into VRAM amount all GPUs with similar amounts of VRAM share a similar price-to-performance ratio.
* Self hosting can save you $$$ if you have sufficient batch sizes. If you built your own inference API, you could serve LLMs utilizing just 50% batches and save money compared to a pay-per-token API (we \[TensorDock\] cost less than $0.07 per million Llama 7 tokens if you use us at 100%)

\-- 

Let me know which GPU to benchmark next, and I'll add that! Or let me know some other workload to measure, and I'd be happy to add an new section for that too. 

P.S. We added some H100s at $1.80/hr for anyone lucky enough to grab them! ",5,36
MachineLearning,t3_1cklipt,1714893079.0,/r/MachineLearning/comments/1cklipt/dwhy_lecuns_jepa_is_not_an_auto_regressive/,Discussion,[D]Why Lecunâ€™s JEPA is not an auto regressive framework?,Any opinion is welcome!,12,0
MachineLearning,t3_1ckl6fj,1714891697.0,/r/MachineLearning/comments/1ckl6fj/n_1st_workshop_on_incontext_learning_at_icml_2024/,News,[N] 1st Workshop on In-Context Learning at ICML 2024,,0,8
MachineLearning,t3_1ckkf5f,1714888637.0,/r/MachineLearning/comments/1ckkf5f/r_a_careful_examination_of_large_language_model/,Research,[R] A Careful Examination of Large Language Model Performance on Grade School Arithmetic,"**Paper**: [https://arxiv.org/abs/2405.00332](https://arxiv.org/abs/2405.00332)

**Abstract**:

>Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission ***Grade School Math 1000*** (**GSM1k**). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with several families of models (e.g., Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes. At the same time, many models, especially those on the frontier, (e.g., Gemini/GPT/Claude) show minimal signs of overfitting. Further analysis suggests a positive relationship (Spearman's *r*^(2)=0.32) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that many models may have partially memorized GSM8k.",16,64
MachineLearning,t3_1ckiamf,1714880480.0,/r/MachineLearning/comments/1ckiamf/d_r_are_there_any_methodsworks_that_enable/,Discussion,[D] [R] Are there any methods/works that enable extracting high-quality dense feature map from CLIP/OpenCLIP image encoders without large scale finetuning?,"Hi, as stated in the title, I'm curious if such methods exist. We know that (trained) CLIP's image and text encoders both output an 1D vector that are aligned in the latent space, which allows to easily compute the similarities between a batch of images and texts. However, in many vision applications, it is desirable to get a 3D feature map of shape C\*H\*W. Ideally, if the vector at each spatial location in this feature map is as high-quality as the final (attention-pooled) encoded 1D image vector of CLIP, we can compute the similarity of the feature map and texts at each location, and get some sort of 2D attention/similarity/score map between image and text, which would be very helpful for many downstream tasks.

I'm aware that CLIP has been applied to many (open-vocabulary/zero-shot) detection/segmentation tasks, and some of the works explores similar ideas I stated above, but most them are pretty complicated and cannot be used in a plug-and-play fashion. The works that are the closest are [this work](https://arxiv.org/abs/2204.03647) and [MaskCLIP](https://arxiv.org/abs/2112.01071), but I found it hard to replicate their results.

I'm curious what you think about this. I know that the objective of contrastive pretraining doesn't really gaurentee good dense feature map but I'm wondering if there's anything that I'm missing.",12,11
